{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.layers.core import Dropout\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "from keras.layers.experimental.preprocessing import Rescaling\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "with open('./NHANES_Full_trimmed_1720.csv', 'r') as f:    \n",
    "    lines = f.readlines()\n",
    "    header = lines[0].rstrip('\\n').split(',')\n",
    "    data = [header[1:]]    \n",
    "        \n",
    "    for line in lines[1:]:\n",
    "        line = line.rstrip('\\n').split(',')\n",
    "        line_new = []\n",
    "        for element in line:             \n",
    "            if element == 'NA': \n",
    "                element = np.nan                \n",
    "            elif (element.isalpha()):\n",
    "                element = ord(element)                \n",
    "            else:\n",
    "                element = pd.to_numeric(element)\n",
    "            line_new.append(element) \n",
    "        \n",
    "        data.append(line_new[1:])\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BMDSTATS</th>\n",
       "      <th>BMXWT</th>\n",
       "      <th>BMXHT</th>\n",
       "      <th>BMXBMI</th>\n",
       "      <th>BMXLEG</th>\n",
       "      <th>BMXARML</th>\n",
       "      <th>BMXARMC</th>\n",
       "      <th>BMXWAIST</th>\n",
       "      <th>OHDDESTS</th>\n",
       "      <th>OHX01TC</th>\n",
       "      <th>...</th>\n",
       "      <th>PUQ100</th>\n",
       "      <th>PUQ110</th>\n",
       "      <th>SMAQUEX2</th>\n",
       "      <th>SMQ860</th>\n",
       "      <th>SMQ870</th>\n",
       "      <th>SMQ872</th>\n",
       "      <th>SMQ874</th>\n",
       "      <th>SMQ878</th>\n",
       "      <th>SMQ940</th>\n",
       "      <th>SMAQUEX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>42.2</td>\n",
       "      <td>154.7</td>\n",
       "      <td>17.6</td>\n",
       "      <td>36.3</td>\n",
       "      <td>33.8</td>\n",
       "      <td>22.7</td>\n",
       "      <td>63.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>89.3</td>\n",
       "      <td>15.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.6</td>\n",
       "      <td>14.8</td>\n",
       "      <td>41.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>97.1</td>\n",
       "      <td>160.2</td>\n",
       "      <td>37.8</td>\n",
       "      <td>40.8</td>\n",
       "      <td>34.7</td>\n",
       "      <td>35.8</td>\n",
       "      <td>117.9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 167 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   BMDSTATS  BMXWT  BMXHT  BMXBMI  BMXLEG  BMXARML  BMXARMC  BMXWAIST  \\\n",
       "0       4.0    NaN    NaN     NaN     NaN      NaN      NaN       NaN   \n",
       "1       1.0   42.2  154.7    17.6    36.3     33.8     22.7      63.8   \n",
       "2       1.0   12.0   89.3    15.0     NaN     18.6     14.8      41.2   \n",
       "3       1.0   97.1  160.2    37.8    40.8     34.7     35.8     117.9   \n",
       "4       NaN    NaN    NaN     NaN     NaN      NaN      NaN       NaN   \n",
       "\n",
       "   OHDDESTS  OHX01TC  ...  PUQ100  PUQ110  SMAQUEX2  SMQ860  SMQ870  SMQ872  \\\n",
       "0       1.0      4.0  ...     NaN     NaN       NaN     1.0     1.0     2.0   \n",
       "1       1.0      4.0  ...     2.0     9.0       2.0     2.0     1.0     2.0   \n",
       "2       1.0      4.0  ...     NaN     NaN       NaN     1.0     1.0     2.0   \n",
       "3       1.0      2.0  ...     2.0     2.0       1.0     2.0     1.0     2.0   \n",
       "4       NaN      NaN  ...     NaN     NaN       1.0     1.0     1.0     2.0   \n",
       "\n",
       "   SMQ874  SMQ878  SMQ940  SMAQUEX  \n",
       "0     1.0     1.0     2.0      3.0  \n",
       "1     1.0     1.0     2.0      1.0  \n",
       "2     1.0     1.0     2.0      3.0  \n",
       "3     1.0     2.0     2.0      2.0  \n",
       "4     1.0     1.0     1.0      2.0  \n",
       "\n",
       "[5 rows x 167 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data = pd.DataFrame(data[1:], columns=data[0])\n",
    "df_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recode the y label into binary\n",
    "# 'DIQ010'\n",
    "# 1,3: Yes (diabetic)\n",
    "# 2,9: No (Non-diabetic)\n",
    "replace_dict = {3: 1, 2: 0, 9: 0 }\n",
    "df_data['DIQ010'] = df_data['DIQ010'].replace(replace_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Min-Max Normalization\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(1, 2))\n",
    "scaler.fit(df_data)\n",
    "data_scaled_minmax = scaler.transform(df_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Standardization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df_data)\n",
    "data_scaled_std = scaler.transform(df_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_data_std = pd.DataFrame(data_scaled_std, columns=data[0])\n",
    "df_data_std.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BMDSTATS</th>\n",
       "      <th>BMXWT</th>\n",
       "      <th>BMXHT</th>\n",
       "      <th>BMXBMI</th>\n",
       "      <th>BMXLEG</th>\n",
       "      <th>BMXARML</th>\n",
       "      <th>BMXARMC</th>\n",
       "      <th>BMXWAIST</th>\n",
       "      <th>OHDDESTS</th>\n",
       "      <th>OHX01TC</th>\n",
       "      <th>...</th>\n",
       "      <th>PUQ100</th>\n",
       "      <th>PUQ110</th>\n",
       "      <th>SMAQUEX2</th>\n",
       "      <th>SMQ860</th>\n",
       "      <th>SMQ870</th>\n",
       "      <th>SMQ872</th>\n",
       "      <th>SMQ874</th>\n",
       "      <th>SMQ878</th>\n",
       "      <th>SMQ940</th>\n",
       "      <th>SMAQUEX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.125</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.125</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.155317</td>\n",
       "      <td>1.629843</td>\n",
       "      <td>1.070896</td>\n",
       "      <td>1.380795</td>\n",
       "      <td>1.602469</td>\n",
       "      <td>1.215760</td>\n",
       "      <td>1.161356</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>1.125</td>\n",
       "      <td>2.000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.125</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.125</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.125</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.035046</td>\n",
       "      <td>1.090684</td>\n",
       "      <td>1.038557</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.227160</td>\n",
       "      <td>1.067542</td>\n",
       "      <td>1.008136</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.125</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.125</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.373955</td>\n",
       "      <td>1.675185</td>\n",
       "      <td>1.322139</td>\n",
       "      <td>1.529801</td>\n",
       "      <td>1.624691</td>\n",
       "      <td>1.461538</td>\n",
       "      <td>1.528136</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.125</td>\n",
       "      <td>1.125</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.125</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.125</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.125</td>\n",
       "      <td>1.125</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.125</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 167 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   BMDSTATS     BMXWT     BMXHT    BMXBMI    BMXLEG   BMXARML   BMXARMC  \\\n",
       "0       2.0       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "1       1.0  1.155317  1.629843  1.070896  1.380795  1.602469  1.215760   \n",
       "2       1.0  1.035046  1.090684  1.038557       NaN  1.227160  1.067542   \n",
       "3       1.0  1.373955  1.675185  1.322139  1.529801  1.624691  1.461538   \n",
       "4       NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "\n",
       "   BMXWAIST  OHDDESTS   OHX01TC  ...  PUQ100  PUQ110  SMAQUEX2  SMQ860  \\\n",
       "0       NaN       1.0  1.666667  ...     NaN     NaN       NaN   1.000   \n",
       "1  1.161356       1.0  1.666667  ...   1.125   2.000       2.0   1.125   \n",
       "2  1.008136       1.0  1.666667  ...     NaN     NaN       NaN   1.000   \n",
       "3  1.528136       1.0  1.000000  ...   1.125   1.125       1.0   1.125   \n",
       "4       NaN       NaN       NaN  ...     NaN     NaN       1.0   1.000   \n",
       "\n",
       "   SMQ870  SMQ872  SMQ874  SMQ878  SMQ940  SMAQUEX  \n",
       "0     1.0   1.125     1.0   1.000   1.125      2.0  \n",
       "1     1.0   1.125     1.0   1.000   1.125      1.0  \n",
       "2     1.0   1.125     1.0   1.000   1.125      2.0  \n",
       "3     1.0   1.125     1.0   1.125   1.125      1.5  \n",
       "4     1.0   1.125     1.0   1.000   1.000      1.5  \n",
       "\n",
       "[5 rows x 167 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data_minmax = pd.DataFrame(data_scaled_minmax, columns=data[0])\n",
    "df_data_minmax.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split of complete and incomplete cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_complete = df_data_minmax[(df_data_minmax.isna().sum(axis=1) <1)]\n",
    "df_incomplete = df_data_minmax[~(df_data_minmax.isna().sum(axis=1) <1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4825, 167)\n",
      "(10161, 167)\n",
      "(15560, 167)\n"
     ]
    }
   ],
   "source": [
    "print(df_complete.shape)\n",
    "print(df_incomplete.shape)\n",
    "print(df_data_minmax.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "complete_x = df_complete.drop('DIQ010', axis=1).to_numpy()\n",
    "complete_y = df_complete['DIQ010'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_incomplete['DIQ010'].isna().sum()  # 574\n",
    "df_incomplete = df_incomplete[~(df_incomplete['DIQ010'].isna())]   #(10161, 167)\n",
    "\n",
    "incomplete_x = df_incomplete.drop('DIQ010', axis=1).to_numpy()\n",
    "incomplete_y = df_incomplete['DIQ010'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4825, 166)\n",
      "(10161, 166)\n"
     ]
    }
   ],
   "source": [
    "print('Complete Samples: ', complete_x.shape)\n",
    "print('Incomplete Samples: ', incomplete_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting training, validating and testing sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# train : test = 8 : 2 \n",
    "x_train, x_test, y_train, y_test = train_test_split(complete_x, complete_y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# validation ratio = 0.1\n",
    "#x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3860, 166)\n",
      "(965, 166)\n",
      "(3860,)\n",
      "(965,)\n"
     ]
    }
   ],
   "source": [
    "for i in [x_train, x_test, y_train, y_test]:\n",
    "    print(i.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DAE Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dropout layer for corrupting the complete data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 166)]             0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 166)               0         \n",
      "_________________________________________________________________\n",
      "rescaling (Rescaling)        (None, 166)               0         \n",
      "=================================================================\n",
      "Total params: 0\n",
      "Trainable params: 0\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dropout_rate = 0.2\n",
    "seed = 0\n",
    "\n",
    "# corrupt the input with a dropout layer and directly output\n",
    "input_dim = (x_train.shape[1],)\n",
    "inputs = keras.Input(shape=input_dim)\n",
    "noising = keras.layers.Dropout(dropout_rate, seed=seed)(inputs, training=True)\n",
    "outputs = Rescaling(1-dropout_rate)(noising)\n",
    "\n",
    "model_0 = keras.Model(inputs, outputs)\n",
    "model_0.compile()\n",
    "\n",
    "model_0.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the output: corrupted train data\n",
    "inp = model_0.input\n",
    "out = model_0.output\n",
    "get_output = K.function([inp], [out])\n",
    "outputs = get_output([x_train])\n",
    "\n",
    "x_train_corrupted = np.array(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mae:  0.2670860465051908\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics\n",
    "\n",
    "#mse = sklearn.metrics.mean_squared_error(x_train, x_train_corrupted)\n",
    "mae = sklearn.metrics.mean_absolute_error(x_train, x_train_corrupted)\n",
    "print('mae: ', mae)  # The idea that if all the missing value are replaced with zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stacking layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Encoder and Decoder structure of the DAE\n",
    "## 'Number of hidden layers' & 'reduce rate of neurons through each layer' \n",
    "##   will be determined based on the result of grid search\n",
    "\n",
    "def AE(n_hid_layer, r):\n",
    "\n",
    "    input_dim = (x_train.shape[1],)\n",
    "    nodes = int(input_dim[0])\n",
    "\n",
    "    ae = keras.models.Sequential()\n",
    "  # input layer\n",
    "    ae.add(keras.Input(shape=input_dim))\n",
    "  # hidden layers  \n",
    "    if (n_hid_layer %2 == 0):\n",
    "        for i in range(1, (n_hid_layer//2)+1):\n",
    "            ae.add(layers.Dense(nodes*pow(r, i), activation='relu'))\n",
    "        for i in reversed(range(1, (n_hid_layer//2)+1)):\n",
    "            ae.add(layers.Dense(nodes*pow(r, i), activation='relu'))        \n",
    "    else:\n",
    "        for i in range(1, (n_hid_layer//2)+1):\n",
    "            ae.add(layers.Dense(nodes*pow(r, i), activation='relu'))\n",
    "            \n",
    "        ae.add(layers.Dense(nodes*pow(r, (n_hid_layer//2)+1), activation='relu'))        \n",
    "        for i in reversed(range(1, (n_hid_layer//2)+1)):\n",
    "            ae.add(layers.Dense(nodes*pow(r, i), activation='relu'))              \n",
    "     \n",
    "    return ae\n",
    "\n",
    "#AE(5, 0.8).summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# compile model with functional API\n",
    "## in order to activate the dropout layer (introduce noise) in \n",
    "### both training and validation phase\n",
    "\n",
    "def DAE(n_hid_layer=5, reduce_rate=0.8): \n",
    "    inputs = keras.Input(shape=input_dim)\n",
    "    noising = keras.layers.Dropout(0.2)(inputs, training=True)\n",
    "    rescaling = Rescaling(0.8)(noising)\n",
    "  # AE\n",
    "    ae_layers = AE(n_hid_layer, reduce_rate)(rescaling)\n",
    "    outputs = layers.Dense(input_dim[0], activation='linear')(ae_layers)\n",
    "\n",
    "    dae = keras.Model(inputs, outputs)\n",
    "    dae.compile(optimizer='adam', loss='mse', metrics='mae')\n",
    "    return dae\n",
    "\n",
    "#DAE().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters tuning with GridSearchCV\n",
    "\n",
    "models = tf.keras.wrappers.scikit_learn.KerasRegressor(build_fn=DAE)\n",
    "DAE_params = {'batch_size': [40, 60],  'epochs': [500],        \n",
    "              'n_hid_layer' : [3, 5, 6], \n",
    "              'reduce_rate' : [0.8, 1.2, 1.3]}  \n",
    "\n",
    "grid = GridSearchCV(models, DAE_params, cv = 5, scoring='neg_mean_absolute_error') \n",
    "\n",
    "grid_result = grid.fit(x_train, x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best score: {grid_result.best_score_} , best_params: {grid_result.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 'batch_size': [40, 60, 100, 120, 200],  'epochs': [400, 500],        \n",
    "- 'n_hid_layer' : [3, 4, 5, 6, 7], \n",
    "- 'reduce_rate' : [0.8, 0.9, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2, 2.1, 2.2, 2.3, 2.4, 2.5, 2.6, 2.7]\n",
    "\n",
    "- == Best score: -0.04737668 , best_params: {'batch_size': 100, 'epochs': 500, 'n_hid_layer': 3, 'reduce_rate': 2.6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"mean: {mean}, STD: {stdev}, param: {param}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build a model with the best combination of hyperparameters and find the training epochs by early-stopping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir train_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1200\n",
      " 2/35 [>.............................] - ETA: 8s - loss: 1.7149 - mae: 1.2183WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0100s vs `on_train_batch_end` time: 0.4993s). Check your callbacks.\n",
      "35/35 [==============================] - 1s 36ms/step - loss: 0.2100 - mae: 0.2932 - val_loss: 0.0406 - val_mae: 0.1421\n",
      "Epoch 2/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0362 - mae: 0.1302 - val_loss: 0.0341 - val_mae: 0.1237\n",
      "Epoch 3/1200\n",
      "35/35 [==============================] - 1s 15ms/step - loss: 0.0334 - mae: 0.1219 - val_loss: 0.0328 - val_mae: 0.1214\n",
      "Epoch 4/1200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 0.0322 - mae: 0.1188 - val_loss: 0.0315 - val_mae: 0.1174\n",
      "Epoch 5/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0311 - mae: 0.1160 - val_loss: 0.0308 - val_mae: 0.1175\n",
      "Epoch 6/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0302 - mae: 0.1145 - val_loss: 0.0297 - val_mae: 0.1129\n",
      "Epoch 7/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0294 - mae: 0.1117 - val_loss: 0.0291 - val_mae: 0.1082\n",
      "Epoch 8/1200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 0.0286 - mae: 0.1093 - val_loss: 0.0296 - val_mae: 0.1146\n",
      "Epoch 9/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0284 - mae: 0.1091 - val_loss: 0.0278 - val_mae: 0.1063\n",
      "Epoch 10/1200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 0.0278 - mae: 0.1069 - val_loss: 0.0277 - val_mae: 0.1075\n",
      "Epoch 11/1200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 0.0276 - mae: 0.1057 - val_loss: 0.0273 - val_mae: 0.1066\n",
      "Epoch 12/1200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 0.0273 - mae: 0.1051 - val_loss: 0.0270 - val_mae: 0.1043\n",
      "Epoch 13/1200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 0.0271 - mae: 0.1049 - val_loss: 0.0279 - val_mae: 0.1080\n",
      "Epoch 14/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0272 - mae: 0.1052 - val_loss: 0.0268 - val_mae: 0.1051\n",
      "Epoch 15/1200\n",
      "35/35 [==============================] - 1s 19ms/step - loss: 0.0268 - mae: 0.1037 - val_loss: 0.0274 - val_mae: 0.1069\n",
      "Epoch 16/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0267 - mae: 0.1028 - val_loss: 0.0267 - val_mae: 0.1035\n",
      "Epoch 17/1200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 0.0263 - mae: 0.1022 - val_loss: 0.0263 - val_mae: 0.1017\n",
      "Epoch 18/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0261 - mae: 0.1016 - val_loss: 0.0261 - val_mae: 0.1004\n",
      "Epoch 19/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0260 - mae: 0.1013 - val_loss: 0.0259 - val_mae: 0.1008\n",
      "Epoch 20/1200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 0.0258 - mae: 0.1004 - val_loss: 0.0257 - val_mae: 0.1024\n",
      "Epoch 21/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0258 - mae: 0.1010 - val_loss: 0.0256 - val_mae: 0.0989\n",
      "Epoch 22/1200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 0.0255 - mae: 0.1001 - val_loss: 0.0267 - val_mae: 0.1053\n",
      "Epoch 23/1200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 0.0255 - mae: 0.1000 - val_loss: 0.0262 - val_mae: 0.1054\n",
      "Epoch 24/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0254 - mae: 0.0999 - val_loss: 0.0255 - val_mae: 0.0993\n",
      "Epoch 25/1200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 0.0254 - mae: 0.1003 - val_loss: 0.0247 - val_mae: 0.0960\n",
      "Epoch 26/1200\n",
      "35/35 [==============================] - 1s 19ms/step - loss: 0.0250 - mae: 0.0992 - val_loss: 0.0247 - val_mae: 0.0967\n",
      "Epoch 27/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0246 - mae: 0.0978 - val_loss: 0.0242 - val_mae: 0.0957\n",
      "Epoch 28/1200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 0.0241 - mae: 0.0969 - val_loss: 0.0242 - val_mae: 0.0986\n",
      "Epoch 29/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0244 - mae: 0.0987 - val_loss: 0.0240 - val_mae: 0.0970\n",
      "Epoch 30/1200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 0.0237 - mae: 0.0957 - val_loss: 0.0233 - val_mae: 0.0940\n",
      "Epoch 31/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0236 - mae: 0.0960 - val_loss: 0.0233 - val_mae: 0.0942\n",
      "Epoch 32/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0233 - mae: 0.0951 - val_loss: 0.0234 - val_mae: 0.0987\n",
      "Epoch 33/1200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 0.0233 - mae: 0.0952 - val_loss: 0.0237 - val_mae: 0.0951\n",
      "Epoch 34/1200\n",
      "35/35 [==============================] - 1s 19ms/step - loss: 0.0238 - mae: 0.0986 - val_loss: 0.0238 - val_mae: 0.0952\n",
      "Epoch 35/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0232 - mae: 0.0957 - val_loss: 0.0229 - val_mae: 0.0929\n",
      "Epoch 36/1200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 0.0225 - mae: 0.0927 - val_loss: 0.0230 - val_mae: 0.0926\n",
      "Epoch 37/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0226 - mae: 0.0934 - val_loss: 0.0225 - val_mae: 0.0955\n",
      "Epoch 38/1200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 0.0229 - mae: 0.0951 - val_loss: 0.0222 - val_mae: 0.0923\n",
      "Epoch 39/1200\n",
      "35/35 [==============================] - 1s 19ms/step - loss: 0.0234 - mae: 0.0974 - val_loss: 0.0242 - val_mae: 0.1040\n",
      "Epoch 40/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0223 - mae: 0.0926 - val_loss: 0.0222 - val_mae: 0.0900\n",
      "Epoch 41/1200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 0.0220 - mae: 0.0908 - val_loss: 0.0217 - val_mae: 0.0881\n",
      "Epoch 42/1200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 0.0218 - mae: 0.0906 - val_loss: 0.0222 - val_mae: 0.0912\n",
      "Epoch 43/1200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 0.0222 - mae: 0.0920 - val_loss: 0.0219 - val_mae: 0.0911\n",
      "Epoch 44/1200\n",
      "35/35 [==============================] - 1s 19ms/step - loss: 0.0220 - mae: 0.0914 - val_loss: 0.0217 - val_mae: 0.0903\n",
      "Epoch 45/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0216 - mae: 0.0898 - val_loss: 0.0213 - val_mae: 0.0882\n",
      "Epoch 46/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0217 - mae: 0.0898 - val_loss: 0.0220 - val_mae: 0.0923\n",
      "Epoch 47/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0218 - mae: 0.0913 - val_loss: 0.0220 - val_mae: 0.0936\n",
      "Epoch 48/1200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 0.0216 - mae: 0.0899 - val_loss: 0.0215 - val_mae: 0.0902\n",
      "Epoch 49/1200\n",
      "35/35 [==============================] - 1s 19ms/step - loss: 0.0215 - mae: 0.0900 - val_loss: 0.0211 - val_mae: 0.0878\n",
      "Epoch 50/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0215 - mae: 0.0901 - val_loss: 0.0209 - val_mae: 0.0890\n",
      "Epoch 51/1200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 0.0219 - mae: 0.0923 - val_loss: 0.0209 - val_mae: 0.0885\n",
      "Epoch 52/1200\n",
      "35/35 [==============================] - 1s 19ms/step - loss: 0.0211 - mae: 0.0887 - val_loss: 0.0211 - val_mae: 0.0901\n",
      "Epoch 53/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0212 - mae: 0.0893 - val_loss: 0.0222 - val_mae: 0.0943\n",
      "Epoch 54/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0218 - mae: 0.0926 - val_loss: 0.0205 - val_mae: 0.0871\n",
      "Epoch 55/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0209 - mae: 0.0874 - val_loss: 0.0208 - val_mae: 0.0864\n",
      "Epoch 56/1200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 0.0213 - mae: 0.0895 - val_loss: 0.0206 - val_mae: 0.0864\n",
      "Epoch 57/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0208 - mae: 0.0874 - val_loss: 0.0211 - val_mae: 0.0890\n",
      "Epoch 58/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0206 - mae: 0.0868 - val_loss: 0.0221 - val_mae: 0.0938\n",
      "Epoch 59/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0209 - mae: 0.0888 - val_loss: 0.0204 - val_mae: 0.0864\n",
      "Epoch 60/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0210 - mae: 0.0885 - val_loss: 0.0213 - val_mae: 0.0911\n",
      "Epoch 61/1200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 0.0208 - mae: 0.0874 - val_loss: 0.0213 - val_mae: 0.0896\n",
      "Epoch 62/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0205 - mae: 0.0866 - val_loss: 0.0208 - val_mae: 0.0905\n",
      "Epoch 63/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0204 - mae: 0.0867 - val_loss: 0.0198 - val_mae: 0.0844\n",
      "Epoch 64/1200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 0.0209 - mae: 0.0894 - val_loss: 0.0200 - val_mae: 0.0863\n",
      "Epoch 65/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0201 - mae: 0.0855 - val_loss: 0.0200 - val_mae: 0.0851\n",
      "Epoch 66/1200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 0.0200 - mae: 0.0852 - val_loss: 0.0195 - val_mae: 0.0839\n",
      "Epoch 67/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0200 - mae: 0.0856 - val_loss: 0.0206 - val_mae: 0.0886\n",
      "Epoch 68/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0203 - mae: 0.0879 - val_loss: 0.0196 - val_mae: 0.0830\n",
      "Epoch 69/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0200 - mae: 0.0857 - val_loss: 0.0198 - val_mae: 0.0838\n",
      "Epoch 70/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0201 - mae: 0.0869 - val_loss: 0.0203 - val_mae: 0.0862\n",
      "Epoch 71/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0198 - mae: 0.0851 - val_loss: 0.0193 - val_mae: 0.0833\n",
      "Epoch 72/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0193 - mae: 0.0839 - val_loss: 0.0194 - val_mae: 0.0851\n",
      "Epoch 73/1200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 0.0197 - mae: 0.0867 - val_loss: 0.0187 - val_mae: 0.0812\n",
      "Epoch 74/1200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 0.0194 - mae: 0.0841 - val_loss: 0.0191 - val_mae: 0.0838\n",
      "Epoch 75/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0193 - mae: 0.0843 - val_loss: 0.0190 - val_mae: 0.0838\n",
      "Epoch 76/1200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 0.0191 - mae: 0.0843 - val_loss: 0.0184 - val_mae: 0.0795\n",
      "Epoch 77/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0187 - mae: 0.0818 - val_loss: 0.0187 - val_mae: 0.0830\n",
      "Epoch 78/1200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 0.0189 - mae: 0.0839 - val_loss: 0.0184 - val_mae: 0.0815\n",
      "Epoch 79/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0187 - mae: 0.0827 - val_loss: 0.0181 - val_mae: 0.0813\n",
      "Epoch 80/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0188 - mae: 0.0839 - val_loss: 0.0190 - val_mae: 0.0848\n",
      "Epoch 81/1200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 0.0187 - mae: 0.0836 - val_loss: 0.0186 - val_mae: 0.0837\n",
      "Epoch 82/1200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 0.0197 - mae: 0.0886 - val_loss: 0.0193 - val_mae: 0.0856\n",
      "Epoch 83/1200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 0.0184 - mae: 0.0822 - val_loss: 0.0181 - val_mae: 0.0804\n",
      "Epoch 84/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0182 - mae: 0.0809 - val_loss: 0.0179 - val_mae: 0.0793\n",
      "Epoch 85/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0178 - mae: 0.0794 - val_loss: 0.0174 - val_mae: 0.0786\n",
      "Epoch 86/1200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 0.0176 - mae: 0.0788 - val_loss: 0.0174 - val_mae: 0.0783\n",
      "Epoch 87/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0178 - mae: 0.0806 - val_loss: 0.0172 - val_mae: 0.0774\n",
      "Epoch 88/1200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 0.0176 - mae: 0.0798 - val_loss: 0.0174 - val_mae: 0.0780\n",
      "Epoch 89/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0176 - mae: 0.0794 - val_loss: 0.0172 - val_mae: 0.0775\n",
      "Epoch 90/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0176 - mae: 0.0796 - val_loss: 0.0172 - val_mae: 0.0785\n",
      "Epoch 91/1200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 0.0178 - mae: 0.0819 - val_loss: 0.0180 - val_mae: 0.0845\n",
      "Epoch 92/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0176 - mae: 0.0809 - val_loss: 0.0178 - val_mae: 0.0822\n",
      "Epoch 93/1200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 0.0176 - mae: 0.0817 - val_loss: 0.0169 - val_mae: 0.0762\n",
      "Epoch 94/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0172 - mae: 0.0791 - val_loss: 0.0164 - val_mae: 0.0755\n",
      "Epoch 95/1200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 0.0167 - mae: 0.0762 - val_loss: 0.0168 - val_mae: 0.0770\n",
      "Epoch 96/1200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 0.0168 - mae: 0.0771 - val_loss: 0.0169 - val_mae: 0.0774\n",
      "Epoch 97/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0169 - mae: 0.0781 - val_loss: 0.0164 - val_mae: 0.0748\n",
      "Epoch 98/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0166 - mae: 0.0766 - val_loss: 0.0174 - val_mae: 0.0825\n",
      "Epoch 99/1200\n",
      "35/35 [==============================] - 1s 19ms/step - loss: 0.0167 - mae: 0.0774 - val_loss: 0.0159 - val_mae: 0.0737\n",
      "Epoch 100/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0164 - mae: 0.0763 - val_loss: 0.0158 - val_mae: 0.0728\n",
      "Epoch 101/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0159 - mae: 0.0738 - val_loss: 0.0161 - val_mae: 0.0750\n",
      "Epoch 102/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0160 - mae: 0.0753 - val_loss: 0.0156 - val_mae: 0.0725\n",
      "Epoch 103/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0158 - mae: 0.0743 - val_loss: 0.0159 - val_mae: 0.0741\n",
      "Epoch 104/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0163 - mae: 0.0774 - val_loss: 0.0157 - val_mae: 0.0742\n",
      "Epoch 105/1200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 0.0167 - mae: 0.0799 - val_loss: 0.0160 - val_mae: 0.0768\n",
      "Epoch 106/1200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 0.0156 - mae: 0.0736 - val_loss: 0.0154 - val_mae: 0.0727\n",
      "Epoch 107/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0153 - mae: 0.0723 - val_loss: 0.0155 - val_mae: 0.0744\n",
      "Epoch 108/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0155 - mae: 0.0742 - val_loss: 0.0159 - val_mae: 0.0758\n",
      "Epoch 109/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0156 - mae: 0.0749 - val_loss: 0.0155 - val_mae: 0.0748\n",
      "Epoch 110/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0150 - mae: 0.0719 - val_loss: 0.0147 - val_mae: 0.0707\n",
      "Epoch 111/1200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 0.0151 - mae: 0.0724 - val_loss: 0.0153 - val_mae: 0.0730\n",
      "Epoch 112/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0150 - mae: 0.0719 - val_loss: 0.0152 - val_mae: 0.0719\n",
      "Epoch 113/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0149 - mae: 0.0715 - val_loss: 0.0149 - val_mae: 0.0711\n",
      "Epoch 114/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0153 - mae: 0.0751 - val_loss: 0.0148 - val_mae: 0.0717\n",
      "Epoch 115/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0146 - mae: 0.0711 - val_loss: 0.0148 - val_mae: 0.0717\n",
      "Epoch 116/1200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 0.0148 - mae: 0.0720 - val_loss: 0.0144 - val_mae: 0.0705\n",
      "Epoch 117/1200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 0.0147 - mae: 0.0712 - val_loss: 0.0149 - val_mae: 0.0733\n",
      "Epoch 118/1200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 0.0146 - mae: 0.0715 - val_loss: 0.0155 - val_mae: 0.0765\n",
      "Epoch 119/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0145 - mae: 0.0710 - val_loss: 0.0149 - val_mae: 0.0707\n",
      "Epoch 120/1200\n",
      "35/35 [==============================] - 1s 19ms/step - loss: 0.0143 - mae: 0.0697 - val_loss: 0.0146 - val_mae: 0.0722\n",
      "Epoch 121/1200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 1s 19ms/step - loss: 0.0145 - mae: 0.0715 - val_loss: 0.0145 - val_mae: 0.0706\n",
      "Epoch 122/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0146 - mae: 0.0724 - val_loss: 0.0159 - val_mae: 0.0800\n",
      "Epoch 123/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0144 - mae: 0.0721 - val_loss: 0.0148 - val_mae: 0.0735\n",
      "Epoch 124/1200\n",
      "35/35 [==============================] - 1s 19ms/step - loss: 0.0141 - mae: 0.0697 - val_loss: 0.0139 - val_mae: 0.0687\n",
      "Epoch 125/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0140 - mae: 0.0693 - val_loss: 0.0142 - val_mae: 0.0690\n",
      "Epoch 126/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0140 - mae: 0.0695 - val_loss: 0.0140 - val_mae: 0.0705\n",
      "Epoch 127/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0139 - mae: 0.0687 - val_loss: 0.0141 - val_mae: 0.0705\n",
      "Epoch 128/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0140 - mae: 0.0697 - val_loss: 0.0134 - val_mae: 0.0663\n",
      "Epoch 129/1200\n",
      "35/35 [==============================] - 1s 20ms/step - loss: 0.0137 - mae: 0.0677 - val_loss: 0.0136 - val_mae: 0.0671\n",
      "Epoch 130/1200\n",
      "35/35 [==============================] - 1s 19ms/step - loss: 0.0139 - mae: 0.0698 - val_loss: 0.0135 - val_mae: 0.0664\n",
      "Epoch 131/1200\n",
      "35/35 [==============================] - 1s 19ms/step - loss: 0.0137 - mae: 0.0679 - val_loss: 0.0138 - val_mae: 0.0674\n",
      "Epoch 132/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0135 - mae: 0.0676 - val_loss: 0.0132 - val_mae: 0.0663\n",
      "Epoch 133/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0134 - mae: 0.0671 - val_loss: 0.0135 - val_mae: 0.0665\n",
      "Epoch 134/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0136 - mae: 0.0683 - val_loss: 0.0137 - val_mae: 0.0701\n",
      "Epoch 135/1200\n",
      "35/35 [==============================] - 1s 20ms/step - loss: 0.0134 - mae: 0.0670 - val_loss: 0.0140 - val_mae: 0.0708\n",
      "Epoch 136/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0135 - mae: 0.0678 - val_loss: 0.0134 - val_mae: 0.0659\n",
      "Epoch 137/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0134 - mae: 0.0678 - val_loss: 0.0130 - val_mae: 0.0652\n",
      "Epoch 138/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0132 - mae: 0.0661 - val_loss: 0.0136 - val_mae: 0.0687\n",
      "Epoch 139/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0133 - mae: 0.0674 - val_loss: 0.0134 - val_mae: 0.0669\n",
      "Epoch 140/1200\n",
      "35/35 [==============================] - 1s 19ms/step - loss: 0.0130 - mae: 0.0659 - val_loss: 0.0157 - val_mae: 0.0820\n",
      "Epoch 141/1200\n",
      "35/35 [==============================] - 1s 19ms/step - loss: 0.0132 - mae: 0.0674 - val_loss: 0.0131 - val_mae: 0.0655\n",
      "Epoch 142/1200\n",
      "35/35 [==============================] - 1s 19ms/step - loss: 0.0130 - mae: 0.0664 - val_loss: 0.0144 - val_mae: 0.0756\n",
      "Epoch 143/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0132 - mae: 0.0677 - val_loss: 0.0128 - val_mae: 0.0638\n",
      "Epoch 144/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0128 - mae: 0.0651 - val_loss: 0.0136 - val_mae: 0.0683\n",
      "Epoch 145/1200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 0.0130 - mae: 0.0662 - val_loss: 0.0130 - val_mae: 0.0671\n",
      "Epoch 146/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0126 - mae: 0.0645 - val_loss: 0.0128 - val_mae: 0.0639\n",
      "Epoch 147/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0127 - mae: 0.0648 - val_loss: 0.0127 - val_mae: 0.0631\n",
      "Epoch 148/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0126 - mae: 0.0644 - val_loss: 0.0126 - val_mae: 0.0636\n",
      "Epoch 149/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0125 - mae: 0.0640 - val_loss: 0.0126 - val_mae: 0.0649\n",
      "Epoch 150/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0126 - mae: 0.0650 - val_loss: 0.0126 - val_mae: 0.0650\n",
      "Epoch 151/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0126 - mae: 0.0650 - val_loss: 0.0133 - val_mae: 0.0689\n",
      "Epoch 152/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0129 - mae: 0.0662 - val_loss: 0.0133 - val_mae: 0.0691\n",
      "Epoch 153/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0127 - mae: 0.0653 - val_loss: 0.0124 - val_mae: 0.0628\n",
      "Epoch 154/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0125 - mae: 0.0642 - val_loss: 0.0126 - val_mae: 0.0639\n",
      "Epoch 155/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0122 - mae: 0.0627 - val_loss: 0.0128 - val_mae: 0.0645\n",
      "Epoch 156/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0123 - mae: 0.0634 - val_loss: 0.0125 - val_mae: 0.0645\n",
      "Epoch 157/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0122 - mae: 0.0628 - val_loss: 0.0125 - val_mae: 0.0634\n",
      "Epoch 158/1200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 0.0127 - mae: 0.0662 - val_loss: 0.0123 - val_mae: 0.0626\n",
      "Epoch 159/1200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 0.0127 - mae: 0.0662 - val_loss: 0.0124 - val_mae: 0.0630\n",
      "Epoch 160/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0121 - mae: 0.0621 - val_loss: 0.0122 - val_mae: 0.0622\n",
      "Epoch 161/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0123 - mae: 0.0642 - val_loss: 0.0129 - val_mae: 0.0687\n",
      "Epoch 162/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0128 - mae: 0.0675 - val_loss: 0.0123 - val_mae: 0.0613\n",
      "Epoch 163/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0121 - mae: 0.0627 - val_loss: 0.0122 - val_mae: 0.0617\n",
      "Epoch 164/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0123 - mae: 0.0639 - val_loss: 0.0118 - val_mae: 0.0616\n",
      "Epoch 165/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0121 - mae: 0.0633 - val_loss: 0.0124 - val_mae: 0.0635\n",
      "Epoch 166/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0118 - mae: 0.0610 - val_loss: 0.0119 - val_mae: 0.0619\n",
      "Epoch 167/1200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 0.0118 - mae: 0.0614 - val_loss: 0.0120 - val_mae: 0.0618\n",
      "Epoch 168/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0119 - mae: 0.0618 - val_loss: 0.0120 - val_mae: 0.0617\n",
      "Epoch 169/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0120 - mae: 0.0630 - val_loss: 0.0117 - val_mae: 0.0608\n",
      "Epoch 170/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0117 - mae: 0.0607 - val_loss: 0.0120 - val_mae: 0.0614\n",
      "Epoch 171/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0117 - mae: 0.0612 - val_loss: 0.0120 - val_mae: 0.0613\n",
      "Epoch 172/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0117 - mae: 0.0607 - val_loss: 0.0120 - val_mae: 0.0613\n",
      "Epoch 173/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0118 - mae: 0.0617 - val_loss: 0.0125 - val_mae: 0.0661\n",
      "Epoch 174/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0120 - mae: 0.0628 - val_loss: 0.0127 - val_mae: 0.0655\n",
      "Epoch 175/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0116 - mae: 0.0603 - val_loss: 0.0122 - val_mae: 0.0617\n",
      "Epoch 176/1200\n",
      "35/35 [==============================] - 1s 19ms/step - loss: 0.0116 - mae: 0.0611 - val_loss: 0.0116 - val_mae: 0.0604\n",
      "Epoch 177/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0117 - mae: 0.0611 - val_loss: 0.0118 - val_mae: 0.0610\n",
      "Epoch 178/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0117 - mae: 0.0613 - val_loss: 0.0121 - val_mae: 0.0613\n",
      "Epoch 179/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0114 - mae: 0.0602 - val_loss: 0.0119 - val_mae: 0.0613\n",
      "Epoch 180/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0116 - mae: 0.0612 - val_loss: 0.0122 - val_mae: 0.0625\n",
      "Epoch 181/1200\n",
      "35/35 [==============================] - 1s 19ms/step - loss: 0.0113 - mae: 0.0593 - val_loss: 0.0119 - val_mae: 0.0623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 182/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0116 - mae: 0.0617 - val_loss: 0.0119 - val_mae: 0.0608\n",
      "Epoch 183/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0114 - mae: 0.0606 - val_loss: 0.0117 - val_mae: 0.0603\n",
      "Epoch 184/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0113 - mae: 0.0595 - val_loss: 0.0117 - val_mae: 0.0593\n",
      "Epoch 185/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0112 - mae: 0.0591 - val_loss: 0.0115 - val_mae: 0.0589\n",
      "Epoch 186/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0112 - mae: 0.0593 - val_loss: 0.0117 - val_mae: 0.0612\n",
      "Epoch 187/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0115 - mae: 0.0609 - val_loss: 0.0118 - val_mae: 0.0621\n",
      "Epoch 188/1200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 0.0113 - mae: 0.0601 - val_loss: 0.0117 - val_mae: 0.0604\n",
      "Epoch 189/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0112 - mae: 0.0594 - val_loss: 0.0115 - val_mae: 0.0594\n",
      "Epoch 190/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0112 - mae: 0.0595 - val_loss: 0.0116 - val_mae: 0.0596\n",
      "Epoch 191/1200\n",
      "35/35 [==============================] - 1s 22ms/step - loss: 0.0112 - mae: 0.0601 - val_loss: 0.0113 - val_mae: 0.0582\n",
      "Epoch 192/1200\n",
      "35/35 [==============================] - 1s 21ms/step - loss: 0.0112 - mae: 0.0598 - val_loss: 0.0115 - val_mae: 0.0590\n",
      "Epoch 193/1200\n",
      "35/35 [==============================] - 1s 20ms/step - loss: 0.0113 - mae: 0.0598 - val_loss: 0.0114 - val_mae: 0.0591\n",
      "Epoch 194/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0110 - mae: 0.0580 - val_loss: 0.0114 - val_mae: 0.0582\n",
      "Epoch 195/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0110 - mae: 0.0585 - val_loss: 0.0112 - val_mae: 0.0583\n",
      "Epoch 196/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0112 - mae: 0.0596 - val_loss: 0.0114 - val_mae: 0.0585\n",
      "Epoch 197/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0111 - mae: 0.0590 - val_loss: 0.0113 - val_mae: 0.0586\n",
      "Epoch 198/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0110 - mae: 0.0581 - val_loss: 0.0111 - val_mae: 0.0581\n",
      "Epoch 199/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0112 - mae: 0.0598 - val_loss: 0.0113 - val_mae: 0.0592\n",
      "Epoch 200/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0109 - mae: 0.0581 - val_loss: 0.0120 - val_mae: 0.0652\n",
      "Epoch 201/1200\n",
      "35/35 [==============================] - 1s 21ms/step - loss: 0.0111 - mae: 0.0594 - val_loss: 0.0111 - val_mae: 0.0582\n",
      "Epoch 202/1200\n",
      "35/35 [==============================] - 1s 19ms/step - loss: 0.0111 - mae: 0.0596 - val_loss: 0.0114 - val_mae: 0.0585\n",
      "Epoch 203/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0108 - mae: 0.0578 - val_loss: 0.0114 - val_mae: 0.0593\n",
      "Epoch 204/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0109 - mae: 0.0582 - val_loss: 0.0110 - val_mae: 0.0576\n",
      "Epoch 205/1200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 0.0108 - mae: 0.0580 - val_loss: 0.0114 - val_mae: 0.0592\n",
      "Epoch 206/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0109 - mae: 0.0578 - val_loss: 0.0110 - val_mae: 0.0579\n",
      "Epoch 207/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0107 - mae: 0.0573 - val_loss: 0.0106 - val_mae: 0.0563\n",
      "Epoch 208/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0108 - mae: 0.0578 - val_loss: 0.0115 - val_mae: 0.0625\n",
      "Epoch 209/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0108 - mae: 0.0580 - val_loss: 0.0109 - val_mae: 0.0567\n",
      "Epoch 210/1200\n",
      "35/35 [==============================] - 1s 19ms/step - loss: 0.0107 - mae: 0.0576 - val_loss: 0.0114 - val_mae: 0.0606\n",
      "Epoch 211/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0111 - mae: 0.0603 - val_loss: 0.0120 - val_mae: 0.0669\n",
      "Epoch 212/1200\n",
      "35/35 [==============================] - 1s 19ms/step - loss: 0.0108 - mae: 0.0580 - val_loss: 0.0109 - val_mae: 0.0567\n",
      "Epoch 213/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0107 - mae: 0.0576 - val_loss: 0.0113 - val_mae: 0.0607\n",
      "Epoch 214/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0108 - mae: 0.0572 - val_loss: 0.0110 - val_mae: 0.0574\n",
      "Epoch 215/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0107 - mae: 0.0574 - val_loss: 0.0108 - val_mae: 0.0568\n",
      "Epoch 216/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0107 - mae: 0.0573 - val_loss: 0.0110 - val_mae: 0.0573\n",
      "Epoch 217/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0105 - mae: 0.0562 - val_loss: 0.0107 - val_mae: 0.0556\n",
      "Epoch 218/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0105 - mae: 0.0561 - val_loss: 0.0108 - val_mae: 0.0583\n",
      "Epoch 219/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0107 - mae: 0.0568 - val_loss: 0.0110 - val_mae: 0.0577\n",
      "Epoch 220/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0106 - mae: 0.0573 - val_loss: 0.0107 - val_mae: 0.0570\n",
      "Epoch 221/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0105 - mae: 0.0563 - val_loss: 0.0109 - val_mae: 0.0578\n",
      "Epoch 222/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0105 - mae: 0.0558 - val_loss: 0.0109 - val_mae: 0.0566\n",
      "Epoch 223/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0105 - mae: 0.0565 - val_loss: 0.0113 - val_mae: 0.0614\n",
      "Epoch 224/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0107 - mae: 0.0584 - val_loss: 0.0115 - val_mae: 0.0642\n",
      "Epoch 225/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0105 - mae: 0.0568 - val_loss: 0.0107 - val_mae: 0.0551\n",
      "Epoch 226/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0105 - mae: 0.0562 - val_loss: 0.0104 - val_mae: 0.0554\n",
      "Epoch 227/1200\n",
      "35/35 [==============================] - 1s 20ms/step - loss: 0.0104 - mae: 0.0561 - val_loss: 0.0110 - val_mae: 0.0575\n",
      "Epoch 228/1200\n",
      "35/35 [==============================] - 1s 19ms/step - loss: 0.0108 - mae: 0.0595 - val_loss: 0.0111 - val_mae: 0.0603\n",
      "Epoch 229/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0105 - mae: 0.0569 - val_loss: 0.0106 - val_mae: 0.0570\n",
      "Epoch 230/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0104 - mae: 0.0566 - val_loss: 0.0112 - val_mae: 0.0598\n",
      "Epoch 231/1200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 0.0105 - mae: 0.0566 - val_loss: 0.0108 - val_mae: 0.0578\n",
      "Epoch 232/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0103 - mae: 0.0561 - val_loss: 0.0103 - val_mae: 0.0545\n",
      "Epoch 233/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0102 - mae: 0.0552 - val_loss: 0.0107 - val_mae: 0.0558\n",
      "Epoch 234/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0103 - mae: 0.0553 - val_loss: 0.0103 - val_mae: 0.0547\n",
      "Epoch 235/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0103 - mae: 0.0564 - val_loss: 0.0105 - val_mae: 0.0555\n",
      "Epoch 236/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0102 - mae: 0.0548 - val_loss: 0.0104 - val_mae: 0.0548\n",
      "Epoch 237/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0102 - mae: 0.0550 - val_loss: 0.0104 - val_mae: 0.0551\n",
      "Epoch 238/1200\n",
      "35/35 [==============================] - 1s 21ms/step - loss: 0.0102 - mae: 0.0553 - val_loss: 0.0105 - val_mae: 0.0574\n",
      "Epoch 239/1200\n",
      "35/35 [==============================] - 1s 20ms/step - loss: 0.0102 - mae: 0.0555 - val_loss: 0.0109 - val_mae: 0.0557\n",
      "Epoch 240/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0102 - mae: 0.0546 - val_loss: 0.0104 - val_mae: 0.0549\n",
      "Epoch 241/1200\n",
      "35/35 [==============================] - 1s 19ms/step - loss: 0.0101 - mae: 0.0548 - val_loss: 0.0102 - val_mae: 0.0549\n",
      "Epoch 242/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0101 - mae: 0.0545 - val_loss: 0.0106 - val_mae: 0.0548\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 243/1200\n",
      "35/35 [==============================] - 1s 19ms/step - loss: 0.0101 - mae: 0.0541 - val_loss: 0.0105 - val_mae: 0.0548\n",
      "Epoch 244/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0101 - mae: 0.0542 - val_loss: 0.0105 - val_mae: 0.0560\n",
      "Epoch 245/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0101 - mae: 0.0550 - val_loss: 0.0106 - val_mae: 0.0551\n",
      "Epoch 246/1200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 0.0102 - mae: 0.0554 - val_loss: 0.0103 - val_mae: 0.0547\n",
      "Epoch 247/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0102 - mae: 0.0553 - val_loss: 0.0106 - val_mae: 0.0583\n",
      "Epoch 248/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0101 - mae: 0.0549 - val_loss: 0.0105 - val_mae: 0.0548\n",
      "Epoch 249/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0102 - mae: 0.0554 - val_loss: 0.0105 - val_mae: 0.0556\n",
      "Epoch 250/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0100 - mae: 0.0538 - val_loss: 0.0101 - val_mae: 0.0538\n",
      "Epoch 251/1200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 0.0103 - mae: 0.0555 - val_loss: 0.0104 - val_mae: 0.0558\n",
      "Epoch 252/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0101 - mae: 0.0547 - val_loss: 0.0103 - val_mae: 0.0538\n",
      "Epoch 253/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0098 - mae: 0.0535 - val_loss: 0.0102 - val_mae: 0.0538\n",
      "Epoch 254/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0100 - mae: 0.0538 - val_loss: 0.0101 - val_mae: 0.0535\n",
      "Epoch 255/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0101 - mae: 0.0550 - val_loss: 0.0105 - val_mae: 0.0545\n",
      "Epoch 256/1200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 0.0100 - mae: 0.0540 - val_loss: 0.0104 - val_mae: 0.0559\n",
      "Epoch 257/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0104 - mae: 0.0580 - val_loss: 0.0122 - val_mae: 0.0691\n",
      "Epoch 258/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0102 - mae: 0.0554 - val_loss: 0.0101 - val_mae: 0.0533\n",
      "Epoch 259/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0099 - mae: 0.0542 - val_loss: 0.0105 - val_mae: 0.0568\n",
      "Epoch 260/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0101 - mae: 0.0557 - val_loss: 0.0105 - val_mae: 0.0579\n",
      "Epoch 261/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0100 - mae: 0.0548 - val_loss: 0.0104 - val_mae: 0.0569\n",
      "Epoch 262/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0099 - mae: 0.0539 - val_loss: 0.0103 - val_mae: 0.0541\n",
      "Epoch 263/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0098 - mae: 0.0534 - val_loss: 0.0108 - val_mae: 0.0576\n",
      "Epoch 264/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0101 - mae: 0.0556 - val_loss: 0.0106 - val_mae: 0.0559\n",
      "Epoch 265/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0099 - mae: 0.0536 - val_loss: 0.0102 - val_mae: 0.0537\n",
      "Epoch 266/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0098 - mae: 0.0528 - val_loss: 0.0099 - val_mae: 0.0527\n",
      "Epoch 267/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0098 - mae: 0.0528 - val_loss: 0.0103 - val_mae: 0.0555\n",
      "Epoch 268/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0099 - mae: 0.0542 - val_loss: 0.0101 - val_mae: 0.0542\n",
      "Epoch 269/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0098 - mae: 0.0529 - val_loss: 0.0102 - val_mae: 0.0545\n",
      "Epoch 270/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0099 - mae: 0.0539 - val_loss: 0.0099 - val_mae: 0.0533\n",
      "Epoch 271/1200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 0.0099 - mae: 0.0543 - val_loss: 0.0102 - val_mae: 0.0534\n",
      "Epoch 272/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0097 - mae: 0.0528 - val_loss: 0.0104 - val_mae: 0.0568\n",
      "Epoch 273/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0098 - mae: 0.0542 - val_loss: 0.0103 - val_mae: 0.0553\n",
      "Epoch 274/1200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 0.0097 - mae: 0.0526 - val_loss: 0.0100 - val_mae: 0.0539\n",
      "Epoch 275/1200\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.0097 - mae: 0.052 - 1s 17ms/step - loss: 0.0097 - mae: 0.0527 - val_loss: 0.0102 - val_mae: 0.0544\n",
      "Epoch 276/1200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 0.0097 - mae: 0.0529 - val_loss: 0.0101 - val_mae: 0.0534\n",
      "Epoch 277/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0097 - mae: 0.0523 - val_loss: 0.0101 - val_mae: 0.0542\n",
      "Epoch 278/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0098 - mae: 0.0524 - val_loss: 0.0101 - val_mae: 0.0548\n",
      "Epoch 279/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0097 - mae: 0.0524 - val_loss: 0.0101 - val_mae: 0.0526\n",
      "Epoch 280/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0095 - mae: 0.0522 - val_loss: 0.0100 - val_mae: 0.0528\n",
      "Epoch 281/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0097 - mae: 0.0530 - val_loss: 0.0102 - val_mae: 0.0532\n",
      "Epoch 282/1200\n",
      "35/35 [==============================] - 1s 19ms/step - loss: 0.0097 - mae: 0.0526 - val_loss: 0.0100 - val_mae: 0.0536\n",
      "Epoch 283/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0099 - mae: 0.0542 - val_loss: 0.0100 - val_mae: 0.0532\n",
      "Epoch 284/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0098 - mae: 0.0530 - val_loss: 0.0099 - val_mae: 0.0537\n",
      "Epoch 285/1200\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.0097 - mae: 0.052 - 1s 18ms/step - loss: 0.0097 - mae: 0.0529 - val_loss: 0.0102 - val_mae: 0.0545\n",
      "Epoch 286/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0096 - mae: 0.0526 - val_loss: 0.0098 - val_mae: 0.0525\n",
      "Epoch 287/1200\n",
      "35/35 [==============================] - 1s 19ms/step - loss: 0.0097 - mae: 0.0526 - val_loss: 0.0098 - val_mae: 0.0529\n",
      "Epoch 288/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0098 - mae: 0.0531 - val_loss: 0.0102 - val_mae: 0.0550\n",
      "Epoch 289/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0096 - mae: 0.0518 - val_loss: 0.0099 - val_mae: 0.0526\n",
      "Epoch 290/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0098 - mae: 0.0531 - val_loss: 0.0098 - val_mae: 0.0527\n",
      "Epoch 291/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0097 - mae: 0.0530 - val_loss: 0.0098 - val_mae: 0.0525\n",
      "Epoch 292/1200\n",
      "35/35 [==============================] - 1s 19ms/step - loss: 0.0097 - mae: 0.0525 - val_loss: 0.0099 - val_mae: 0.0529\n",
      "Epoch 293/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0096 - mae: 0.0518 - val_loss: 0.0099 - val_mae: 0.0524\n",
      "Epoch 294/1200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 0.0096 - mae: 0.0524 - val_loss: 0.0099 - val_mae: 0.0515\n",
      "Epoch 295/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0095 - mae: 0.0516 - val_loss: 0.0098 - val_mae: 0.0525\n",
      "Epoch 296/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0096 - mae: 0.0517 - val_loss: 0.0100 - val_mae: 0.0526\n",
      "Epoch 297/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0097 - mae: 0.0525 - val_loss: 0.0098 - val_mae: 0.0525\n",
      "Epoch 298/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0096 - mae: 0.0519 - val_loss: 0.0100 - val_mae: 0.0528\n",
      "Epoch 299/1200\n",
      "35/35 [==============================] - 1s 19ms/step - loss: 0.0096 - mae: 0.0524 - val_loss: 0.0103 - val_mae: 0.0555\n",
      "Epoch 300/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0095 - mae: 0.0519 - val_loss: 0.0099 - val_mae: 0.0519\n",
      "Epoch 301/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0095 - mae: 0.0516 - val_loss: 0.0100 - val_mae: 0.0547\n",
      "Epoch 302/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0097 - mae: 0.0530 - val_loss: 0.0103 - val_mae: 0.0570\n",
      "Epoch 303/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0096 - mae: 0.0526 - val_loss: 0.0099 - val_mae: 0.0528\n",
      "Epoch 304/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0096 - mae: 0.0525 - val_loss: 0.0102 - val_mae: 0.0550\n",
      "Epoch 305/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0095 - mae: 0.0514 - val_loss: 0.0099 - val_mae: 0.0521\n",
      "Epoch 306/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0096 - mae: 0.0517 - val_loss: 0.0098 - val_mae: 0.0527\n",
      "Epoch 307/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0096 - mae: 0.0519 - val_loss: 0.0096 - val_mae: 0.0512\n",
      "Epoch 308/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0094 - mae: 0.0514 - val_loss: 0.0098 - val_mae: 0.0512\n",
      "Epoch 309/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0096 - mae: 0.0514 - val_loss: 0.0099 - val_mae: 0.0528\n",
      "Epoch 310/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0095 - mae: 0.0517 - val_loss: 0.0097 - val_mae: 0.0522\n",
      "Epoch 311/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0094 - mae: 0.0511 - val_loss: 0.0099 - val_mae: 0.0536\n",
      "Epoch 312/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0095 - mae: 0.0517 - val_loss: 0.0100 - val_mae: 0.0533\n",
      "Epoch 313/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0095 - mae: 0.0521 - val_loss: 0.0101 - val_mae: 0.0545\n",
      "Epoch 314/1200\n",
      "35/35 [==============================] - 1s 19ms/step - loss: 0.0095 - mae: 0.0517 - val_loss: 0.0097 - val_mae: 0.0515\n",
      "Epoch 315/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0095 - mae: 0.0512 - val_loss: 0.0099 - val_mae: 0.0524\n",
      "Epoch 316/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0093 - mae: 0.0505 - val_loss: 0.0098 - val_mae: 0.0517\n",
      "Epoch 317/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0096 - mae: 0.0524 - val_loss: 0.0100 - val_mae: 0.0521\n",
      "Epoch 318/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0094 - mae: 0.0510 - val_loss: 0.0100 - val_mae: 0.0549\n",
      "Epoch 319/1200\n",
      "35/35 [==============================] - 1s 19ms/step - loss: 0.0096 - mae: 0.0525 - val_loss: 0.0102 - val_mae: 0.0564\n",
      "Epoch 320/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0094 - mae: 0.0513 - val_loss: 0.0099 - val_mae: 0.0527\n",
      "Epoch 321/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0094 - mae: 0.0512 - val_loss: 0.0100 - val_mae: 0.0530\n",
      "Epoch 322/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0095 - mae: 0.0519 - val_loss: 0.0099 - val_mae: 0.0542\n",
      "Epoch 323/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0095 - mae: 0.0516 - val_loss: 0.0101 - val_mae: 0.0526\n",
      "Epoch 324/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0095 - mae: 0.0518 - val_loss: 0.0101 - val_mae: 0.0553\n",
      "Epoch 325/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0094 - mae: 0.0514 - val_loss: 0.0097 - val_mae: 0.0511\n",
      "Epoch 326/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0094 - mae: 0.0507 - val_loss: 0.0098 - val_mae: 0.0515\n",
      "Epoch 327/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0095 - mae: 0.0524 - val_loss: 0.0099 - val_mae: 0.0538\n",
      "Epoch 328/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0095 - mae: 0.0520 - val_loss: 0.0102 - val_mae: 0.0532\n",
      "Epoch 329/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0095 - mae: 0.0516 - val_loss: 0.0096 - val_mae: 0.0515\n",
      "Epoch 330/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0095 - mae: 0.0508 - val_loss: 0.0096 - val_mae: 0.0503\n",
      "Epoch 331/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0094 - mae: 0.0513 - val_loss: 0.0099 - val_mae: 0.0520\n",
      "Epoch 332/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0094 - mae: 0.0506 - val_loss: 0.0097 - val_mae: 0.0507\n",
      "Epoch 333/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0094 - mae: 0.0511 - val_loss: 0.0095 - val_mae: 0.0504\n",
      "Epoch 334/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0095 - mae: 0.0520 - val_loss: 0.0098 - val_mae: 0.0529\n",
      "Epoch 335/1200\n",
      "35/35 [==============================] - 1s 19ms/step - loss: 0.0095 - mae: 0.0516 - val_loss: 0.0099 - val_mae: 0.0528\n",
      "Epoch 336/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0093 - mae: 0.0510 - val_loss: 0.0099 - val_mae: 0.0522\n",
      "Epoch 337/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0094 - mae: 0.0513 - val_loss: 0.0096 - val_mae: 0.0518\n",
      "Epoch 338/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0094 - mae: 0.0509 - val_loss: 0.0096 - val_mae: 0.0504\n",
      "Epoch 339/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0094 - mae: 0.0516 - val_loss: 0.0098 - val_mae: 0.0501\n",
      "Epoch 340/1200\n",
      "35/35 [==============================] - 1s 19ms/step - loss: 0.0093 - mae: 0.0503 - val_loss: 0.0096 - val_mae: 0.0513\n",
      "Epoch 341/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0093 - mae: 0.0503 - val_loss: 0.0097 - val_mae: 0.0515\n",
      "Epoch 342/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0093 - mae: 0.0505 - val_loss: 0.0095 - val_mae: 0.0513\n",
      "Epoch 343/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0092 - mae: 0.0498 - val_loss: 0.0097 - val_mae: 0.0510\n",
      "Epoch 344/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0092 - mae: 0.0503 - val_loss: 0.0098 - val_mae: 0.0531\n",
      "Epoch 345/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0093 - mae: 0.0508 - val_loss: 0.0101 - val_mae: 0.0541\n",
      "Epoch 346/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0094 - mae: 0.0522 - val_loss: 0.0096 - val_mae: 0.0515\n",
      "Epoch 347/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0093 - mae: 0.0510 - val_loss: 0.0101 - val_mae: 0.0547\n",
      "Epoch 348/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0094 - mae: 0.0514 - val_loss: 0.0097 - val_mae: 0.0523\n",
      "Epoch 349/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0093 - mae: 0.0506 - val_loss: 0.0096 - val_mae: 0.0519\n",
      "Epoch 350/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0093 - mae: 0.0513 - val_loss: 0.0092 - val_mae: 0.0507\n",
      "Epoch 351/1200\n",
      "35/35 [==============================] - 1s 19ms/step - loss: 0.0092 - mae: 0.0503 - val_loss: 0.0095 - val_mae: 0.0506\n",
      "Epoch 352/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0093 - mae: 0.0504 - val_loss: 0.0098 - val_mae: 0.0510\n",
      "Epoch 353/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0091 - mae: 0.0499 - val_loss: 0.0094 - val_mae: 0.0506\n",
      "Epoch 354/1200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 0.0091 - mae: 0.0498 - val_loss: 0.0096 - val_mae: 0.0505\n",
      "Epoch 355/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0091 - mae: 0.0503 - val_loss: 0.0096 - val_mae: 0.0509\n",
      "Epoch 356/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0090 - mae: 0.0495 - val_loss: 0.0094 - val_mae: 0.0504\n",
      "Epoch 357/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0091 - mae: 0.0500 - val_loss: 0.0096 - val_mae: 0.0518\n",
      "Epoch 358/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0091 - mae: 0.0497 - val_loss: 0.0096 - val_mae: 0.0508\n",
      "Epoch 359/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0093 - mae: 0.0519 - val_loss: 0.0094 - val_mae: 0.0504\n",
      "Epoch 360/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0090 - mae: 0.0496 - val_loss: 0.0096 - val_mae: 0.0519\n",
      "Epoch 361/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0091 - mae: 0.0501 - val_loss: 0.0094 - val_mae: 0.0506\n",
      "Epoch 362/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0091 - mae: 0.0507 - val_loss: 0.0096 - val_mae: 0.0521\n",
      "Epoch 363/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0091 - mae: 0.0501 - val_loss: 0.0095 - val_mae: 0.0512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 364/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0090 - mae: 0.0498 - val_loss: 0.0095 - val_mae: 0.0508\n",
      "Epoch 365/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0090 - mae: 0.0495 - val_loss: 0.0093 - val_mae: 0.0499\n",
      "Epoch 366/1200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 0.0091 - mae: 0.0502 - val_loss: 0.0094 - val_mae: 0.0502\n",
      "Epoch 367/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0091 - mae: 0.0498 - val_loss: 0.0094 - val_mae: 0.0504\n",
      "Epoch 368/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0090 - mae: 0.0495 - val_loss: 0.0096 - val_mae: 0.0518\n",
      "Epoch 369/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0090 - mae: 0.0494 - val_loss: 0.0091 - val_mae: 0.0490\n",
      "Epoch 370/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0089 - mae: 0.0492 - val_loss: 0.0093 - val_mae: 0.0497\n",
      "Epoch 371/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0091 - mae: 0.0515 - val_loss: 0.0094 - val_mae: 0.0511\n",
      "Epoch 372/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0090 - mae: 0.0497 - val_loss: 0.0093 - val_mae: 0.0508\n",
      "Epoch 373/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0090 - mae: 0.0495 - val_loss: 0.0091 - val_mae: 0.0499\n",
      "Epoch 374/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0088 - mae: 0.0490 - val_loss: 0.0095 - val_mae: 0.0509\n",
      "Epoch 375/1200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 0.0089 - mae: 0.0496 - val_loss: 0.0092 - val_mae: 0.0499\n",
      "Epoch 376/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0091 - mae: 0.0498 - val_loss: 0.0093 - val_mae: 0.0499\n",
      "Epoch 377/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0089 - mae: 0.0493 - val_loss: 0.0092 - val_mae: 0.0498\n",
      "Epoch 378/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0088 - mae: 0.0489 - val_loss: 0.0093 - val_mae: 0.0511\n",
      "Epoch 379/1200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 0.0088 - mae: 0.0490 - val_loss: 0.0090 - val_mae: 0.0496\n",
      "Epoch 380/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0088 - mae: 0.0489 - val_loss: 0.0090 - val_mae: 0.0494\n",
      "Epoch 381/1200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 0.0088 - mae: 0.0491 - val_loss: 0.0090 - val_mae: 0.0491\n",
      "Epoch 382/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0089 - mae: 0.0493 - val_loss: 0.0092 - val_mae: 0.0495\n",
      "Epoch 383/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0089 - mae: 0.0491 - val_loss: 0.0093 - val_mae: 0.0493\n",
      "Epoch 384/1200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 0.0088 - mae: 0.0487 - val_loss: 0.0092 - val_mae: 0.0508\n",
      "Epoch 385/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0090 - mae: 0.0502 - val_loss: 0.0091 - val_mae: 0.0496\n",
      "Epoch 386/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0087 - mae: 0.0483 - val_loss: 0.0091 - val_mae: 0.0498\n",
      "Epoch 387/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0088 - mae: 0.0490 - val_loss: 0.0093 - val_mae: 0.0508\n",
      "Epoch 388/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0088 - mae: 0.0489 - val_loss: 0.0090 - val_mae: 0.0490\n",
      "Epoch 389/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0088 - mae: 0.0493 - val_loss: 0.0094 - val_mae: 0.0516\n",
      "Epoch 390/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0089 - mae: 0.0498 - val_loss: 0.0091 - val_mae: 0.0495\n",
      "Epoch 391/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0087 - mae: 0.0485 - val_loss: 0.0092 - val_mae: 0.0499\n",
      "Epoch 392/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0087 - mae: 0.0487 - val_loss: 0.0090 - val_mae: 0.0491\n",
      "Epoch 393/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0088 - mae: 0.0486 - val_loss: 0.0093 - val_mae: 0.0514\n",
      "Epoch 394/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0088 - mae: 0.0490 - val_loss: 0.0093 - val_mae: 0.0506\n",
      "Epoch 395/1200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 0.0088 - mae: 0.0493 - val_loss: 0.0088 - val_mae: 0.0488\n",
      "Epoch 396/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0086 - mae: 0.0478 - val_loss: 0.0089 - val_mae: 0.0490\n",
      "Epoch 397/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0087 - mae: 0.0486 - val_loss: 0.0088 - val_mae: 0.0477\n",
      "Epoch 398/1200\n",
      "35/35 [==============================] - 1s 19ms/step - loss: 0.0086 - mae: 0.0485 - val_loss: 0.0089 - val_mae: 0.0495\n",
      "Epoch 399/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0093 - mae: 0.0532 - val_loss: 0.0092 - val_mae: 0.0506\n",
      "Epoch 400/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0086 - mae: 0.0487 - val_loss: 0.0088 - val_mae: 0.0479\n",
      "Epoch 401/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0086 - mae: 0.0481 - val_loss: 0.0091 - val_mae: 0.0489\n",
      "Epoch 402/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0087 - mae: 0.0482 - val_loss: 0.0093 - val_mae: 0.0501\n",
      "Epoch 403/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0086 - mae: 0.0481 - val_loss: 0.0090 - val_mae: 0.0498\n",
      "Epoch 404/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0086 - mae: 0.0483 - val_loss: 0.0089 - val_mae: 0.0491\n",
      "Epoch 405/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0086 - mae: 0.0476 - val_loss: 0.0090 - val_mae: 0.0506\n",
      "Epoch 406/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0086 - mae: 0.0482 - val_loss: 0.0090 - val_mae: 0.0496\n",
      "Epoch 407/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0086 - mae: 0.0484 - val_loss: 0.0092 - val_mae: 0.0496\n",
      "Epoch 408/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0087 - mae: 0.0491 - val_loss: 0.0090 - val_mae: 0.0497\n",
      "Epoch 409/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0087 - mae: 0.0483 - val_loss: 0.0090 - val_mae: 0.0492\n",
      "Epoch 410/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0086 - mae: 0.0478 - val_loss: 0.0090 - val_mae: 0.0494\n",
      "Epoch 411/1200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 0.0086 - mae: 0.0479 - val_loss: 0.0088 - val_mae: 0.0480\n",
      "Epoch 412/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0085 - mae: 0.0478 - val_loss: 0.0088 - val_mae: 0.0479\n",
      "Epoch 413/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0086 - mae: 0.0477 - val_loss: 0.0087 - val_mae: 0.0480\n",
      "Epoch 414/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0085 - mae: 0.0472 - val_loss: 0.0089 - val_mae: 0.0484\n",
      "Epoch 415/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0086 - mae: 0.0477 - val_loss: 0.0090 - val_mae: 0.0504\n",
      "Epoch 416/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0086 - mae: 0.0479 - val_loss: 0.0090 - val_mae: 0.0494\n",
      "Epoch 417/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0086 - mae: 0.0478 - val_loss: 0.0090 - val_mae: 0.0481\n",
      "Epoch 418/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0086 - mae: 0.0482 - val_loss: 0.0090 - val_mae: 0.0493\n",
      "Epoch 419/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0087 - mae: 0.0485 - val_loss: 0.0090 - val_mae: 0.0489\n",
      "Epoch 420/1200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 0.0084 - mae: 0.0472 - val_loss: 0.0090 - val_mae: 0.0498\n",
      "Epoch 421/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0086 - mae: 0.0480 - val_loss: 0.0090 - val_mae: 0.0491\n",
      "Epoch 422/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0084 - mae: 0.0473 - val_loss: 0.0086 - val_mae: 0.0478\n",
      "Epoch 423/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0084 - mae: 0.0472 - val_loss: 0.0089 - val_mae: 0.0491\n",
      "Epoch 424/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0085 - mae: 0.0479 - val_loss: 0.0090 - val_mae: 0.0492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 425/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0086 - mae: 0.0482 - val_loss: 0.0088 - val_mae: 0.0480\n",
      "Epoch 426/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0085 - mae: 0.0479 - val_loss: 0.0087 - val_mae: 0.0480\n",
      "Epoch 427/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0083 - mae: 0.0473 - val_loss: 0.0087 - val_mae: 0.0483\n",
      "Epoch 428/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0084 - mae: 0.0471 - val_loss: 0.0088 - val_mae: 0.0475\n",
      "Epoch 429/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0084 - mae: 0.0471 - val_loss: 0.0086 - val_mae: 0.0472\n",
      "Epoch 430/1200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 0.0083 - mae: 0.0464 - val_loss: 0.0087 - val_mae: 0.0473\n",
      "Epoch 431/1200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 0.0084 - mae: 0.0473 - val_loss: 0.0089 - val_mae: 0.0482\n",
      "Epoch 432/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0085 - mae: 0.0475 - val_loss: 0.0091 - val_mae: 0.0504\n",
      "Epoch 433/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0087 - mae: 0.0486 - val_loss: 0.0087 - val_mae: 0.0475\n",
      "Epoch 434/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0086 - mae: 0.0482 - val_loss: 0.0089 - val_mae: 0.0489\n",
      "Epoch 435/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0084 - mae: 0.0474 - val_loss: 0.0086 - val_mae: 0.0480\n",
      "Epoch 436/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0085 - mae: 0.0475 - val_loss: 0.0087 - val_mae: 0.0483\n",
      "Epoch 437/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0084 - mae: 0.0469 - val_loss: 0.0089 - val_mae: 0.0507\n",
      "Epoch 438/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0085 - mae: 0.0474 - val_loss: 0.0087 - val_mae: 0.0487\n",
      "Epoch 439/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0086 - mae: 0.0481 - val_loss: 0.0089 - val_mae: 0.0490\n",
      "Epoch 440/1200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 0.0086 - mae: 0.0489 - val_loss: 0.0090 - val_mae: 0.0513\n",
      "Epoch 441/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0084 - mae: 0.0469 - val_loss: 0.0086 - val_mae: 0.0480\n",
      "Epoch 442/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0084 - mae: 0.0475 - val_loss: 0.0089 - val_mae: 0.0516\n",
      "Epoch 443/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0084 - mae: 0.0474 - val_loss: 0.0089 - val_mae: 0.0492\n",
      "Epoch 444/1200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 0.0083 - mae: 0.0468 - val_loss: 0.0085 - val_mae: 0.0472\n",
      "Epoch 445/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0084 - mae: 0.0471 - val_loss: 0.0088 - val_mae: 0.0480\n",
      "Epoch 446/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0083 - mae: 0.0468 - val_loss: 0.0088 - val_mae: 0.0496\n",
      "Epoch 447/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0083 - mae: 0.0468 - val_loss: 0.0085 - val_mae: 0.0474\n",
      "Epoch 448/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0082 - mae: 0.0464 - val_loss: 0.0086 - val_mae: 0.0461\n",
      "Epoch 449/1200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 0.0082 - mae: 0.0461 - val_loss: 0.0086 - val_mae: 0.0471\n",
      "Epoch 450/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0084 - mae: 0.0472 - val_loss: 0.0089 - val_mae: 0.0481\n",
      "Epoch 451/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0082 - mae: 0.0464 - val_loss: 0.0088 - val_mae: 0.0481\n",
      "Epoch 452/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0083 - mae: 0.0470 - val_loss: 0.0087 - val_mae: 0.0480\n",
      "Epoch 453/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0084 - mae: 0.0471 - val_loss: 0.0086 - val_mae: 0.0470\n",
      "Epoch 454/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0083 - mae: 0.0466 - val_loss: 0.0087 - val_mae: 0.0487\n",
      "Epoch 455/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0083 - mae: 0.0465 - val_loss: 0.0087 - val_mae: 0.0478\n",
      "Epoch 456/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0082 - mae: 0.0462 - val_loss: 0.0086 - val_mae: 0.0482\n",
      "Epoch 457/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0083 - mae: 0.0465 - val_loss: 0.0086 - val_mae: 0.0471\n",
      "Epoch 458/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0084 - mae: 0.0470 - val_loss: 0.0087 - val_mae: 0.0474\n",
      "Epoch 459/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0082 - mae: 0.0457 - val_loss: 0.0085 - val_mae: 0.0467\n",
      "Epoch 460/1200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 0.0083 - mae: 0.0461 - val_loss: 0.0088 - val_mae: 0.0482\n",
      "Epoch 461/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0082 - mae: 0.0462 - val_loss: 0.0085 - val_mae: 0.0465\n",
      "Epoch 462/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0082 - mae: 0.0464 - val_loss: 0.0084 - val_mae: 0.0471\n",
      "Epoch 463/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0083 - mae: 0.0471 - val_loss: 0.0087 - val_mae: 0.0473\n",
      "Epoch 464/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0083 - mae: 0.0463 - val_loss: 0.0085 - val_mae: 0.0474\n",
      "Epoch 465/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0084 - mae: 0.0471 - val_loss: 0.0086 - val_mae: 0.0474\n",
      "Epoch 466/1200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 0.0083 - mae: 0.0471 - val_loss: 0.0084 - val_mae: 0.0466\n",
      "Epoch 467/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0083 - mae: 0.0466 - val_loss: 0.0092 - val_mae: 0.0512\n",
      "Epoch 468/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0082 - mae: 0.0465 - val_loss: 0.0085 - val_mae: 0.0470\n",
      "Epoch 469/1200\n",
      "35/35 [==============================] - 1s 19ms/step - loss: 0.0083 - mae: 0.0466 - val_loss: 0.0091 - val_mae: 0.0480\n",
      "Epoch 470/1200\n",
      "35/35 [==============================] - 1s 20ms/step - loss: 0.0083 - mae: 0.0466 - val_loss: 0.0086 - val_mae: 0.0463\n",
      "Epoch 471/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0084 - mae: 0.0469 - val_loss: 0.0086 - val_mae: 0.0471\n",
      "Epoch 472/1200\n",
      "35/35 [==============================] - 1s 23ms/step - loss: 0.0082 - mae: 0.0462 - val_loss: 0.0085 - val_mae: 0.0474\n",
      "Epoch 473/1200\n",
      "35/35 [==============================] - 1s 20ms/step - loss: 0.0083 - mae: 0.0468 - val_loss: 0.0089 - val_mae: 0.0480\n",
      "Epoch 474/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0082 - mae: 0.0458 - val_loss: 0.0084 - val_mae: 0.0459\n",
      "Epoch 475/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0081 - mae: 0.0456 - val_loss: 0.0083 - val_mae: 0.0465\n",
      "Epoch 476/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0084 - mae: 0.0470 - val_loss: 0.0088 - val_mae: 0.0484\n",
      "Epoch 477/1200\n",
      "35/35 [==============================] - 1s 19ms/step - loss: 0.0083 - mae: 0.0463 - val_loss: 0.0086 - val_mae: 0.0468\n",
      "Epoch 478/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0082 - mae: 0.0459 - val_loss: 0.0087 - val_mae: 0.0465\n",
      "Epoch 479/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0084 - mae: 0.0470 - val_loss: 0.0086 - val_mae: 0.0478\n",
      "Epoch 480/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0084 - mae: 0.0471 - val_loss: 0.0086 - val_mae: 0.0475\n",
      "Epoch 481/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0083 - mae: 0.0464 - val_loss: 0.0085 - val_mae: 0.0461\n",
      "Epoch 482/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0082 - mae: 0.0457 - val_loss: 0.0087 - val_mae: 0.0491\n",
      "Epoch 483/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0083 - mae: 0.0462 - val_loss: 0.0084 - val_mae: 0.0460\n",
      "Epoch 484/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0082 - mae: 0.0459 - val_loss: 0.0085 - val_mae: 0.0473\n",
      "Epoch 485/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0081 - mae: 0.0455 - val_loss: 0.0084 - val_mae: 0.0464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 486/1200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 0.0082 - mae: 0.0456 - val_loss: 0.0087 - val_mae: 0.0476\n",
      "Epoch 487/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0083 - mae: 0.0464 - val_loss: 0.0087 - val_mae: 0.0469\n",
      "Epoch 488/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0083 - mae: 0.0461 - val_loss: 0.0084 - val_mae: 0.0465\n",
      "Epoch 489/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0082 - mae: 0.0462 - val_loss: 0.0084 - val_mae: 0.0470\n",
      "Epoch 490/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0082 - mae: 0.0457 - val_loss: 0.0086 - val_mae: 0.0466\n",
      "Epoch 491/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0082 - mae: 0.0460 - val_loss: 0.0086 - val_mae: 0.0461\n",
      "Epoch 492/1200\n",
      "35/35 [==============================] - 1s 20ms/step - loss: 0.0084 - mae: 0.0472 - val_loss: 0.0088 - val_mae: 0.0470\n",
      "Epoch 493/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0083 - mae: 0.0461 - val_loss: 0.0085 - val_mae: 0.0461\n",
      "Epoch 494/1200\n",
      "35/35 [==============================] - 1s 20ms/step - loss: 0.0082 - mae: 0.0454 - val_loss: 0.0085 - val_mae: 0.0465\n",
      "Epoch 495/1200\n",
      "35/35 [==============================] - 1s 19ms/step - loss: 0.0082 - mae: 0.0452 - val_loss: 0.0084 - val_mae: 0.0460\n",
      "Epoch 496/1200\n",
      "35/35 [==============================] - 1s 19ms/step - loss: 0.0081 - mae: 0.0454 - val_loss: 0.0086 - val_mae: 0.0461\n",
      "Epoch 497/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0081 - mae: 0.0453 - val_loss: 0.0088 - val_mae: 0.0479\n",
      "Epoch 498/1200\n",
      "35/35 [==============================] - 1s 20ms/step - loss: 0.0084 - mae: 0.0476 - val_loss: 0.0083 - val_mae: 0.0484\n",
      "Epoch 499/1200\n",
      "35/35 [==============================] - 1s 19ms/step - loss: 0.0082 - mae: 0.0459 - val_loss: 0.0086 - val_mae: 0.0478\n",
      "Epoch 500/1200\n",
      "35/35 [==============================] - 1s 19ms/step - loss: 0.0082 - mae: 0.0459 - val_loss: 0.0083 - val_mae: 0.0468\n",
      "Epoch 501/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0082 - mae: 0.0456 - val_loss: 0.0085 - val_mae: 0.0467\n",
      "Epoch 502/1200\n",
      "35/35 [==============================] - 1s 19ms/step - loss: 0.0082 - mae: 0.0463 - val_loss: 0.0084 - val_mae: 0.0466\n",
      "Epoch 503/1200\n",
      "35/35 [==============================] - 1s 20ms/step - loss: 0.0082 - mae: 0.0467 - val_loss: 0.0086 - val_mae: 0.0487\n",
      "Epoch 504/1200\n",
      "35/35 [==============================] - 1s 19ms/step - loss: 0.0082 - mae: 0.0458 - val_loss: 0.0085 - val_mae: 0.0462\n",
      "Epoch 505/1200\n",
      "35/35 [==============================] - 1s 19ms/step - loss: 0.0081 - mae: 0.0455 - val_loss: 0.0084 - val_mae: 0.0461\n",
      "Epoch 506/1200\n",
      "35/35 [==============================] - 1s 20ms/step - loss: 0.0082 - mae: 0.0463 - val_loss: 0.0087 - val_mae: 0.0476\n",
      "Epoch 507/1200\n",
      "35/35 [==============================] - 1s 21ms/step - loss: 0.0082 - mae: 0.0462 - val_loss: 0.0084 - val_mae: 0.0479\n",
      "Epoch 508/1200\n",
      "35/35 [==============================] - 1s 21ms/step - loss: 0.0082 - mae: 0.0459 - val_loss: 0.0083 - val_mae: 0.0465\n",
      "Epoch 509/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0081 - mae: 0.0453 - val_loss: 0.0083 - val_mae: 0.0461\n",
      "Epoch 510/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0081 - mae: 0.0456 - val_loss: 0.0089 - val_mae: 0.0512\n",
      "Epoch 511/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0083 - mae: 0.0471 - val_loss: 0.0086 - val_mae: 0.0464\n",
      "Epoch 512/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0081 - mae: 0.0454 - val_loss: 0.0083 - val_mae: 0.0449\n",
      "Epoch 513/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0081 - mae: 0.0454 - val_loss: 0.0089 - val_mae: 0.0502\n",
      "Epoch 514/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0082 - mae: 0.0457 - val_loss: 0.0084 - val_mae: 0.0456\n",
      "Epoch 515/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0081 - mae: 0.0452 - val_loss: 0.0085 - val_mae: 0.0466\n",
      "Epoch 516/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0081 - mae: 0.0450 - val_loss: 0.0086 - val_mae: 0.0473\n",
      "Epoch 517/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0081 - mae: 0.0456 - val_loss: 0.0086 - val_mae: 0.0464\n",
      "Epoch 518/1200\n",
      "35/35 [==============================] - 1s 19ms/step - loss: 0.0080 - mae: 0.0449 - val_loss: 0.0084 - val_mae: 0.0461\n",
      "Epoch 519/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0081 - mae: 0.0450 - val_loss: 0.0083 - val_mae: 0.0458\n",
      "Epoch 520/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0081 - mae: 0.0451 - val_loss: 0.0082 - val_mae: 0.0458\n",
      "Epoch 521/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0083 - mae: 0.0476 - val_loss: 0.0089 - val_mae: 0.0499\n",
      "Epoch 522/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0083 - mae: 0.0467 - val_loss: 0.0085 - val_mae: 0.0463\n",
      "Epoch 523/1200\n",
      "35/35 [==============================] - 1s 19ms/step - loss: 0.0081 - mae: 0.0452 - val_loss: 0.0085 - val_mae: 0.0480\n",
      "Epoch 524/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0081 - mae: 0.0455 - val_loss: 0.0085 - val_mae: 0.0484\n",
      "Epoch 525/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0080 - mae: 0.0453 - val_loss: 0.0085 - val_mae: 0.0461\n",
      "Epoch 526/1200\n",
      "35/35 [==============================] - 1s 19ms/step - loss: 0.0079 - mae: 0.0447 - val_loss: 0.0084 - val_mae: 0.0458\n",
      "Epoch 527/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0080 - mae: 0.0450 - val_loss: 0.0081 - val_mae: 0.0455\n",
      "Epoch 528/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0080 - mae: 0.0448 - val_loss: 0.0085 - val_mae: 0.0469\n",
      "Epoch 529/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0079 - mae: 0.0447 - val_loss: 0.0084 - val_mae: 0.0447\n",
      "Epoch 530/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0080 - mae: 0.0449 - val_loss: 0.0086 - val_mae: 0.0486\n",
      "Epoch 531/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0080 - mae: 0.0452 - val_loss: 0.0084 - val_mae: 0.0456\n",
      "Epoch 532/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0081 - mae: 0.0450 - val_loss: 0.0084 - val_mae: 0.0459\n",
      "Epoch 533/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0080 - mae: 0.0449 - val_loss: 0.0085 - val_mae: 0.0464\n",
      "Epoch 534/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0081 - mae: 0.0452 - val_loss: 0.0085 - val_mae: 0.0465\n",
      "Epoch 535/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0080 - mae: 0.0452 - val_loss: 0.0087 - val_mae: 0.0464\n",
      "Epoch 536/1200\n",
      "35/35 [==============================] - 1s 20ms/step - loss: 0.0079 - mae: 0.0448 - val_loss: 0.0087 - val_mae: 0.0463\n",
      "Epoch 537/1200\n",
      "35/35 [==============================] - 1s 20ms/step - loss: 0.0081 - mae: 0.0456 - val_loss: 0.0085 - val_mae: 0.0480\n",
      "Epoch 538/1200\n",
      "35/35 [==============================] - 1s 21ms/step - loss: 0.0082 - mae: 0.0471 - val_loss: 0.0085 - val_mae: 0.0475\n",
      "Epoch 539/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0080 - mae: 0.0454 - val_loss: 0.0084 - val_mae: 0.0457\n",
      "Epoch 540/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0081 - mae: 0.0455 - val_loss: 0.0086 - val_mae: 0.0464\n",
      "Epoch 541/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0079 - mae: 0.0446 - val_loss: 0.0084 - val_mae: 0.0460\n",
      "Epoch 542/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0080 - mae: 0.0451 - val_loss: 0.0085 - val_mae: 0.0459\n",
      "Epoch 543/1200\n",
      "35/35 [==============================] - 1s 21ms/step - loss: 0.0082 - mae: 0.0457 - val_loss: 0.0089 - val_mae: 0.0483\n",
      "Epoch 544/1200\n",
      "35/35 [==============================] - 1s 21ms/step - loss: 0.0080 - mae: 0.0453 - val_loss: 0.0084 - val_mae: 0.0457\n",
      "Epoch 545/1200\n",
      "35/35 [==============================] - 1s 21ms/step - loss: 0.0080 - mae: 0.0446 - val_loss: 0.0084 - val_mae: 0.0458\n",
      "Epoch 546/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0080 - mae: 0.0445 - val_loss: 0.0083 - val_mae: 0.0447\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 547/1200\n",
      "35/35 [==============================] - 1s 19ms/step - loss: 0.0080 - mae: 0.0445 - val_loss: 0.0084 - val_mae: 0.0455\n",
      "Epoch 548/1200\n",
      "35/35 [==============================] - 1s 20ms/step - loss: 0.0080 - mae: 0.0448 - val_loss: 0.0084 - val_mae: 0.0470\n",
      "Epoch 549/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0082 - mae: 0.0457 - val_loss: 0.0085 - val_mae: 0.0458\n",
      "Epoch 550/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0081 - mae: 0.0449 - val_loss: 0.0085 - val_mae: 0.0471\n",
      "Epoch 551/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0081 - mae: 0.0449 - val_loss: 0.0083 - val_mae: 0.0456\n",
      "Epoch 552/1200\n",
      "35/35 [==============================] - 1s 19ms/step - loss: 0.0081 - mae: 0.0451 - val_loss: 0.0084 - val_mae: 0.0459\n",
      "Epoch 553/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0081 - mae: 0.0451 - val_loss: 0.0086 - val_mae: 0.0465\n",
      "Epoch 554/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0080 - mae: 0.0450 - val_loss: 0.0083 - val_mae: 0.0458\n",
      "Epoch 555/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0082 - mae: 0.0454 - val_loss: 0.0086 - val_mae: 0.0474\n",
      "Epoch 556/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0081 - mae: 0.0450 - val_loss: 0.0083 - val_mae: 0.0455\n",
      "Epoch 557/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0081 - mae: 0.0447 - val_loss: 0.0085 - val_mae: 0.0457\n",
      "Epoch 558/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0080 - mae: 0.0450 - val_loss: 0.0085 - val_mae: 0.0457\n",
      "Epoch 559/1200\n",
      "35/35 [==============================] - 1s 20ms/step - loss: 0.0079 - mae: 0.0446 - val_loss: 0.0082 - val_mae: 0.0444\n",
      "Epoch 560/1200\n",
      "35/35 [==============================] - 1s 19ms/step - loss: 0.0080 - mae: 0.0453 - val_loss: 0.0085 - val_mae: 0.0463\n",
      "Epoch 561/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0081 - mae: 0.0453 - val_loss: 0.0087 - val_mae: 0.0472\n",
      "Epoch 562/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0081 - mae: 0.0452 - val_loss: 0.0083 - val_mae: 0.0450\n",
      "Epoch 563/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0083 - mae: 0.0474 - val_loss: 0.0086 - val_mae: 0.0486\n",
      "Epoch 564/1200\n",
      "35/35 [==============================] - 1s 20ms/step - loss: 0.0081 - mae: 0.0454 - val_loss: 0.0085 - val_mae: 0.0460\n",
      "Epoch 565/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0080 - mae: 0.0447 - val_loss: 0.0084 - val_mae: 0.0454\n",
      "Epoch 566/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0079 - mae: 0.0446 - val_loss: 0.0084 - val_mae: 0.0454\n",
      "Epoch 567/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0080 - mae: 0.0446 - val_loss: 0.0086 - val_mae: 0.0477\n",
      "Epoch 568/1200\n",
      "35/35 [==============================] - 1s 20ms/step - loss: 0.0080 - mae: 0.0453 - val_loss: 0.0084 - val_mae: 0.0462\n",
      "Epoch 569/1200\n",
      "35/35 [==============================] - 1s 20ms/step - loss: 0.0080 - mae: 0.0453 - val_loss: 0.0089 - val_mae: 0.0480\n",
      "Epoch 570/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0080 - mae: 0.0447 - val_loss: 0.0085 - val_mae: 0.0453\n",
      "Epoch 571/1200\n",
      "35/35 [==============================] - 1s 19ms/step - loss: 0.0079 - mae: 0.0446 - val_loss: 0.0085 - val_mae: 0.0470\n",
      "Epoch 572/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0081 - mae: 0.0456 - val_loss: 0.0084 - val_mae: 0.0469\n",
      "Epoch 573/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0079 - mae: 0.0446 - val_loss: 0.0084 - val_mae: 0.0469\n",
      "Epoch 574/1200\n",
      "35/35 [==============================] - 1s 19ms/step - loss: 0.0080 - mae: 0.0456 - val_loss: 0.0082 - val_mae: 0.0455\n",
      "Epoch 575/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0080 - mae: 0.0446 - val_loss: 0.0086 - val_mae: 0.0467\n",
      "Epoch 576/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0081 - mae: 0.0457 - val_loss: 0.0087 - val_mae: 0.0480\n",
      "Epoch 577/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0080 - mae: 0.0452 - val_loss: 0.0083 - val_mae: 0.0455\n",
      "Epoch 578/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0080 - mae: 0.0449 - val_loss: 0.0084 - val_mae: 0.0460\n",
      "Epoch 579/1200\n",
      "35/35 [==============================] - 1s 20ms/step - loss: 0.0079 - mae: 0.0445 - val_loss: 0.0081 - val_mae: 0.0448\n",
      "Epoch 580/1200\n",
      "35/35 [==============================] - 1s 19ms/step - loss: 0.0079 - mae: 0.0444 - val_loss: 0.0083 - val_mae: 0.0460\n",
      "Epoch 581/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0080 - mae: 0.0449 - val_loss: 0.0085 - val_mae: 0.0454\n",
      "Epoch 582/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0079 - mae: 0.0443 - val_loss: 0.0082 - val_mae: 0.0447\n",
      "Epoch 583/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0081 - mae: 0.0461 - val_loss: 0.0084 - val_mae: 0.0476\n",
      "Epoch 584/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0079 - mae: 0.0446 - val_loss: 0.0081 - val_mae: 0.0453\n",
      "Epoch 585/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0079 - mae: 0.0446 - val_loss: 0.0081 - val_mae: 0.0453\n",
      "Epoch 586/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0078 - mae: 0.0444 - val_loss: 0.0085 - val_mae: 0.0472\n",
      "Epoch 587/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0079 - mae: 0.0448 - val_loss: 0.0084 - val_mae: 0.0472\n",
      "Epoch 588/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0080 - mae: 0.0448 - val_loss: 0.0081 - val_mae: 0.0450\n",
      "Epoch 589/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0079 - mae: 0.0444 - val_loss: 0.0083 - val_mae: 0.0458\n",
      "Epoch 590/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0079 - mae: 0.0443 - val_loss: 0.0085 - val_mae: 0.0457\n",
      "Epoch 591/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0079 - mae: 0.0445 - val_loss: 0.0082 - val_mae: 0.0443\n",
      "Epoch 592/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0078 - mae: 0.0439 - val_loss: 0.0083 - val_mae: 0.0447\n",
      "Epoch 593/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0079 - mae: 0.0451 - val_loss: 0.0081 - val_mae: 0.0466\n",
      "Epoch 594/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0079 - mae: 0.0445 - val_loss: 0.0082 - val_mae: 0.0454\n",
      "Epoch 595/1200\n",
      "35/35 [==============================] - 1s 19ms/step - loss: 0.0079 - mae: 0.0443 - val_loss: 0.0081 - val_mae: 0.0454\n",
      "Epoch 596/1200\n",
      "35/35 [==============================] - 1s 20ms/step - loss: 0.0077 - mae: 0.0441 - val_loss: 0.0083 - val_mae: 0.0458\n",
      "Epoch 597/1200\n",
      "35/35 [==============================] - 1s 20ms/step - loss: 0.0079 - mae: 0.0445 - val_loss: 0.0083 - val_mae: 0.0448\n",
      "Epoch 598/1200\n",
      "35/35 [==============================] - 1s 20ms/step - loss: 0.0078 - mae: 0.0441 - val_loss: 0.0082 - val_mae: 0.0459\n",
      "Epoch 599/1200\n",
      "35/35 [==============================] - 1s 19ms/step - loss: 0.0079 - mae: 0.0445 - val_loss: 0.0084 - val_mae: 0.0462\n",
      "Epoch 600/1200\n",
      "35/35 [==============================] - 1s 20ms/step - loss: 0.0079 - mae: 0.0443 - val_loss: 0.0083 - val_mae: 0.0446\n",
      "Epoch 601/1200\n",
      "35/35 [==============================] - 1s 20ms/step - loss: 0.0078 - mae: 0.0445 - val_loss: 0.0080 - val_mae: 0.0445\n",
      "Epoch 602/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0077 - mae: 0.0442 - val_loss: 0.0084 - val_mae: 0.0471\n",
      "Epoch 603/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0078 - mae: 0.0442 - val_loss: 0.0081 - val_mae: 0.0454\n",
      "Epoch 604/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0079 - mae: 0.0448 - val_loss: 0.0084 - val_mae: 0.0457\n",
      "Epoch 605/1200\n",
      "35/35 [==============================] - 1s 20ms/step - loss: 0.0078 - mae: 0.0441 - val_loss: 0.0084 - val_mae: 0.0460\n",
      "Epoch 606/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0078 - mae: 0.0441 - val_loss: 0.0084 - val_mae: 0.0454\n",
      "Epoch 607/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0079 - mae: 0.0447 - val_loss: 0.0084 - val_mae: 0.0456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 608/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0078 - mae: 0.0445 - val_loss: 0.0082 - val_mae: 0.0464\n",
      "Epoch 609/1200\n",
      "35/35 [==============================] - 1s 20ms/step - loss: 0.0079 - mae: 0.0450 - val_loss: 0.0085 - val_mae: 0.0476\n",
      "Epoch 610/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0078 - mae: 0.0444 - val_loss: 0.0084 - val_mae: 0.0456\n",
      "Epoch 611/1200\n",
      "35/35 [==============================] - 1s 19ms/step - loss: 0.0079 - mae: 0.0453 - val_loss: 0.0083 - val_mae: 0.0455\n",
      "Epoch 612/1200\n",
      "35/35 [==============================] - 1s 20ms/step - loss: 0.0077 - mae: 0.0436 - val_loss: 0.0085 - val_mae: 0.0474\n",
      "Epoch 613/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0078 - mae: 0.0445 - val_loss: 0.0082 - val_mae: 0.0468\n",
      "Epoch 614/1200\n",
      "35/35 [==============================] - 1s 19ms/step - loss: 0.0078 - mae: 0.0440 - val_loss: 0.0082 - val_mae: 0.0452\n",
      "Epoch 615/1200\n",
      "35/35 [==============================] - 1s 19ms/step - loss: 0.0078 - mae: 0.0442 - val_loss: 0.0081 - val_mae: 0.0454\n",
      "Epoch 616/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0078 - mae: 0.0444 - val_loss: 0.0081 - val_mae: 0.0454\n",
      "Epoch 617/1200\n",
      "35/35 [==============================] - 1s 20ms/step - loss: 0.0078 - mae: 0.0445 - val_loss: 0.0081 - val_mae: 0.0461\n",
      "Epoch 618/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0078 - mae: 0.0446 - val_loss: 0.0081 - val_mae: 0.0455\n",
      "Epoch 619/1200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 0.0077 - mae: 0.0442 - val_loss: 0.0081 - val_mae: 0.0454\n",
      "Epoch 620/1200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 0.0078 - mae: 0.0447 - val_loss: 0.0078 - val_mae: 0.0455\n",
      "Epoch 621/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0077 - mae: 0.0440 - val_loss: 0.0082 - val_mae: 0.0474\n",
      "Epoch 622/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0077 - mae: 0.0442 - val_loss: 0.0081 - val_mae: 0.0449\n",
      "Epoch 623/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0077 - mae: 0.0437 - val_loss: 0.0080 - val_mae: 0.0444\n",
      "Epoch 624/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0078 - mae: 0.0440 - val_loss: 0.0082 - val_mae: 0.0458\n",
      "Epoch 625/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0078 - mae: 0.0445 - val_loss: 0.0081 - val_mae: 0.0463\n",
      "Epoch 626/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0078 - mae: 0.0446 - val_loss: 0.0081 - val_mae: 0.0451\n",
      "Epoch 627/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0077 - mae: 0.0447 - val_loss: 0.0085 - val_mae: 0.0464\n",
      "Epoch 628/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0077 - mae: 0.0436 - val_loss: 0.0083 - val_mae: 0.0459\n",
      "Epoch 629/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0076 - mae: 0.0439 - val_loss: 0.0084 - val_mae: 0.0469\n",
      "Epoch 630/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0077 - mae: 0.0442 - val_loss: 0.0081 - val_mae: 0.0460\n",
      "Epoch 631/1200\n",
      "35/35 [==============================] - 1s 19ms/step - loss: 0.0076 - mae: 0.0436 - val_loss: 0.0080 - val_mae: 0.0445\n",
      "Epoch 632/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0076 - mae: 0.0435 - val_loss: 0.0080 - val_mae: 0.0452\n",
      "Epoch 633/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0077 - mae: 0.0441 - val_loss: 0.0080 - val_mae: 0.0458\n",
      "Epoch 634/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0076 - mae: 0.0436 - val_loss: 0.0082 - val_mae: 0.0448\n",
      "Epoch 635/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0077 - mae: 0.0438 - val_loss: 0.0080 - val_mae: 0.0447\n",
      "Epoch 636/1200\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0077 - mae: 0.0441 - val_loss: 0.0079 - val_mae: 0.0447\n",
      "Epoch 637/1200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 0.0077 - mae: 0.0439 - val_loss: 0.0080 - val_mae: 0.0444\n",
      "Epoch 638/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0077 - mae: 0.0436 - val_loss: 0.0082 - val_mae: 0.0446\n",
      "Epoch 639/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0077 - mae: 0.0438 - val_loss: 0.0081 - val_mae: 0.0448\n",
      "Epoch 640/1200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.0077 - mae: 0.0440 - val_loss: 0.0081 - val_mae: 0.0454\n",
      "Epoch 641/1200\n",
      "35/35 [==============================] - 1s 19ms/step - loss: 0.0077 - mae: 0.0439 - val_loss: 0.0080 - val_mae: 0.0453\n"
     ]
    }
   ],
   "source": [
    "model = DAE(3, 2.6)\n",
    "callbacks_list = [keras.callbacks.EarlyStopping(monitor='val_mae', patience=50, ),  \n",
    "                  keras.callbacks.TensorBoard(log_dir='train_log', histogram_freq=1,embeddings_freq=1,)]\n",
    "history = model.fit(x_train, x_train, epochs=1200, batch_size=100, \n",
    "                    validation_data=(x_val, x_val),callbacks=callbacks_list)\n",
    "\n",
    "#history = model.fit(x_train, x_train, epochs=1200, batch_size=100, validation_split=0.1, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! tensorboard --logdir=train_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x24c90bfde80>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting the training and validation \"loss\" \n",
    "loss_values = history.history['loss'][200::5]\n",
    "val_loss_values = history.history['val_loss'][200::5]\n",
    "epochs = range(201, len(loss_values)*5 + 201, 5)\n",
    "\n",
    "plt.plot(epochs, loss_values, 'b', label='Training loss')\n",
    "plt.plot(epochs, val_loss_values, 'g', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x24c90458670>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting the training and validation \"mae\" \n",
    "mae_values = history.history['mae'][200::5]\n",
    "val_mae_values = history.history['val_mae'][200::5]\n",
    "epochs = range(201, len(mae_values)*5 + 201, 5)\n",
    "plt.plot(epochs, mae_values, 'b', label='Training mae')\n",
    "plt.plot(epochs, val_mae_values, 'g', label='Validation mae')\n",
    "plt.title('Training and validation MAE')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Knock out the 'noising' dropout layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_25\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_10 (InputLayer)        [(None, 166)]             0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 166)               0         \n",
      "_________________________________________________________________\n",
      "rescaling_5 (Rescaling)      (None, 166)               0         \n",
      "_________________________________________________________________\n",
      "sequential_4 (Sequential)    (None, 431)               1040694   \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 166)               71712     \n",
      "=================================================================\n",
      "Total params: 1,112,406\n",
      "Trainable params: 1,112,406\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_25\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_10 (InputLayer)        [(None, 166)]             0         \n",
      "_________________________________________________________________\n",
      "sequential_4 (Sequential)    (None, 431)               1040694   \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 166)               71712     \n",
      "=================================================================\n",
      "Total params: 1,112,406\n",
      "Trainable params: 1,112,406\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_for_imputation = keras.models.clone_model(model)\n",
    "model_for_imputation.set_weights(model.get_weights())\n",
    "model_for_imputation._layers.pop(1)  # remove the dropout layer\n",
    "model_for_imputation._layers.pop(1)  # remove the rescaling layer\n",
    "model_for_imputation.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "#### Create a 'map' of missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "missing_map = np.copy(incomplete_x)\n",
    "missing_map[~np.isnan(missing_map)] = False\n",
    "missing_map[np.isnan(missing_map)] = True\n",
    "missing_map = missing_map.astype(int, copy=False)\n",
    "#missing_map[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feed the incomplete set and get the output value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "incomplete_x_num = np.copy(incomplete_x)\n",
    "incomplete_x_num[np.isnan(incomplete_x_num)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "# obtain the output of an intermediate layer\n",
    "inp = model_for_imputation.input\n",
    "out = model_for_imputation.output\n",
    "get_output = K.function([inp], [out])\n",
    "\n",
    "outputs = get_output([incomplete_x_num])\n",
    "#print(np.array(outputs)[0])\n",
    "dae_predict = outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dae_predict[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing value imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14986, 166)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imputation by DAE\n",
    "## the full dataset for classification: imputation_dae\n",
    "imputation_dae = np.copy(incomplete_x)\n",
    "np.putmask(imputation_dae, missing_map, dae_predict)\n",
    "imputation_dae = np.concatenate([complete_x, imputation_dae], axis=0)\n",
    "imputation_dae.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14986, 166)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imputation with column mean\n",
    "## the full dataset for classification: imputation_mean\n",
    "imputation_mean = np.concatenate([complete_x, incomplete_x], axis=0)\n",
    "mask = np.zeros(shape=complete_x.shape, dtype=int)\n",
    "mask = np.concatenate([mask, missing_map], axis=0)\n",
    "imputation_mean = np.where(mask, np.nanmean(imputation_mean, axis=0), imputation_mean)\n",
    "imputation_mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let me check the MAE first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corrupt the test set of complete cases\n",
    "inp = model_0.input\n",
    "out = model_0.output\n",
    "get_output = K.function([inp], [out])\n",
    "outputs = get_output([x_test])\n",
    "\n",
    "x_test_corrupted = np.array(outputs[0])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_corrupted[x_test_corrupted==0] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing map\n",
    "masking = np.copy(x_test_corrupted)\n",
    "masking[~np.isnan(masking)] = False\n",
    "masking[np.isnan(masking)] = True\n",
    "masking = masking.astype(int, copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace with mean\n",
    "mean_replace = np.where(np.isnan(x_test_corrupted), np.nanmean(x_test_corrupted, axis=0), x_test_corrupted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputation by dae\n",
    "x_test_corrupted[np.isnan(x_test_corrupted)] = 0\n",
    "inp = model_for_imputation.input\n",
    "out = model_for_imputation.output\n",
    "get_output = K.function([inp], [out])\n",
    "\n",
    "outputs = get_output([x_test_corrupted])\n",
    "dae_predict = outputs[0]\n",
    "\n",
    "imput_dae = np.copy(x_test_corrupted)\n",
    "np.putmask(imput_dae, masking, dae_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE if without imputation:  0.2662043238335415\n",
      "MAE if imputation by mean:  0.021418720774534647\n",
      "MAE if imputation by DAE:   0.017297812001236236\n"
     ]
    }
   ],
   "source": [
    "# to what extent the missing values were recovered\n",
    "mae_corrupt = sklearn.metrics.mean_absolute_error(x_test, x_test_corrupted)\n",
    "mae_mean = sklearn.metrics.mean_absolute_error(x_test, mean_replace)\n",
    "mae_dae = sklearn.metrics.mean_absolute_error(x_test, imput_dae)\n",
    "print('MAE if without imputation: ', mae_corrupt)\n",
    "print('MAE if imputation by mean: ', mae_mean)\n",
    "print('MAE if imputation by DAE:  ', mae_dae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification task\n",
    "\n",
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "complete_y = df_complete['DIQ010'].to_numpy()\n",
    "imputation_y = np.concatenate([complete_y, incomplete_y], axis=0)\n",
    "complete_y_bin = np.where(complete_y==1, 0, complete_y)\n",
    "complete_y_bin = np.where(complete_y==2, 1, complete_y_bin)\n",
    "imputation_y_bin = np.where(imputation_y==1, 0, imputation_y)\n",
    "imputation_y_bin = np.where(imputation_y==2, 1, imputation_y_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_y_cat = pd.Categorical(complete_y_bin)\n",
    "imputation_y_cat = pd.Categorical(imputation_y_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete 4825: train=3860, test=965\n",
    "x_train_c, x_test_c, y_train_c, y_test_c = train_test_split(complete_x, complete_y_cat, \n",
    "                                                            test_size=0.2, random_state=1)\n",
    "# train classifier\n",
    "clf_complete = svm.SVC()\n",
    "clf_complete.fit(x_train_c, y_train_c)\n",
    "\n",
    "y_pred_c = clf_complete.predict(x_test_c)\n",
    "cm_c = sklearn.metrics.confusion_matrix(y_test_c, y_pred_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[824,   0],\n",
       "       [141,   0]], dtype=int64)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whole 14986: train=11988, test=2998\n",
    "x_train_mean, x_test_mean, y_train_mean, y_test_mean = train_test_split(imputation_mean, imputation_y_cat, \n",
    "                                                                        test_size=0.2, random_state=1)\n",
    "\n",
    "clf_mean = svm.SVC()\n",
    "clf_mean.fit(x_train_mean, y_train_mean)\n",
    "\n",
    "y_pred_mean = clf_mean.predict(x_test_mean)\n",
    "cm_mean = sklearn.metrics.confusion_matrix(y_test_mean, y_pred_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2637,    0],\n",
       "       [ 361,    0]], dtype=int64)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_dae, x_test_dae, y_train_dae, y_test_dae = train_test_split(imputation_dae, imputation_y_cat, \n",
    "                                                                    test_size=0.2, random_state=1)\n",
    "\n",
    "clf_dae = svm.SVC()\n",
    "clf_dae.fit(x_train_dae, y_train_dae)\n",
    "\n",
    "y_pred_dae = clf_dae.predict(x_test_dae)\n",
    "cm_dae = sklearn.metrics.confusion_matrix(y_test_dae, y_pred_dae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2637,    0],\n",
       "       [ 361,    0]], dtype=int64)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm_dae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Well, maybe the class-imbalance issue should be considered... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>counts</th>\n",
       "      <th>freqs</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>categories</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>13257</td>\n",
       "      <td>0.884626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>1729</td>\n",
       "      <td>0.115374</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            counts     freqs\n",
       "categories                  \n",
       "0.0          13257  0.884626\n",
       "1.0           1729  0.115374"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputation_y_cat.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>counts</th>\n",
       "      <th>freqs</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>categories</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>4102</td>\n",
       "      <td>0.850155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>723</td>\n",
       "      <td>0.149845</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            counts     freqs\n",
       "categories                  \n",
       "0.0           4102  0.850155\n",
       "1.0            723  0.149845"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_y_cat.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>counts</th>\n",
       "      <th>freqs</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>categories</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>2637</td>\n",
       "      <td>0.879586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>361</td>\n",
       "      <td>0.120414</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            counts     freqs\n",
       "categories                  \n",
       "0.0           2637  0.879586\n",
       "1.0            361  0.120414"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_dae.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>counts</th>\n",
       "      <th>freqs</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>categories</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>824</td>\n",
       "      <td>0.853886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>141</td>\n",
       "      <td>0.146114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            counts     freqs\n",
       "categories                  \n",
       "0.0            824  0.853886\n",
       "1.0            141  0.146114"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_c.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### or, try to carry out the classification task by neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a neural network for classification \n",
    "model_clf = keras.models.Sequential()  \n",
    "\n",
    "model_clf.add(keras.Input(shape=input_dim))\n",
    "model_clf.add(layers.Dense(256, activation='relu'))\n",
    "model_clf.add(layers.Dense(128, activation='relu'))\n",
    "model_clf.add(layers.Dense(64, activation='relu'))\n",
    "model_clf.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_clf.compile(optimizer='adam', loss='binary_crossentropy', metrics='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.4421 - accuracy: 0.8301\n",
      "Epoch 2/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.4097 - accuracy: 0.8492\n",
      "Epoch 3/500\n",
      "39/39 [==============================] - 0s 1ms/step - loss: 0.4096 - accuracy: 0.8492\n",
      "Epoch 4/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3933 - accuracy: 0.8487\n",
      "Epoch 5/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3826 - accuracy: 0.8484\n",
      "Epoch 6/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3863 - accuracy: 0.8492\n",
      "Epoch 7/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3797 - accuracy: 0.8469\n",
      "Epoch 8/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3832 - accuracy: 0.8487\n",
      "Epoch 9/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3792 - accuracy: 0.8466\n",
      "Epoch 10/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3745 - accuracy: 0.8484\n",
      "Epoch 11/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3734 - accuracy: 0.8490\n",
      "Epoch 12/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3662 - accuracy: 0.8497\n",
      "Epoch 13/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3866 - accuracy: 0.8484\n",
      "Epoch 14/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3680 - accuracy: 0.8487\n",
      "Epoch 15/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3635 - accuracy: 0.8492\n",
      "Epoch 16/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3638 - accuracy: 0.8487\n",
      "Epoch 17/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3615 - accuracy: 0.8505\n",
      "Epoch 18/500\n",
      "39/39 [==============================] - 0s 1ms/step - loss: 0.3674 - accuracy: 0.8497\n",
      "Epoch 19/500\n",
      "39/39 [==============================] - 0s 1ms/step - loss: 0.3575 - accuracy: 0.8495\n",
      "Epoch 20/500\n",
      "39/39 [==============================] - 0s 1ms/step - loss: 0.3582 - accuracy: 0.8497\n",
      "Epoch 21/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3684 - accuracy: 0.8490\n",
      "Epoch 22/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3562 - accuracy: 0.8487\n",
      "Epoch 23/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3512 - accuracy: 0.8490\n",
      "Epoch 24/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3525 - accuracy: 0.8513\n",
      "Epoch 25/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3484 - accuracy: 0.8513\n",
      "Epoch 26/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3532 - accuracy: 0.8497\n",
      "Epoch 27/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3392 - accuracy: 0.8508\n",
      "Epoch 28/500\n",
      "39/39 [==============================] - 0s 1ms/step - loss: 0.3562 - accuracy: 0.8505\n",
      "Epoch 29/500\n",
      "39/39 [==============================] - 0s 1ms/step - loss: 0.3442 - accuracy: 0.8526\n",
      "Epoch 30/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3608 - accuracy: 0.8521\n",
      "Epoch 31/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3541 - accuracy: 0.8495\n",
      "Epoch 32/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3428 - accuracy: 0.8505\n",
      "Epoch 33/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3348 - accuracy: 0.8554\n",
      "Epoch 34/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3447 - accuracy: 0.8528\n",
      "Epoch 35/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3447 - accuracy: 0.8510\n",
      "Epoch 36/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3315 - accuracy: 0.8554\n",
      "Epoch 37/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3536 - accuracy: 0.8505\n",
      "Epoch 38/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3436 - accuracy: 0.8534\n",
      "Epoch 39/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3383 - accuracy: 0.8547\n",
      "Epoch 40/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3611 - accuracy: 0.8536\n",
      "Epoch 41/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3381 - accuracy: 0.8528\n",
      "Epoch 42/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3286 - accuracy: 0.8549\n",
      "Epoch 43/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3398 - accuracy: 0.8513\n",
      "Epoch 44/500\n",
      "39/39 [==============================] - 0s 1ms/step - loss: 0.3405 - accuracy: 0.8526\n",
      "Epoch 45/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3464 - accuracy: 0.8510\n",
      "Epoch 46/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3413 - accuracy: 0.8526\n",
      "Epoch 47/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3352 - accuracy: 0.8526\n",
      "Epoch 48/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3338 - accuracy: 0.8526\n",
      "Epoch 49/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3261 - accuracy: 0.8554\n",
      "Epoch 50/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3273 - accuracy: 0.8562\n",
      "Epoch 51/500\n",
      "39/39 [==============================] - 0s 1ms/step - loss: 0.3295 - accuracy: 0.8588\n",
      "Epoch 52/500\n",
      "39/39 [==============================] - 0s 1ms/step - loss: 0.3483 - accuracy: 0.8523\n",
      "Epoch 53/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3321 - accuracy: 0.8580\n",
      "Epoch 54/500\n",
      "39/39 [==============================] - 0s 1ms/step - loss: 0.3205 - accuracy: 0.8585\n",
      "Epoch 55/500\n",
      "39/39 [==============================] - 0s 1ms/step - loss: 0.3230 - accuracy: 0.8593\n",
      "Epoch 56/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3337 - accuracy: 0.8536\n",
      "Epoch 57/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3259 - accuracy: 0.8562\n",
      "Epoch 58/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3248 - accuracy: 0.8617\n",
      "Epoch 59/500\n",
      "39/39 [==============================] - 0s 1ms/step - loss: 0.3224 - accuracy: 0.8614\n",
      "Epoch 60/500\n",
      "39/39 [==============================] - 0s 1ms/step - loss: 0.3258 - accuracy: 0.8578\n",
      "Epoch 61/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3326 - accuracy: 0.8549\n",
      "Epoch 62/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3343 - accuracy: 0.8565\n",
      "Epoch 63/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3324 - accuracy: 0.8593\n",
      "Epoch 64/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3192 - accuracy: 0.8565\n",
      "Epoch 65/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3502 - accuracy: 0.8534\n",
      "Epoch 66/500\n",
      "39/39 [==============================] - 0s 1ms/step - loss: 0.3264 - accuracy: 0.8578\n",
      "Epoch 67/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3284 - accuracy: 0.8549\n",
      "Epoch 68/500\n",
      "39/39 [==============================] - 0s 1ms/step - loss: 0.3237 - accuracy: 0.8580\n",
      "Epoch 69/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3259 - accuracy: 0.8601\n",
      "Epoch 70/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3310 - accuracy: 0.8531\n",
      "Epoch 71/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3347 - accuracy: 0.8554\n",
      "Epoch 72/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3185 - accuracy: 0.8578\n",
      "Epoch 73/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3256 - accuracy: 0.8601\n",
      "Epoch 74/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3226 - accuracy: 0.8580\n",
      "Epoch 75/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3182 - accuracy: 0.8637\n",
      "Epoch 76/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3219 - accuracy: 0.8565\n",
      "Epoch 77/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3290 - accuracy: 0.8547\n",
      "Epoch 78/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3214 - accuracy: 0.8604\n",
      "Epoch 79/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3201 - accuracy: 0.8580\n",
      "Epoch 80/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3196 - accuracy: 0.8580\n",
      "Epoch 81/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3310 - accuracy: 0.8565\n",
      "Epoch 82/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3157 - accuracy: 0.8611\n",
      "Epoch 83/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3310 - accuracy: 0.8565\n",
      "Epoch 84/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3193 - accuracy: 0.8614\n",
      "Epoch 85/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3175 - accuracy: 0.8619\n",
      "Epoch 86/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3393 - accuracy: 0.8583\n",
      "Epoch 87/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3424 - accuracy: 0.8518\n",
      "Epoch 88/500\n",
      "39/39 [==============================] - 0s 1ms/step - loss: 0.3216 - accuracy: 0.8606\n",
      "Epoch 89/500\n",
      "39/39 [==============================] - 0s 1ms/step - loss: 0.3192 - accuracy: 0.8617\n",
      "Epoch 90/500\n",
      "39/39 [==============================] - 0s 1ms/step - loss: 0.3282 - accuracy: 0.8567\n",
      "Epoch 91/500\n",
      "39/39 [==============================] - 0s 1ms/step - loss: 0.3207 - accuracy: 0.8604\n",
      "Epoch 92/500\n",
      "39/39 [==============================] - 0s 1ms/step - loss: 0.3193 - accuracy: 0.8606\n",
      "Epoch 93/500\n",
      "39/39 [==============================] - 0s 1ms/step - loss: 0.3128 - accuracy: 0.8627\n",
      "Epoch 94/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3148 - accuracy: 0.8624\n",
      "Epoch 95/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3147 - accuracy: 0.8622\n",
      "Epoch 96/500\n",
      "39/39 [==============================] - 0s 1ms/step - loss: 0.3283 - accuracy: 0.8619\n",
      "Epoch 97/500\n",
      "39/39 [==============================] - 0s 1ms/step - loss: 0.3284 - accuracy: 0.8518\n",
      "Epoch 98/500\n",
      "39/39 [==============================] - 0s 1ms/step - loss: 0.3246 - accuracy: 0.8609\n",
      "Epoch 99/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3163 - accuracy: 0.8614\n",
      "Epoch 100/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3155 - accuracy: 0.8606\n",
      "Epoch 101/500\n",
      "39/39 [==============================] - 0s 1ms/step - loss: 0.3457 - accuracy: 0.8510\n",
      "Epoch 102/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3178 - accuracy: 0.8617\n",
      "Epoch 103/500\n",
      "39/39 [==============================] - 0s 1ms/step - loss: 0.3219 - accuracy: 0.8570\n",
      "Epoch 104/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3192 - accuracy: 0.8578\n",
      "Epoch 105/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3161 - accuracy: 0.8624\n",
      "Epoch 106/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3103 - accuracy: 0.8632\n",
      "Epoch 107/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3186 - accuracy: 0.8588\n",
      "Epoch 108/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3164 - accuracy: 0.8598\n",
      "Epoch 109/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3223 - accuracy: 0.8552\n",
      "Epoch 110/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3287 - accuracy: 0.8565\n",
      "Epoch 111/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3255 - accuracy: 0.8609\n",
      "Epoch 112/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3242 - accuracy: 0.8570\n",
      "Epoch 113/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3179 - accuracy: 0.8630\n",
      "Epoch 114/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3250 - accuracy: 0.8573\n",
      "Epoch 115/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3244 - accuracy: 0.8588\n",
      "Epoch 116/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3379 - accuracy: 0.8526\n",
      "Epoch 117/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3196 - accuracy: 0.8588\n",
      "Epoch 118/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3148 - accuracy: 0.8640\n",
      "Epoch 119/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3225 - accuracy: 0.8598\n",
      "Epoch 120/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3138 - accuracy: 0.8593\n",
      "Epoch 121/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3136 - accuracy: 0.8622\n",
      "Epoch 122/500\n",
      "39/39 [==============================] - 0s 1ms/step - loss: 0.3130 - accuracy: 0.8606\n",
      "Epoch 123/500\n",
      "39/39 [==============================] - 0s 1ms/step - loss: 0.3402 - accuracy: 0.8575\n",
      "Epoch 124/500\n",
      "39/39 [==============================] - 0s 1ms/step - loss: 0.3178 - accuracy: 0.8619\n",
      "Epoch 125/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3247 - accuracy: 0.8596\n",
      "Epoch 126/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3112 - accuracy: 0.8655\n",
      "Epoch 127/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3237 - accuracy: 0.8609\n",
      "Epoch 128/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3249 - accuracy: 0.8614\n",
      "Epoch 129/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3154 - accuracy: 0.8627\n",
      "Epoch 130/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3080 - accuracy: 0.8653\n",
      "Epoch 131/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3306 - accuracy: 0.8567\n",
      "Epoch 132/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3119 - accuracy: 0.8637\n",
      "Epoch 133/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3180 - accuracy: 0.8604\n",
      "Epoch 134/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3139 - accuracy: 0.8604\n",
      "Epoch 135/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3262 - accuracy: 0.8573\n",
      "Epoch 136/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3135 - accuracy: 0.8624\n",
      "Epoch 137/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3120 - accuracy: 0.8635\n",
      "Epoch 138/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3272 - accuracy: 0.8593\n",
      "Epoch 139/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3139 - accuracy: 0.8635\n",
      "Epoch 140/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3187 - accuracy: 0.8593\n",
      "Epoch 141/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3210 - accuracy: 0.8570\n",
      "Epoch 142/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3088 - accuracy: 0.8645\n",
      "Epoch 143/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3174 - accuracy: 0.8630\n",
      "Epoch 144/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3181 - accuracy: 0.8668\n",
      "Epoch 145/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3252 - accuracy: 0.8617\n",
      "Epoch 146/500\n",
      "39/39 [==============================] - 0s 1ms/step - loss: 0.3121 - accuracy: 0.8630\n",
      "Epoch 147/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3126 - accuracy: 0.8617\n",
      "Epoch 148/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3082 - accuracy: 0.8663\n",
      "Epoch 149/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3121 - accuracy: 0.8627\n",
      "Epoch 150/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3069 - accuracy: 0.8671\n",
      "Epoch 151/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3169 - accuracy: 0.8630\n",
      "Epoch 152/500\n",
      "39/39 [==============================] - 0s 1ms/step - loss: 0.3043 - accuracy: 0.8676\n",
      "Epoch 153/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3206 - accuracy: 0.8642\n",
      "Epoch 154/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3235 - accuracy: 0.8560\n",
      "Epoch 155/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3101 - accuracy: 0.8624\n",
      "Epoch 156/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3174 - accuracy: 0.8614\n",
      "Epoch 157/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3149 - accuracy: 0.8640\n",
      "Epoch 158/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3094 - accuracy: 0.8627\n",
      "Epoch 159/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3198 - accuracy: 0.8567\n",
      "Epoch 160/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3234 - accuracy: 0.8557\n",
      "Epoch 161/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3089 - accuracy: 0.8645\n",
      "Epoch 162/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3081 - accuracy: 0.8640\n",
      "Epoch 163/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3092 - accuracy: 0.8648\n",
      "Epoch 164/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3099 - accuracy: 0.8674\n",
      "Epoch 165/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3266 - accuracy: 0.8523\n",
      "Epoch 166/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3205 - accuracy: 0.8619\n",
      "Epoch 167/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3206 - accuracy: 0.8567\n",
      "Epoch 168/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3097 - accuracy: 0.8687\n",
      "Epoch 169/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3083 - accuracy: 0.8650\n",
      "Epoch 170/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3260 - accuracy: 0.8580\n",
      "Epoch 171/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3124 - accuracy: 0.8650\n",
      "Epoch 172/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3177 - accuracy: 0.8622\n",
      "Epoch 173/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3095 - accuracy: 0.8679\n",
      "Epoch 174/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3142 - accuracy: 0.8622\n",
      "Epoch 175/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3214 - accuracy: 0.8614\n",
      "Epoch 176/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3128 - accuracy: 0.8642\n",
      "Epoch 177/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3215 - accuracy: 0.8591\n",
      "Epoch 178/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3097 - accuracy: 0.8648\n",
      "Epoch 179/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3065 - accuracy: 0.8684\n",
      "Epoch 180/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3113 - accuracy: 0.8609\n",
      "Epoch 181/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3177 - accuracy: 0.8617\n",
      "Epoch 182/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3091 - accuracy: 0.8624\n",
      "Epoch 183/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3123 - accuracy: 0.8658\n",
      "Epoch 184/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3059 - accuracy: 0.8668\n",
      "Epoch 185/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3065 - accuracy: 0.8694\n",
      "Epoch 186/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3049 - accuracy: 0.8666\n",
      "Epoch 187/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3042 - accuracy: 0.8692\n",
      "Epoch 188/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3018 - accuracy: 0.8671\n",
      "Epoch 189/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3203 - accuracy: 0.8575\n",
      "Epoch 190/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3144 - accuracy: 0.8679\n",
      "Epoch 191/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3132 - accuracy: 0.8681\n",
      "Epoch 192/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3061 - accuracy: 0.8630\n",
      "Epoch 193/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3128 - accuracy: 0.8635\n",
      "Epoch 194/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3155 - accuracy: 0.8622\n",
      "Epoch 195/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3127 - accuracy: 0.8588\n",
      "Epoch 196/500\n",
      "39/39 [==============================] - 0s 3ms/step - loss: 0.3067 - accuracy: 0.8720\n",
      "Epoch 197/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3117 - accuracy: 0.8619\n",
      "Epoch 198/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3180 - accuracy: 0.8593\n",
      "Epoch 199/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3113 - accuracy: 0.8627\n",
      "Epoch 200/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3142 - accuracy: 0.8653\n",
      "Epoch 201/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3080 - accuracy: 0.8661\n",
      "Epoch 202/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3054 - accuracy: 0.8684\n",
      "Epoch 203/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3184 - accuracy: 0.8617\n",
      "Epoch 204/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3135 - accuracy: 0.8622\n",
      "Epoch 205/500\n",
      "39/39 [==============================] - 0s 1ms/step - loss: 0.3078 - accuracy: 0.8627\n",
      "Epoch 206/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3096 - accuracy: 0.8650\n",
      "Epoch 207/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3155 - accuracy: 0.8619\n",
      "Epoch 208/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3063 - accuracy: 0.8679\n",
      "Epoch 209/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3015 - accuracy: 0.8661\n",
      "Epoch 210/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3047 - accuracy: 0.8661\n",
      "Epoch 211/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3004 - accuracy: 0.8671\n",
      "Epoch 212/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3026 - accuracy: 0.8679\n",
      "Epoch 213/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3071 - accuracy: 0.8648\n",
      "Epoch 214/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3131 - accuracy: 0.8642\n",
      "Epoch 215/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3047 - accuracy: 0.8661\n",
      "Epoch 216/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3047 - accuracy: 0.8671\n",
      "Epoch 217/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3133 - accuracy: 0.8619: 0s - loss: 0.3141 - accuracy: 0.85\n",
      "Epoch 218/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3195 - accuracy: 0.8624\n",
      "Epoch 219/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3142 - accuracy: 0.8617\n",
      "Epoch 220/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3031 - accuracy: 0.8720\n",
      "Epoch 221/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3112 - accuracy: 0.8650\n",
      "Epoch 222/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3020 - accuracy: 0.8663\n",
      "Epoch 223/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3013 - accuracy: 0.8674\n",
      "Epoch 224/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3159 - accuracy: 0.8614\n",
      "Epoch 225/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3099 - accuracy: 0.8611\n",
      "Epoch 226/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3045 - accuracy: 0.8671\n",
      "Epoch 227/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3011 - accuracy: 0.8674\n",
      "Epoch 228/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3135 - accuracy: 0.8622\n",
      "Epoch 229/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3140 - accuracy: 0.8637\n",
      "Epoch 230/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2990 - accuracy: 0.8707\n",
      "Epoch 231/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3059 - accuracy: 0.8640\n",
      "Epoch 232/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3084 - accuracy: 0.8661\n",
      "Epoch 233/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3182 - accuracy: 0.8617\n",
      "Epoch 234/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3160 - accuracy: 0.8604\n",
      "Epoch 235/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3090 - accuracy: 0.8601\n",
      "Epoch 236/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3126 - accuracy: 0.8648\n",
      "Epoch 237/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3060 - accuracy: 0.8692\n",
      "Epoch 238/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2975 - accuracy: 0.8723\n",
      "Epoch 239/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3060 - accuracy: 0.8702\n",
      "Epoch 240/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3038 - accuracy: 0.8679\n",
      "Epoch 241/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3132 - accuracy: 0.8598\n",
      "Epoch 242/500\n",
      "39/39 [==============================] - 0s 3ms/step - loss: 0.2969 - accuracy: 0.8723\n",
      "Epoch 243/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3072 - accuracy: 0.8642\n",
      "Epoch 244/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3026 - accuracy: 0.8663\n",
      "Epoch 245/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3255 - accuracy: 0.8575\n",
      "Epoch 246/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3088 - accuracy: 0.8674\n",
      "Epoch 247/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3002 - accuracy: 0.8674\n",
      "Epoch 248/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3001 - accuracy: 0.8671\n",
      "Epoch 249/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3030 - accuracy: 0.8653\n",
      "Epoch 250/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3002 - accuracy: 0.8754\n",
      "Epoch 251/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2953 - accuracy: 0.8725\n",
      "Epoch 252/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3092 - accuracy: 0.8617\n",
      "Epoch 253/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2995 - accuracy: 0.8692\n",
      "Epoch 254/500\n",
      "39/39 [==============================] - 0s 1ms/step - loss: 0.2967 - accuracy: 0.8697\n",
      "Epoch 255/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3174 - accuracy: 0.8604\n",
      "Epoch 256/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2976 - accuracy: 0.8705\n",
      "Epoch 257/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2914 - accuracy: 0.8715\n",
      "Epoch 258/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3039 - accuracy: 0.8671\n",
      "Epoch 259/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2953 - accuracy: 0.8707\n",
      "Epoch 260/500\n",
      "39/39 [==============================] - 0s 1ms/step - loss: 0.2942 - accuracy: 0.8715\n",
      "Epoch 261/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2950 - accuracy: 0.8687\n",
      "Epoch 262/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2916 - accuracy: 0.8741\n",
      "Epoch 263/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3156 - accuracy: 0.8627\n",
      "Epoch 264/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2959 - accuracy: 0.8744\n",
      "Epoch 265/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3005 - accuracy: 0.8702\n",
      "Epoch 266/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2922 - accuracy: 0.8723\n",
      "Epoch 267/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2961 - accuracy: 0.8746\n",
      "Epoch 268/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3052 - accuracy: 0.8622\n",
      "Epoch 269/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3019 - accuracy: 0.8746\n",
      "Epoch 270/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2989 - accuracy: 0.8694\n",
      "Epoch 271/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2919 - accuracy: 0.8725\n",
      "Epoch 272/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2968 - accuracy: 0.8718\n",
      "Epoch 273/500\n",
      "39/39 [==============================] - 0s 3ms/step - loss: 0.3025 - accuracy: 0.8715\n",
      "Epoch 274/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2967 - accuracy: 0.8697\n",
      "Epoch 275/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3032 - accuracy: 0.8705\n",
      "Epoch 276/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2920 - accuracy: 0.8738\n",
      "Epoch 277/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3101 - accuracy: 0.8627\n",
      "Epoch 278/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3097 - accuracy: 0.8645\n",
      "Epoch 279/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2964 - accuracy: 0.8756\n",
      "Epoch 280/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2955 - accuracy: 0.8694\n",
      "Epoch 281/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2946 - accuracy: 0.8699\n",
      "Epoch 282/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3135 - accuracy: 0.8687\n",
      "Epoch 283/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3067 - accuracy: 0.8679\n",
      "Epoch 284/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3091 - accuracy: 0.8666\n",
      "Epoch 285/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3149 - accuracy: 0.8588\n",
      "Epoch 286/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3013 - accuracy: 0.8705\n",
      "Epoch 287/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3064 - accuracy: 0.8653\n",
      "Epoch 288/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3204 - accuracy: 0.8596\n",
      "Epoch 289/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3041 - accuracy: 0.8710\n",
      "Epoch 290/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2962 - accuracy: 0.8681\n",
      "Epoch 291/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2941 - accuracy: 0.8707\n",
      "Epoch 292/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3028 - accuracy: 0.8692\n",
      "Epoch 293/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3220 - accuracy: 0.8575\n",
      "Epoch 294/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2944 - accuracy: 0.8725\n",
      "Epoch 295/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2902 - accuracy: 0.8720\n",
      "Epoch 296/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2901 - accuracy: 0.8767\n",
      "Epoch 297/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2945 - accuracy: 0.8723\n",
      "Epoch 298/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2928 - accuracy: 0.8744\n",
      "Epoch 299/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2935 - accuracy: 0.8723\n",
      "Epoch 300/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2912 - accuracy: 0.8720\n",
      "Epoch 301/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2902 - accuracy: 0.8720\n",
      "Epoch 302/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2967 - accuracy: 0.8699\n",
      "Epoch 303/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2954 - accuracy: 0.8707\n",
      "Epoch 304/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2857 - accuracy: 0.8795\n",
      "Epoch 305/500\n",
      "39/39 [==============================] - 0s 1ms/step - loss: 0.2899 - accuracy: 0.8697\n",
      "Epoch 306/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2852 - accuracy: 0.8741\n",
      "Epoch 307/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2817 - accuracy: 0.8777\n",
      "Epoch 308/500\n",
      "39/39 [==============================] - 0s 3ms/step - loss: 0.2939 - accuracy: 0.8681\n",
      "Epoch 309/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2978 - accuracy: 0.8712\n",
      "Epoch 310/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2937 - accuracy: 0.8699\n",
      "Epoch 311/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2806 - accuracy: 0.8803\n",
      "Epoch 312/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2878 - accuracy: 0.8751\n",
      "Epoch 313/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3017 - accuracy: 0.8725\n",
      "Epoch 314/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2851 - accuracy: 0.8728\n",
      "Epoch 315/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2897 - accuracy: 0.8736\n",
      "Epoch 316/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3058 - accuracy: 0.8650\n",
      "Epoch 317/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2889 - accuracy: 0.8775\n",
      "Epoch 318/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2980 - accuracy: 0.8775\n",
      "Epoch 319/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2999 - accuracy: 0.8707\n",
      "Epoch 320/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2919 - accuracy: 0.8751\n",
      "Epoch 321/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2897 - accuracy: 0.8769\n",
      "Epoch 322/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2939 - accuracy: 0.8733\n",
      "Epoch 323/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2821 - accuracy: 0.8746\n",
      "Epoch 324/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2839 - accuracy: 0.8751\n",
      "Epoch 325/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2876 - accuracy: 0.8767\n",
      "Epoch 326/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2932 - accuracy: 0.8707\n",
      "Epoch 327/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3124 - accuracy: 0.8580\n",
      "Epoch 328/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3025 - accuracy: 0.8668\n",
      "Epoch 329/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2850 - accuracy: 0.8780\n",
      "Epoch 330/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2882 - accuracy: 0.8746\n",
      "Epoch 331/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2887 - accuracy: 0.8785\n",
      "Epoch 332/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2876 - accuracy: 0.8723\n",
      "Epoch 333/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2938 - accuracy: 0.8723\n",
      "Epoch 334/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2924 - accuracy: 0.8699\n",
      "Epoch 335/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2833 - accuracy: 0.8795\n",
      "Epoch 336/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2994 - accuracy: 0.8715\n",
      "Epoch 337/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2952 - accuracy: 0.8718\n",
      "Epoch 338/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2893 - accuracy: 0.8718\n",
      "Epoch 339/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2793 - accuracy: 0.8803\n",
      "Epoch 340/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2752 - accuracy: 0.8816\n",
      "Epoch 341/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2801 - accuracy: 0.8816\n",
      "Epoch 342/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2982 - accuracy: 0.8738\n",
      "Epoch 343/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2991 - accuracy: 0.8661\n",
      "Epoch 344/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3014 - accuracy: 0.8702\n",
      "Epoch 345/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2924 - accuracy: 0.8710\n",
      "Epoch 346/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3035 - accuracy: 0.8671\n",
      "Epoch 347/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2912 - accuracy: 0.8749\n",
      "Epoch 348/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3033 - accuracy: 0.8728\n",
      "Epoch 349/500\n",
      "39/39 [==============================] - 0s 1ms/step - loss: 0.2948 - accuracy: 0.8744\n",
      "Epoch 350/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2860 - accuracy: 0.8782\n",
      "Epoch 351/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2851 - accuracy: 0.8767\n",
      "Epoch 352/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2898 - accuracy: 0.8782\n",
      "Epoch 353/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2838 - accuracy: 0.8803\n",
      "Epoch 354/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2840 - accuracy: 0.8754\n",
      "Epoch 355/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2937 - accuracy: 0.8780\n",
      "Epoch 356/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3037 - accuracy: 0.8707\n",
      "Epoch 357/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2999 - accuracy: 0.8697\n",
      "Epoch 358/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2902 - accuracy: 0.8744\n",
      "Epoch 359/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2882 - accuracy: 0.8772\n",
      "Epoch 360/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2838 - accuracy: 0.8733\n",
      "Epoch 361/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2720 - accuracy: 0.8816\n",
      "Epoch 362/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3020 - accuracy: 0.8642\n",
      "Epoch 363/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2864 - accuracy: 0.8723\n",
      "Epoch 364/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2907 - accuracy: 0.8736\n",
      "Epoch 365/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2857 - accuracy: 0.8793\n",
      "Epoch 366/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2822 - accuracy: 0.8803\n",
      "Epoch 367/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2844 - accuracy: 0.8793\n",
      "Epoch 368/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2843 - accuracy: 0.8780\n",
      "Epoch 369/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2849 - accuracy: 0.8759\n",
      "Epoch 370/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2751 - accuracy: 0.8801\n",
      "Epoch 371/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2890 - accuracy: 0.8762\n",
      "Epoch 372/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2811 - accuracy: 0.8795\n",
      "Epoch 373/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2896 - accuracy: 0.8697\n",
      "Epoch 374/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3097 - accuracy: 0.8588\n",
      "Epoch 375/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3018 - accuracy: 0.8591\n",
      "Epoch 376/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3060 - accuracy: 0.8637\n",
      "Epoch 377/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2799 - accuracy: 0.8839\n",
      "Epoch 378/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2777 - accuracy: 0.8801\n",
      "Epoch 379/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2769 - accuracy: 0.8816\n",
      "Epoch 380/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2894 - accuracy: 0.8772\n",
      "Epoch 381/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2717 - accuracy: 0.8821\n",
      "Epoch 382/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2836 - accuracy: 0.8793\n",
      "Epoch 383/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2709 - accuracy: 0.8847\n",
      "Epoch 384/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2738 - accuracy: 0.8858\n",
      "Epoch 385/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2687 - accuracy: 0.8845\n",
      "Epoch 386/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2799 - accuracy: 0.8788\n",
      "Epoch 387/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2732 - accuracy: 0.8811\n",
      "Epoch 388/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2770 - accuracy: 0.8793\n",
      "Epoch 389/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2833 - accuracy: 0.8764\n",
      "Epoch 390/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2767 - accuracy: 0.8795\n",
      "Epoch 391/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2694 - accuracy: 0.8845\n",
      "Epoch 392/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2894 - accuracy: 0.8720\n",
      "Epoch 393/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2768 - accuracy: 0.8782\n",
      "Epoch 394/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2747 - accuracy: 0.8788\n",
      "Epoch 395/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2711 - accuracy: 0.8832\n",
      "Epoch 396/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2795 - accuracy: 0.8798\n",
      "Epoch 397/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3156 - accuracy: 0.8567\n",
      "Epoch 398/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2919 - accuracy: 0.8697\n",
      "Epoch 399/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2741 - accuracy: 0.8754\n",
      "Epoch 400/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2953 - accuracy: 0.8741\n",
      "Epoch 401/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2744 - accuracy: 0.8801\n",
      "Epoch 402/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2963 - accuracy: 0.8697\n",
      "Epoch 403/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3065 - accuracy: 0.8591\n",
      "Epoch 404/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2947 - accuracy: 0.8635\n",
      "Epoch 405/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2930 - accuracy: 0.8736\n",
      "Epoch 406/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2874 - accuracy: 0.8723\n",
      "Epoch 407/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2772 - accuracy: 0.8782\n",
      "Epoch 408/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2788 - accuracy: 0.8772\n",
      "Epoch 409/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2831 - accuracy: 0.8728\n",
      "Epoch 410/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2851 - accuracy: 0.8756\n",
      "Epoch 411/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2718 - accuracy: 0.8772\n",
      "Epoch 412/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2798 - accuracy: 0.8744\n",
      "Epoch 413/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2733 - accuracy: 0.8782\n",
      "Epoch 414/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2953 - accuracy: 0.8720\n",
      "Epoch 415/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2896 - accuracy: 0.8715\n",
      "Epoch 416/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2763 - accuracy: 0.8824\n",
      "Epoch 417/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2808 - accuracy: 0.8782\n",
      "Epoch 418/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2978 - accuracy: 0.8718\n",
      "Epoch 419/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2917 - accuracy: 0.8671\n",
      "Epoch 420/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2737 - accuracy: 0.8801\n",
      "Epoch 421/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2811 - accuracy: 0.8754\n",
      "Epoch 422/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2812 - accuracy: 0.8697\n",
      "Epoch 423/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2755 - accuracy: 0.8764\n",
      "Epoch 424/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2755 - accuracy: 0.8808\n",
      "Epoch 425/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2829 - accuracy: 0.8728\n",
      "Epoch 426/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2826 - accuracy: 0.8754\n",
      "Epoch 427/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2839 - accuracy: 0.8687\n",
      "Epoch 428/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2721 - accuracy: 0.8850\n",
      "Epoch 429/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2794 - accuracy: 0.8777\n",
      "Epoch 430/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2747 - accuracy: 0.8793\n",
      "Epoch 431/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2619 - accuracy: 0.8863\n",
      "Epoch 432/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2803 - accuracy: 0.8785\n",
      "Epoch 433/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2797 - accuracy: 0.8793\n",
      "Epoch 434/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2691 - accuracy: 0.8834: 0s - loss: 0.2702 - accuracy: 0.88\n",
      "Epoch 435/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2677 - accuracy: 0.8889\n",
      "Epoch 436/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2779 - accuracy: 0.8795\n",
      "Epoch 437/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2757 - accuracy: 0.8803\n",
      "Epoch 438/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2770 - accuracy: 0.8832\n",
      "Epoch 439/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2655 - accuracy: 0.8876\n",
      "Epoch 440/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2700 - accuracy: 0.8826\n",
      "Epoch 441/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2831 - accuracy: 0.8793\n",
      "Epoch 442/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2842 - accuracy: 0.8731\n",
      "Epoch 443/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2695 - accuracy: 0.8816\n",
      "Epoch 444/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2669 - accuracy: 0.8829\n",
      "Epoch 445/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2757 - accuracy: 0.8816\n",
      "Epoch 446/500\n",
      "39/39 [==============================] - 0s 1ms/step - loss: 0.2717 - accuracy: 0.8842\n",
      "Epoch 447/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2640 - accuracy: 0.8873\n",
      "Epoch 448/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2595 - accuracy: 0.8927\n",
      "Epoch 449/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2663 - accuracy: 0.8845\n",
      "Epoch 450/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2782 - accuracy: 0.8759\n",
      "Epoch 451/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3013 - accuracy: 0.8648\n",
      "Epoch 452/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2762 - accuracy: 0.8780\n",
      "Epoch 453/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2879 - accuracy: 0.8801\n",
      "Epoch 454/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2653 - accuracy: 0.8816\n",
      "Epoch 455/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2575 - accuracy: 0.8891\n",
      "Epoch 456/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2668 - accuracy: 0.8852\n",
      "Epoch 457/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2660 - accuracy: 0.8816\n",
      "Epoch 458/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2667 - accuracy: 0.8855\n",
      "Epoch 459/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2806 - accuracy: 0.8790\n",
      "Epoch 460/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2578 - accuracy: 0.8896\n",
      "Epoch 461/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2583 - accuracy: 0.8868\n",
      "Epoch 462/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2641 - accuracy: 0.8868\n",
      "Epoch 463/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2704 - accuracy: 0.8816\n",
      "Epoch 464/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2641 - accuracy: 0.8845\n",
      "Epoch 465/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2527 - accuracy: 0.8927\n",
      "Epoch 466/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2594 - accuracy: 0.8907\n",
      "Epoch 467/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2687 - accuracy: 0.8909\n",
      "Epoch 468/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2627 - accuracy: 0.8826\n",
      "Epoch 469/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2577 - accuracy: 0.8920\n",
      "Epoch 470/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2521 - accuracy: 0.8891\n",
      "Epoch 471/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2582 - accuracy: 0.8889\n",
      "Epoch 472/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2582 - accuracy: 0.8868\n",
      "Epoch 473/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2641 - accuracy: 0.8832\n",
      "Epoch 474/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2713 - accuracy: 0.8767\n",
      "Epoch 475/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2704 - accuracy: 0.8826\n",
      "Epoch 476/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2553 - accuracy: 0.8953\n",
      "Epoch 477/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2616 - accuracy: 0.8847\n",
      "Epoch 478/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2617 - accuracy: 0.8858\n",
      "Epoch 479/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2799 - accuracy: 0.8767\n",
      "Epoch 480/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2586 - accuracy: 0.8889\n",
      "Epoch 481/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2599 - accuracy: 0.8876\n",
      "Epoch 482/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2574 - accuracy: 0.8881\n",
      "Epoch 483/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2566 - accuracy: 0.8889\n",
      "Epoch 484/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2522 - accuracy: 0.8943\n",
      "Epoch 485/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2562 - accuracy: 0.8896\n",
      "Epoch 486/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2489 - accuracy: 0.8927\n",
      "Epoch 487/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2605 - accuracy: 0.8842\n",
      "Epoch 488/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2565 - accuracy: 0.8870\n",
      "Epoch 489/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2719 - accuracy: 0.8899\n",
      "Epoch 490/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2452 - accuracy: 0.8935\n",
      "Epoch 491/500\n",
      "39/39 [==============================] - 0s 1ms/step - loss: 0.2483 - accuracy: 0.8946\n",
      "Epoch 492/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2424 - accuracy: 0.8990\n",
      "Epoch 493/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2595 - accuracy: 0.8839\n",
      "Epoch 494/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2769 - accuracy: 0.8741\n",
      "Epoch 495/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2715 - accuracy: 0.8738\n",
      "Epoch 496/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3029 - accuracy: 0.8630\n",
      "Epoch 497/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2773 - accuracy: 0.8790\n",
      "Epoch 498/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2505 - accuracy: 0.8920\n",
      "Epoch 499/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2638 - accuracy: 0.8850\n",
      "Epoch 500/500\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2485 - accuracy: 0.8891\n",
      "31/31 [==============================] - 0s 847us/step - loss: 0.4197 - accuracy: 0.8218\n"
     ]
    }
   ],
   "source": [
    "# Complete 4825: train=3860, test=965\n",
    "x_train_c, x_test_c, y_train_c, y_test_c = train_test_split(complete_x, complete_y_bin, \n",
    "                                                            test_size=0.2, random_state=1)\n",
    "\n",
    "history_c = model_clf.fit(x_train_c, y_train_c, epochs=500, batch_size=100)\n",
    "y_pred = model_clf.evaluate(x_test_c, y_test_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss: 0.3732 - accuracy: 0.8497"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.3486 - accuracy: 0.8800\n",
      "Epoch 2/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.3070 - accuracy: 0.8845\n",
      "Epoch 3/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.3023 - accuracy: 0.8857\n",
      "Epoch 4/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2893 - accuracy: 0.8856\n",
      "Epoch 5/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2863 - accuracy: 0.8861\n",
      "Epoch 6/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2842 - accuracy: 0.8856: 0s - loss: 0.2917 - accuracy: 0.\n",
      "Epoch 7/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2759 - accuracy: 0.8860\n",
      "Epoch 8/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2819 - accuracy: 0.8860\n",
      "Epoch 9/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2749 - accuracy: 0.8861\n",
      "Epoch 10/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2746 - accuracy: 0.8860\n",
      "Epoch 11/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2684 - accuracy: 0.8862\n",
      "Epoch 12/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2730 - accuracy: 0.8857\n",
      "Epoch 13/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2733 - accuracy: 0.8860\n",
      "Epoch 14/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2658 - accuracy: 0.8861\n",
      "Epoch 15/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2656 - accuracy: 0.8861\n",
      "Epoch 16/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2672 - accuracy: 0.8860\n",
      "Epoch 17/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2648 - accuracy: 0.8852\n",
      "Epoch 18/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2667 - accuracy: 0.8859\n",
      "Epoch 19/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2613 - accuracy: 0.8872\n",
      "Epoch 20/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2591 - accuracy: 0.8881\n",
      "Epoch 21/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2636 - accuracy: 0.8858\n",
      "Epoch 22/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2588 - accuracy: 0.8878\n",
      "Epoch 23/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2590 - accuracy: 0.8861\n",
      "Epoch 24/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2657 - accuracy: 0.8875\n",
      "Epoch 25/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2594 - accuracy: 0.8861\n",
      "Epoch 26/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2596 - accuracy: 0.8870\n",
      "Epoch 27/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2578 - accuracy: 0.8868\n",
      "Epoch 28/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2604 - accuracy: 0.8884\n",
      "Epoch 29/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2581 - accuracy: 0.8866\n",
      "Epoch 30/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2594 - accuracy: 0.8896\n",
      "Epoch 31/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2617 - accuracy: 0.8865\n",
      "Epoch 32/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2677 - accuracy: 0.8864\n",
      "Epoch 33/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2540 - accuracy: 0.8900\n",
      "Epoch 34/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2574 - accuracy: 0.8901\n",
      "Epoch 35/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2538 - accuracy: 0.8887\n",
      "Epoch 36/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2587 - accuracy: 0.8885\n",
      "Epoch 37/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2553 - accuracy: 0.8866\n",
      "Epoch 38/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2544 - accuracy: 0.8887\n",
      "Epoch 39/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2543 - accuracy: 0.8886\n",
      "Epoch 40/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2527 - accuracy: 0.8901\n",
      "Epoch 41/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2527 - accuracy: 0.8886\n",
      "Epoch 42/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2544 - accuracy: 0.8886\n",
      "Epoch 43/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2584 - accuracy: 0.8888\n",
      "Epoch 44/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2492 - accuracy: 0.8908\n",
      "Epoch 45/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.8897\n",
      "Epoch 46/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2577 - accuracy: 0.8894\n",
      "Epoch 47/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2482 - accuracy: 0.8918\n",
      "Epoch 48/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2503 - accuracy: 0.8904\n",
      "Epoch 49/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2486 - accuracy: 0.8897\n",
      "Epoch 50/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2528 - accuracy: 0.8890\n",
      "Epoch 51/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2555 - accuracy: 0.8906\n",
      "Epoch 52/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2537 - accuracy: 0.8895\n",
      "Epoch 53/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2540 - accuracy: 0.8900\n",
      "Epoch 54/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2516 - accuracy: 0.8901\n",
      "Epoch 55/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2481 - accuracy: 0.8912\n",
      "Epoch 56/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2492 - accuracy: 0.8910\n",
      "Epoch 57/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2525 - accuracy: 0.8893\n",
      "Epoch 58/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2520 - accuracy: 0.8893\n",
      "Epoch 59/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2576 - accuracy: 0.8917\n",
      "Epoch 60/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2538 - accuracy: 0.8896\n",
      "Epoch 61/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2526 - accuracy: 0.8900\n",
      "Epoch 62/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2490 - accuracy: 0.8891\n",
      "Epoch 63/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2541 - accuracy: 0.8901\n",
      "Epoch 64/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2517 - accuracy: 0.8921\n",
      "Epoch 65/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2531 - accuracy: 0.8882\n",
      "Epoch 66/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2538 - accuracy: 0.8879\n",
      "Epoch 67/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2468 - accuracy: 0.8915\n",
      "Epoch 68/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2542 - accuracy: 0.8881\n",
      "Epoch 69/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2466 - accuracy: 0.8921\n",
      "Epoch 70/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2482 - accuracy: 0.8924\n",
      "Epoch 71/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2530 - accuracy: 0.8876\n",
      "Epoch 72/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2522 - accuracy: 0.8890\n",
      "Epoch 73/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2512 - accuracy: 0.8919\n",
      "Epoch 74/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2540 - accuracy: 0.8911\n",
      "Epoch 75/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2468 - accuracy: 0.8914\n",
      "Epoch 76/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2472 - accuracy: 0.8918\n",
      "Epoch 77/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2510 - accuracy: 0.8900\n",
      "Epoch 78/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2558 - accuracy: 0.8904\n",
      "Epoch 79/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2463 - accuracy: 0.8921\n",
      "Epoch 80/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2488 - accuracy: 0.8907\n",
      "Epoch 81/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2489 - accuracy: 0.8906\n",
      "Epoch 82/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2545 - accuracy: 0.8880\n",
      "Epoch 83/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2470 - accuracy: 0.8915\n",
      "Epoch 84/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2534 - accuracy: 0.8900\n",
      "Epoch 85/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2467 - accuracy: 0.8921\n",
      "Epoch 86/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2505 - accuracy: 0.8917\n",
      "Epoch 87/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2460 - accuracy: 0.8922\n",
      "Epoch 88/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2493 - accuracy: 0.8904\n",
      "Epoch 89/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2456 - accuracy: 0.8916\n",
      "Epoch 90/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2480 - accuracy: 0.8913\n",
      "Epoch 91/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2485 - accuracy: 0.8908\n",
      "Epoch 92/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2455 - accuracy: 0.8919\n",
      "Epoch 93/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2460 - accuracy: 0.8911\n",
      "Epoch 94/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2446 - accuracy: 0.8934\n",
      "Epoch 95/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2440 - accuracy: 0.8923\n",
      "Epoch 96/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2503 - accuracy: 0.8916\n",
      "Epoch 97/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2470 - accuracy: 0.8929\n",
      "Epoch 98/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2450 - accuracy: 0.8936\n",
      "Epoch 99/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2452 - accuracy: 0.8928\n",
      "Epoch 100/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2488 - accuracy: 0.8927\n",
      "Epoch 101/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2446 - accuracy: 0.8923\n",
      "Epoch 102/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2498 - accuracy: 0.8926\n",
      "Epoch 103/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2488 - accuracy: 0.8907\n",
      "Epoch 104/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2451 - accuracy: 0.8921\n",
      "Epoch 105/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2474 - accuracy: 0.8921\n",
      "Epoch 106/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2482 - accuracy: 0.8918\n",
      "Epoch 107/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2492 - accuracy: 0.8908\n",
      "Epoch 108/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2467 - accuracy: 0.8904\n",
      "Epoch 109/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2530 - accuracy: 0.8918\n",
      "Epoch 110/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2456 - accuracy: 0.8903\n",
      "Epoch 111/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2460 - accuracy: 0.8924\n",
      "Epoch 112/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2433 - accuracy: 0.8933\n",
      "Epoch 113/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2472 - accuracy: 0.8915\n",
      "Epoch 114/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2509 - accuracy: 0.8886\n",
      "Epoch 115/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2544 - accuracy: 0.8923\n",
      "Epoch 116/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2469 - accuracy: 0.8908\n",
      "Epoch 117/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2465 - accuracy: 0.8940\n",
      "Epoch 118/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2423 - accuracy: 0.8942\n",
      "Epoch 119/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2453 - accuracy: 0.8939\n",
      "Epoch 120/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2429 - accuracy: 0.8929\n",
      "Epoch 121/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2473 - accuracy: 0.8911\n",
      "Epoch 122/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2535 - accuracy: 0.8908\n",
      "Epoch 123/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2510 - accuracy: 0.8902\n",
      "Epoch 124/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2446 - accuracy: 0.8917\n",
      "Epoch 125/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2440 - accuracy: 0.8928\n",
      "Epoch 126/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2489 - accuracy: 0.8887\n",
      "Epoch 127/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2445 - accuracy: 0.8928\n",
      "Epoch 128/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2442 - accuracy: 0.8934\n",
      "Epoch 129/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2450 - accuracy: 0.8930\n",
      "Epoch 130/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2526 - accuracy: 0.8926\n",
      "Epoch 131/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2449 - accuracy: 0.8931\n",
      "Epoch 132/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2499 - accuracy: 0.8923\n",
      "Epoch 133/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2455 - accuracy: 0.8942\n",
      "Epoch 134/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2437 - accuracy: 0.8926\n",
      "Epoch 135/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2445 - accuracy: 0.8922\n",
      "Epoch 136/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2438 - accuracy: 0.8918\n",
      "Epoch 137/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2476 - accuracy: 0.8941\n",
      "Epoch 138/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2472 - accuracy: 0.8917\n",
      "Epoch 139/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2412 - accuracy: 0.8951\n",
      "Epoch 140/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2406 - accuracy: 0.8951\n",
      "Epoch 141/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2492 - accuracy: 0.8923\n",
      "Epoch 142/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2426 - accuracy: 0.8928\n",
      "Epoch 143/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2451 - accuracy: 0.8928\n",
      "Epoch 144/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2433 - accuracy: 0.8930\n",
      "Epoch 145/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2441 - accuracy: 0.8948\n",
      "Epoch 146/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2468 - accuracy: 0.8931\n",
      "Epoch 147/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2420 - accuracy: 0.8951\n",
      "Epoch 148/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2440 - accuracy: 0.8945\n",
      "Epoch 149/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2422 - accuracy: 0.8938\n",
      "Epoch 150/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2417 - accuracy: 0.8951\n",
      "Epoch 151/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2401 - accuracy: 0.8964\n",
      "Epoch 152/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2419 - accuracy: 0.8951\n",
      "Epoch 153/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2452 - accuracy: 0.8951\n",
      "Epoch 154/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2420 - accuracy: 0.8944\n",
      "Epoch 155/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2414 - accuracy: 0.8946\n",
      "Epoch 156/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2466 - accuracy: 0.8930\n",
      "Epoch 157/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2461 - accuracy: 0.8931\n",
      "Epoch 158/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2448 - accuracy: 0.8928\n",
      "Epoch 159/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2466 - accuracy: 0.8916\n",
      "Epoch 160/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2422 - accuracy: 0.8930\n",
      "Epoch 161/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2457 - accuracy: 0.8919\n",
      "Epoch 162/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2437 - accuracy: 0.8921\n",
      "Epoch 163/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2493 - accuracy: 0.8929\n",
      "Epoch 164/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2446 - accuracy: 0.8936\n",
      "Epoch 165/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.8944\n",
      "Epoch 166/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2396 - accuracy: 0.8951\n",
      "Epoch 167/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2444 - accuracy: 0.8931\n",
      "Epoch 168/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2499 - accuracy: 0.8900\n",
      "Epoch 169/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2405 - accuracy: 0.8949\n",
      "Epoch 170/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2429 - accuracy: 0.8922\n",
      "Epoch 171/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2426 - accuracy: 0.8926\n",
      "Epoch 172/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2427 - accuracy: 0.8911\n",
      "Epoch 173/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2460 - accuracy: 0.8935\n",
      "Epoch 174/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2415 - accuracy: 0.8934\n",
      "Epoch 175/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2411 - accuracy: 0.8940\n",
      "Epoch 176/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2413 - accuracy: 0.8923\n",
      "Epoch 177/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2427 - accuracy: 0.8927\n",
      "Epoch 178/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2411 - accuracy: 0.8941\n",
      "Epoch 179/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2462 - accuracy: 0.8916\n",
      "Epoch 180/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2404 - accuracy: 0.8934\n",
      "Epoch 181/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2424 - accuracy: 0.8918\n",
      "Epoch 182/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2404 - accuracy: 0.8933\n",
      "Epoch 183/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2400 - accuracy: 0.8939\n",
      "Epoch 184/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2417 - accuracy: 0.8939\n",
      "Epoch 185/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2460 - accuracy: 0.8923\n",
      "Epoch 186/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2434 - accuracy: 0.8934\n",
      "Epoch 187/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2386 - accuracy: 0.8952\n",
      "Epoch 188/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2406 - accuracy: 0.8926\n",
      "Epoch 189/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2399 - accuracy: 0.8941\n",
      "Epoch 190/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2392 - accuracy: 0.8934\n",
      "Epoch 191/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2403 - accuracy: 0.8921\n",
      "Epoch 192/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2505 - accuracy: 0.8921\n",
      "Epoch 193/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2391 - accuracy: 0.8951\n",
      "Epoch 194/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2389 - accuracy: 0.8943\n",
      "Epoch 195/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2380 - accuracy: 0.8951\n",
      "Epoch 196/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2393 - accuracy: 0.8936\n",
      "Epoch 197/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2386 - accuracy: 0.8951\n",
      "Epoch 198/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2383 - accuracy: 0.8941\n",
      "Epoch 199/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2398 - accuracy: 0.8943\n",
      "Epoch 200/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2390 - accuracy: 0.8946\n",
      "Epoch 201/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2380 - accuracy: 0.8941\n",
      "Epoch 202/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2380 - accuracy: 0.8967\n",
      "Epoch 203/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2364 - accuracy: 0.8958\n",
      "Epoch 204/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2378 - accuracy: 0.8954\n",
      "Epoch 205/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2355 - accuracy: 0.8952\n",
      "Epoch 206/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2422 - accuracy: 0.8923\n",
      "Epoch 207/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2395 - accuracy: 0.8949\n",
      "Epoch 208/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2369 - accuracy: 0.8946\n",
      "Epoch 209/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2356 - accuracy: 0.8949\n",
      "Epoch 210/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2362 - accuracy: 0.8956\n",
      "Epoch 211/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2375 - accuracy: 0.8961\n",
      "Epoch 212/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2379 - accuracy: 0.8945\n",
      "Epoch 213/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2398 - accuracy: 0.8927\n",
      "Epoch 214/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2389 - accuracy: 0.8934\n",
      "Epoch 215/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2374 - accuracy: 0.8948\n",
      "Epoch 216/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2390 - accuracy: 0.8945\n",
      "Epoch 217/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2362 - accuracy: 0.8951\n",
      "Epoch 218/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2440 - accuracy: 0.8933\n",
      "Epoch 219/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2384 - accuracy: 0.8965\n",
      "Epoch 220/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2344 - accuracy: 0.8960\n",
      "Epoch 221/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2338 - accuracy: 0.8956\n",
      "Epoch 222/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2349 - accuracy: 0.8949\n",
      "Epoch 223/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2377 - accuracy: 0.8958\n",
      "Epoch 224/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2370 - accuracy: 0.8961\n",
      "Epoch 225/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2373 - accuracy: 0.8939\n",
      "Epoch 226/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2344 - accuracy: 0.8958\n",
      "Epoch 227/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2366 - accuracy: 0.8963\n",
      "Epoch 228/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2349 - accuracy: 0.8978\n",
      "Epoch 229/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2376 - accuracy: 0.8945\n",
      "Epoch 230/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2342 - accuracy: 0.8949\n",
      "Epoch 231/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2396 - accuracy: 0.8935\n",
      "Epoch 232/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2352 - accuracy: 0.8953\n",
      "Epoch 233/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2386 - accuracy: 0.8940\n",
      "Epoch 234/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2357 - accuracy: 0.8946\n",
      "Epoch 235/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2320 - accuracy: 0.8934\n",
      "Epoch 236/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2348 - accuracy: 0.8971\n",
      "Epoch 237/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2327 - accuracy: 0.8979\n",
      "Epoch 238/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2356 - accuracy: 0.8947\n",
      "Epoch 239/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2363 - accuracy: 0.8954\n",
      "Epoch 240/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2372 - accuracy: 0.8963\n",
      "Epoch 241/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2354 - accuracy: 0.8952\n",
      "Epoch 242/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2421 - accuracy: 0.8921\n",
      "Epoch 243/500\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.2370 - accuracy: 0.89 - 0s 2ms/step - loss: 0.2340 - accuracy: 0.8975\n",
      "Epoch 244/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2305 - accuracy: 0.8974\n",
      "Epoch 245/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2355 - accuracy: 0.8964\n",
      "Epoch 246/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2361 - accuracy: 0.8933\n",
      "Epoch 247/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2450 - accuracy: 0.8913\n",
      "Epoch 248/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2398 - accuracy: 0.8928\n",
      "Epoch 249/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2390 - accuracy: 0.8931\n",
      "Epoch 250/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2331 - accuracy: 0.8962\n",
      "Epoch 251/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2338 - accuracy: 0.8971\n",
      "Epoch 252/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2401 - accuracy: 0.8945\n",
      "Epoch 253/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2336 - accuracy: 0.8975\n",
      "Epoch 254/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2359 - accuracy: 0.8950\n",
      "Epoch 255/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2335 - accuracy: 0.8947\n",
      "Epoch 256/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2300 - accuracy: 0.8978\n",
      "Epoch 257/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2371 - accuracy: 0.8946\n",
      "Epoch 258/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2314 - accuracy: 0.8971\n",
      "Epoch 259/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2301 - accuracy: 0.8986\n",
      "Epoch 260/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2327 - accuracy: 0.8974\n",
      "Epoch 261/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2310 - accuracy: 0.8968\n",
      "Epoch 262/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2326 - accuracy: 0.8966\n",
      "Epoch 263/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2333 - accuracy: 0.8961\n",
      "Epoch 264/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2375 - accuracy: 0.8946\n",
      "Epoch 265/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2330 - accuracy: 0.8962\n",
      "Epoch 266/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2343 - accuracy: 0.8970\n",
      "Epoch 267/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2329 - accuracy: 0.8952\n",
      "Epoch 268/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2334 - accuracy: 0.8956\n",
      "Epoch 269/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2326 - accuracy: 0.8984\n",
      "Epoch 270/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2317 - accuracy: 0.8956\n",
      "Epoch 271/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2322 - accuracy: 0.8965\n",
      "Epoch 272/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2324 - accuracy: 0.8954\n",
      "Epoch 273/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2299 - accuracy: 0.8980\n",
      "Epoch 274/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2354 - accuracy: 0.8984\n",
      "Epoch 275/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2320 - accuracy: 0.8967\n",
      "Epoch 276/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2282 - accuracy: 0.8981\n",
      "Epoch 277/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2328 - accuracy: 0.8963\n",
      "Epoch 278/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2341 - accuracy: 0.8968\n",
      "Epoch 279/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2347 - accuracy: 0.8973\n",
      "Epoch 280/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2301 - accuracy: 0.8993\n",
      "Epoch 281/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2334 - accuracy: 0.8950\n",
      "Epoch 282/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2327 - accuracy: 0.8975\n",
      "Epoch 283/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2379 - accuracy: 0.8951\n",
      "Epoch 284/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2322 - accuracy: 0.8978\n",
      "Epoch 285/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2294 - accuracy: 0.8991\n",
      "Epoch 286/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2301 - accuracy: 0.8977\n",
      "Epoch 287/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2353 - accuracy: 0.8936\n",
      "Epoch 288/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2319 - accuracy: 0.8996\n",
      "Epoch 289/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2299 - accuracy: 0.8976\n",
      "Epoch 290/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2265 - accuracy: 0.8986\n",
      "Epoch 291/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2299 - accuracy: 0.8992\n",
      "Epoch 292/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2302 - accuracy: 0.8986\n",
      "Epoch 293/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2293 - accuracy: 0.8965\n",
      "Epoch 294/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2314 - accuracy: 0.8971\n",
      "Epoch 295/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2258 - accuracy: 0.8985\n",
      "Epoch 296/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2338 - accuracy: 0.8964\n",
      "Epoch 297/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2344 - accuracy: 0.8965\n",
      "Epoch 298/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2274 - accuracy: 0.8979\n",
      "Epoch 299/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2320 - accuracy: 0.8956\n",
      "Epoch 300/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2333 - accuracy: 0.8964\n",
      "Epoch 301/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2282 - accuracy: 0.8985\n",
      "Epoch 302/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2352 - accuracy: 0.8954\n",
      "Epoch 303/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2297 - accuracy: 0.8984\n",
      "Epoch 304/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2313 - accuracy: 0.8976\n",
      "Epoch 305/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2281 - accuracy: 0.8960\n",
      "Epoch 306/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2328 - accuracy: 0.8961\n",
      "Epoch 307/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2308 - accuracy: 0.8981\n",
      "Epoch 308/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2297 - accuracy: 0.8996\n",
      "Epoch 309/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2275 - accuracy: 0.8985\n",
      "Epoch 310/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2299 - accuracy: 0.8974\n",
      "Epoch 311/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2246 - accuracy: 0.9005\n",
      "Epoch 312/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2276 - accuracy: 0.8992\n",
      "Epoch 313/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2272 - accuracy: 0.9005\n",
      "Epoch 314/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2331 - accuracy: 0.8951\n",
      "Epoch 315/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2278 - accuracy: 0.8982\n",
      "Epoch 316/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2279 - accuracy: 0.8974\n",
      "Epoch 317/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2310 - accuracy: 0.8990\n",
      "Epoch 318/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2234 - accuracy: 0.9016\n",
      "Epoch 319/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2244 - accuracy: 0.8998\n",
      "Epoch 320/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2329 - accuracy: 0.8966\n",
      "Epoch 321/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2265 - accuracy: 0.8989\n",
      "Epoch 322/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2229 - accuracy: 0.9002\n",
      "Epoch 323/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2269 - accuracy: 0.8987\n",
      "Epoch 324/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2272 - accuracy: 0.8996\n",
      "Epoch 325/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2304 - accuracy: 0.8981\n",
      "Epoch 326/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2247 - accuracy: 0.8999\n",
      "Epoch 327/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2238 - accuracy: 0.9007\n",
      "Epoch 328/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2317 - accuracy: 0.8988\n",
      "Epoch 329/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2267 - accuracy: 0.8991\n",
      "Epoch 330/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2240 - accuracy: 0.9010\n",
      "Epoch 331/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2265 - accuracy: 0.8984\n",
      "Epoch 332/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2256 - accuracy: 0.8989\n",
      "Epoch 333/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2261 - accuracy: 0.8996\n",
      "Epoch 334/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2212 - accuracy: 0.9028\n",
      "Epoch 335/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2246 - accuracy: 0.8998\n",
      "Epoch 336/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2262 - accuracy: 0.8993\n",
      "Epoch 337/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2217 - accuracy: 0.9029\n",
      "Epoch 338/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2246 - accuracy: 0.8995\n",
      "Epoch 339/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2184 - accuracy: 0.9029\n",
      "Epoch 340/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2242 - accuracy: 0.8995\n",
      "Epoch 341/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2355 - accuracy: 0.8969\n",
      "Epoch 342/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2332 - accuracy: 0.8964\n",
      "Epoch 343/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2212 - accuracy: 0.9007\n",
      "Epoch 344/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2313 - accuracy: 0.8965\n",
      "Epoch 345/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2212 - accuracy: 0.9004\n",
      "Epoch 346/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2306 - accuracy: 0.8952\n",
      "Epoch 347/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2267 - accuracy: 0.9008\n",
      "Epoch 348/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2209 - accuracy: 0.8996\n",
      "Epoch 349/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2257 - accuracy: 0.9004\n",
      "Epoch 350/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2205 - accuracy: 0.9031\n",
      "Epoch 351/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2223 - accuracy: 0.9024\n",
      "Epoch 352/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2203 - accuracy: 0.9033\n",
      "Epoch 353/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2199 - accuracy: 0.9032\n",
      "Epoch 354/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2230 - accuracy: 0.9019\n",
      "Epoch 355/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2283 - accuracy: 0.8981\n",
      "Epoch 356/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2250 - accuracy: 0.8993\n",
      "Epoch 357/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2252 - accuracy: 0.9007\n",
      "Epoch 358/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2237 - accuracy: 0.9004\n",
      "Epoch 359/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2274 - accuracy: 0.8977\n",
      "Epoch 360/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2239 - accuracy: 0.9030\n",
      "Epoch 361/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2230 - accuracy: 0.9009\n",
      "Epoch 362/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2244 - accuracy: 0.9001\n",
      "Epoch 363/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2237 - accuracy: 0.8995\n",
      "Epoch 364/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2199 - accuracy: 0.9007: 0s - loss: 0.2202 - accuracy: 0.89\n",
      "Epoch 365/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2163 - accuracy: 0.9032\n",
      "Epoch 366/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2211 - accuracy: 0.9016\n",
      "Epoch 367/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2159 - accuracy: 0.9051\n",
      "Epoch 368/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2170 - accuracy: 0.9020\n",
      "Epoch 369/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2227 - accuracy: 0.9006\n",
      "Epoch 370/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2272 - accuracy: 0.9004\n",
      "Epoch 371/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2184 - accuracy: 0.9017\n",
      "Epoch 372/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2205 - accuracy: 0.9023\n",
      "Epoch 373/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2209 - accuracy: 0.9025\n",
      "Epoch 374/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2254 - accuracy: 0.9005\n",
      "Epoch 375/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2185 - accuracy: 0.9040\n",
      "Epoch 376/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2198 - accuracy: 0.9031\n",
      "Epoch 377/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2172 - accuracy: 0.9049\n",
      "Epoch 378/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2219 - accuracy: 0.9012\n",
      "Epoch 379/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2159 - accuracy: 0.9075\n",
      "Epoch 380/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2169 - accuracy: 0.9036\n",
      "Epoch 381/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2216 - accuracy: 0.9028\n",
      "Epoch 382/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2234 - accuracy: 0.9016\n",
      "Epoch 383/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2169 - accuracy: 0.9040\n",
      "Epoch 384/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2131 - accuracy: 0.9067\n",
      "Epoch 385/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2182 - accuracy: 0.9027\n",
      "Epoch 386/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2240 - accuracy: 0.9015\n",
      "Epoch 387/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2165 - accuracy: 0.9053\n",
      "Epoch 388/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2152 - accuracy: 0.9057\n",
      "Epoch 389/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2130 - accuracy: 0.9062\n",
      "Epoch 390/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2210 - accuracy: 0.9006\n",
      "Epoch 391/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2164 - accuracy: 0.9056\n",
      "Epoch 392/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2169 - accuracy: 0.9071\n",
      "Epoch 393/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2189 - accuracy: 0.9033\n",
      "Epoch 394/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2166 - accuracy: 0.9039\n",
      "Epoch 395/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2122 - accuracy: 0.9070\n",
      "Epoch 396/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2131 - accuracy: 0.9067\n",
      "Epoch 397/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2138 - accuracy: 0.9074\n",
      "Epoch 398/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2160 - accuracy: 0.9040\n",
      "Epoch 399/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2178 - accuracy: 0.9013\n",
      "Epoch 400/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2208 - accuracy: 0.9015\n",
      "Epoch 401/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2133 - accuracy: 0.9047\n",
      "Epoch 402/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2117 - accuracy: 0.9066\n",
      "Epoch 403/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2143 - accuracy: 0.9047\n",
      "Epoch 404/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2097 - accuracy: 0.9067\n",
      "Epoch 405/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2149 - accuracy: 0.9039\n",
      "Epoch 406/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2139 - accuracy: 0.9057\n",
      "Epoch 407/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2111 - accuracy: 0.9071\n",
      "Epoch 408/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2151 - accuracy: 0.9057\n",
      "Epoch 409/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2133 - accuracy: 0.9082\n",
      "Epoch 410/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2127 - accuracy: 0.9042\n",
      "Epoch 411/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2143 - accuracy: 0.9056\n",
      "Epoch 412/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2127 - accuracy: 0.9053\n",
      "Epoch 413/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2203 - accuracy: 0.9020\n",
      "Epoch 414/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2105 - accuracy: 0.9068\n",
      "Epoch 415/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2112 - accuracy: 0.9071\n",
      "Epoch 416/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2106 - accuracy: 0.9067\n",
      "Epoch 417/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2062 - accuracy: 0.9094\n",
      "Epoch 418/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2120 - accuracy: 0.9056\n",
      "Epoch 419/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2122 - accuracy: 0.9054\n",
      "Epoch 420/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2087 - accuracy: 0.9096\n",
      "Epoch 421/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2113 - accuracy: 0.9077\n",
      "Epoch 422/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2127 - accuracy: 0.9058\n",
      "Epoch 423/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2067 - accuracy: 0.9087\n",
      "Epoch 424/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2100 - accuracy: 0.9070\n",
      "Epoch 425/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2079 - accuracy: 0.9077\n",
      "Epoch 426/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2092 - accuracy: 0.9103\n",
      "Epoch 427/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2127 - accuracy: 0.9082\n",
      "Epoch 428/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2173 - accuracy: 0.9058\n",
      "Epoch 429/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2075 - accuracy: 0.9082\n",
      "Epoch 430/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2121 - accuracy: 0.9051\n",
      "Epoch 431/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2174 - accuracy: 0.9044\n",
      "Epoch 432/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2051 - accuracy: 0.9104\n",
      "Epoch 433/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2106 - accuracy: 0.9092\n",
      "Epoch 434/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2058 - accuracy: 0.9104\n",
      "Epoch 435/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2139 - accuracy: 0.9062\n",
      "Epoch 436/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2118 - accuracy: 0.9072\n",
      "Epoch 437/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2124 - accuracy: 0.9076\n",
      "Epoch 438/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2076 - accuracy: 0.9083\n",
      "Epoch 439/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2086 - accuracy: 0.9082\n",
      "Epoch 440/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2049 - accuracy: 0.9083\n",
      "Epoch 441/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2102 - accuracy: 0.9086\n",
      "Epoch 442/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2060 - accuracy: 0.9107\n",
      "Epoch 443/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2062 - accuracy: 0.9090\n",
      "Epoch 444/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2119 - accuracy: 0.9072\n",
      "Epoch 445/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2076 - accuracy: 0.9109\n",
      "Epoch 446/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2079 - accuracy: 0.9090\n",
      "Epoch 447/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2126 - accuracy: 0.9056\n",
      "Epoch 448/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2012 - accuracy: 0.9112\n",
      "Epoch 449/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2046 - accuracy: 0.9086\n",
      "Epoch 450/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2048 - accuracy: 0.9111\n",
      "Epoch 451/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2072 - accuracy: 0.9097\n",
      "Epoch 452/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2065 - accuracy: 0.9087\n",
      "Epoch 453/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2098 - accuracy: 0.9087\n",
      "Epoch 454/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2046 - accuracy: 0.9115\n",
      "Epoch 455/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2104 - accuracy: 0.9088\n",
      "Epoch 456/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2010 - accuracy: 0.9131\n",
      "Epoch 457/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2044 - accuracy: 0.9108\n",
      "Epoch 458/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2012 - accuracy: 0.9114\n",
      "Epoch 459/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2065 - accuracy: 0.9102\n",
      "Epoch 460/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2067 - accuracy: 0.9091\n",
      "Epoch 461/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2083 - accuracy: 0.9077\n",
      "Epoch 462/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2031 - accuracy: 0.9071\n",
      "Epoch 463/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2007 - accuracy: 0.9132\n",
      "Epoch 464/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.1998 - accuracy: 0.9134\n",
      "Epoch 465/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.1976 - accuracy: 0.9137\n",
      "Epoch 466/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2026 - accuracy: 0.9122\n",
      "Epoch 467/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2017 - accuracy: 0.9129\n",
      "Epoch 468/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.1990 - accuracy: 0.9132\n",
      "Epoch 469/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2028 - accuracy: 0.9127\n",
      "Epoch 470/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.1974 - accuracy: 0.9158\n",
      "Epoch 471/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.1955 - accuracy: 0.9145\n",
      "Epoch 472/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2081 - accuracy: 0.9090\n",
      "Epoch 473/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.1975 - accuracy: 0.9129\n",
      "Epoch 474/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.1988 - accuracy: 0.9138\n",
      "Epoch 475/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2016 - accuracy: 0.9119\n",
      "Epoch 476/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2008 - accuracy: 0.9133\n",
      "Epoch 477/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.1996 - accuracy: 0.9115\n",
      "Epoch 478/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2006 - accuracy: 0.9137\n",
      "Epoch 479/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.1987 - accuracy: 0.9123\n",
      "Epoch 480/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2004 - accuracy: 0.9127\n",
      "Epoch 481/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2011 - accuracy: 0.9123\n",
      "Epoch 482/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.1963 - accuracy: 0.9156\n",
      "Epoch 483/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.1978 - accuracy: 0.9115\n",
      "Epoch 484/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.1924 - accuracy: 0.9140\n",
      "Epoch 485/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.1996 - accuracy: 0.9138\n",
      "Epoch 486/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.1999 - accuracy: 0.9131\n",
      "Epoch 487/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.1983 - accuracy: 0.9145\n",
      "Epoch 488/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.1966 - accuracy: 0.9152\n",
      "Epoch 489/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.1981 - accuracy: 0.9115\n",
      "Epoch 490/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.1993 - accuracy: 0.9128\n",
      "Epoch 491/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.1987 - accuracy: 0.9163\n",
      "Epoch 492/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.1911 - accuracy: 0.9186\n",
      "Epoch 493/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.1964 - accuracy: 0.9133\n",
      "Epoch 494/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.1929 - accuracy: 0.9142\n",
      "Epoch 495/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2010 - accuracy: 0.9124\n",
      "Epoch 496/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.1888 - accuracy: 0.9173\n",
      "Epoch 497/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.1988 - accuracy: 0.9131\n",
      "Epoch 498/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.1942 - accuracy: 0.9156\n",
      "Epoch 499/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.1966 - accuracy: 0.9128\n",
      "Epoch 500/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.1944 - accuracy: 0.9174\n",
      "94/94 [==============================] - 0s 692us/step - loss: 0.3113 - accuracy: 0.8716\n"
     ]
    }
   ],
   "source": [
    "# Whole 14986: train=11988, test=2998\n",
    "x_train_mean, x_test_mean, y_train_mean, y_test_mean = train_test_split(imputation_mean, imputation_y_bin, \n",
    "                                                                    test_size=0.2, random_state=1)\n",
    "history_mean = model_clf.fit(x_train_mean, y_train_mean, epochs=500, batch_size=100)\n",
    "y_pred = model_clf.evaluate(x_test_mean, y_test_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss: 0.3113 - accuracy: 0.8716"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.3216 - accuracy: 0.8851\n",
      "Epoch 2/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.3011 - accuracy: 0.8851\n",
      "Epoch 3/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2956 - accuracy: 0.8853\n",
      "Epoch 4/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2943 - accuracy: 0.8851\n",
      "Epoch 5/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2829 - accuracy: 0.8861\n",
      "Epoch 6/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2840 - accuracy: 0.8856\n",
      "Epoch 7/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2759 - accuracy: 0.8856\n",
      "Epoch 8/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2869 - accuracy: 0.8852\n",
      "Epoch 9/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2781 - accuracy: 0.8859\n",
      "Epoch 10/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2756 - accuracy: 0.8860\n",
      "Epoch 11/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2793 - accuracy: 0.8859\n",
      "Epoch 12/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2715 - accuracy: 0.8863\n",
      "Epoch 13/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2652 - accuracy: 0.8861\n",
      "Epoch 14/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2657 - accuracy: 0.8868\n",
      "Epoch 15/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2687 - accuracy: 0.8864\n",
      "Epoch 16/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2621 - accuracy: 0.8861\n",
      "Epoch 17/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2688 - accuracy: 0.8872\n",
      "Epoch 18/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2626 - accuracy: 0.8864\n",
      "Epoch 19/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2691 - accuracy: 0.8853\n",
      "Epoch 20/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2587 - accuracy: 0.8857\n",
      "Epoch 21/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2620 - accuracy: 0.8871\n",
      "Epoch 22/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2612 - accuracy: 0.8866\n",
      "Epoch 23/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2584 - accuracy: 0.8879\n",
      "Epoch 24/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2641 - accuracy: 0.8874\n",
      "Epoch 25/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2602 - accuracy: 0.8884\n",
      "Epoch 26/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2703 - accuracy: 0.8874\n",
      "Epoch 27/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2565 - accuracy: 0.8875\n",
      "Epoch 28/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2554 - accuracy: 0.8897\n",
      "Epoch 29/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2536 - accuracy: 0.8886\n",
      "Epoch 30/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2512 - accuracy: 0.8879\n",
      "Epoch 31/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2559 - accuracy: 0.8881\n",
      "Epoch 32/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2508 - accuracy: 0.8866\n",
      "Epoch 33/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2586 - accuracy: 0.8872\n",
      "Epoch 34/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2517 - accuracy: 0.8908\n",
      "Epoch 35/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2543 - accuracy: 0.8890\n",
      "Epoch 36/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2611 - accuracy: 0.8875\n",
      "Epoch 37/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2590 - accuracy: 0.8877\n",
      "Epoch 38/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2569 - accuracy: 0.8880\n",
      "Epoch 39/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2541 - accuracy: 0.8884\n",
      "Epoch 40/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2557 - accuracy: 0.8889\n",
      "Epoch 41/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2541 - accuracy: 0.8872\n",
      "Epoch 42/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2526 - accuracy: 0.8913\n",
      "Epoch 43/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2514 - accuracy: 0.8906\n",
      "Epoch 44/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2485 - accuracy: 0.8935\n",
      "Epoch 45/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2542 - accuracy: 0.8879\n",
      "Epoch 46/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2560 - accuracy: 0.8888\n",
      "Epoch 47/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2524 - accuracy: 0.8902\n",
      "Epoch 48/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2590 - accuracy: 0.8863\n",
      "Epoch 49/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2516 - accuracy: 0.8889\n",
      "Epoch 50/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2567 - accuracy: 0.8877\n",
      "Epoch 51/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2561 - accuracy: 0.8889\n",
      "Epoch 52/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2547 - accuracy: 0.8899\n",
      "Epoch 53/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2483 - accuracy: 0.8906\n",
      "Epoch 54/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2519 - accuracy: 0.8882\n",
      "Epoch 55/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2492 - accuracy: 0.8901\n",
      "Epoch 56/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2535 - accuracy: 0.8884\n",
      "Epoch 57/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2488 - accuracy: 0.8888\n",
      "Epoch 58/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2475 - accuracy: 0.8916\n",
      "Epoch 59/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2516 - accuracy: 0.8907\n",
      "Epoch 60/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2561 - accuracy: 0.8886\n",
      "Epoch 61/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2502 - accuracy: 0.8883\n",
      "Epoch 62/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2555 - accuracy: 0.8894\n",
      "Epoch 63/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2512 - accuracy: 0.8897\n",
      "Epoch 64/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2485 - accuracy: 0.8918\n",
      "Epoch 65/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2523 - accuracy: 0.8909\n",
      "Epoch 66/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2534 - accuracy: 0.8895\n",
      "Epoch 67/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.8914\n",
      "Epoch 68/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2505 - accuracy: 0.8896\n",
      "Epoch 69/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2484 - accuracy: 0.8906\n",
      "Epoch 70/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2492 - accuracy: 0.8911\n",
      "Epoch 71/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2500 - accuracy: 0.8907\n",
      "Epoch 72/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2485 - accuracy: 0.8914\n",
      "Epoch 73/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2535 - accuracy: 0.8893\n",
      "Epoch 74/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2450 - accuracy: 0.8928\n",
      "Epoch 75/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2465 - accuracy: 0.8914\n",
      "Epoch 76/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2481 - accuracy: 0.8937\n",
      "Epoch 77/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2472 - accuracy: 0.8901\n",
      "Epoch 78/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2506 - accuracy: 0.8886\n",
      "Epoch 79/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2470 - accuracy: 0.8931\n",
      "Epoch 80/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2482 - accuracy: 0.8907\n",
      "Epoch 81/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2433 - accuracy: 0.8939\n",
      "Epoch 82/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2504 - accuracy: 0.8896\n",
      "Epoch 83/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2494 - accuracy: 0.8902\n",
      "Epoch 84/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2439 - accuracy: 0.8915\n",
      "Epoch 85/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2441 - accuracy: 0.8911\n",
      "Epoch 86/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2461 - accuracy: 0.8908\n",
      "Epoch 87/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2467 - accuracy: 0.8917\n",
      "Epoch 88/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2486 - accuracy: 0.8894\n",
      "Epoch 89/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2466 - accuracy: 0.8926\n",
      "Epoch 90/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2453 - accuracy: 0.8919\n",
      "Epoch 91/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2471 - accuracy: 0.8926\n",
      "Epoch 92/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2468 - accuracy: 0.8922\n",
      "Epoch 93/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2484 - accuracy: 0.8907\n",
      "Epoch 94/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2510 - accuracy: 0.8905\n",
      "Epoch 95/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2470 - accuracy: 0.8909\n",
      "Epoch 96/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2480 - accuracy: 0.8925\n",
      "Epoch 97/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2461 - accuracy: 0.8901\n",
      "Epoch 98/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2467 - accuracy: 0.8919\n",
      "Epoch 99/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2470 - accuracy: 0.8894\n",
      "Epoch 100/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2474 - accuracy: 0.8927\n",
      "Epoch 101/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2458 - accuracy: 0.8926\n",
      "Epoch 102/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2447 - accuracy: 0.8926\n",
      "Epoch 103/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2482 - accuracy: 0.8929\n",
      "Epoch 104/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2469 - accuracy: 0.8927\n",
      "Epoch 105/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2476 - accuracy: 0.8907\n",
      "Epoch 106/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2455 - accuracy: 0.8915\n",
      "Epoch 107/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2484 - accuracy: 0.8918\n",
      "Epoch 108/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2451 - accuracy: 0.8944\n",
      "Epoch 109/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2465 - accuracy: 0.8928\n",
      "Epoch 110/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2426 - accuracy: 0.8943\n",
      "Epoch 111/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2406 - accuracy: 0.8957\n",
      "Epoch 112/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2507 - accuracy: 0.8908\n",
      "Epoch 113/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2475 - accuracy: 0.8917\n",
      "Epoch 114/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2484 - accuracy: 0.8933\n",
      "Epoch 115/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2467 - accuracy: 0.8919\n",
      "Epoch 116/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2451 - accuracy: 0.8914\n",
      "Epoch 117/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2423 - accuracy: 0.8931\n",
      "Epoch 118/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2416 - accuracy: 0.8951\n",
      "Epoch 119/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2408 - accuracy: 0.8960\n",
      "Epoch 120/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2464 - accuracy: 0.8927\n",
      "Epoch 121/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2507 - accuracy: 0.8932\n",
      "Epoch 122/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.8911\n",
      "Epoch 123/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2445 - accuracy: 0.8946\n",
      "Epoch 124/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2431 - accuracy: 0.8968\n",
      "Epoch 125/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2469 - accuracy: 0.8934\n",
      "Epoch 126/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2416 - accuracy: 0.8932\n",
      "Epoch 127/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2461 - accuracy: 0.8943\n",
      "Epoch 128/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2427 - accuracy: 0.8952\n",
      "Epoch 129/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2413 - accuracy: 0.8961\n",
      "Epoch 130/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2431 - accuracy: 0.8926\n",
      "Epoch 131/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2453 - accuracy: 0.8930\n",
      "Epoch 132/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2394 - accuracy: 0.8966\n",
      "Epoch 133/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2412 - accuracy: 0.8936\n",
      "Epoch 134/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2435 - accuracy: 0.8931\n",
      "Epoch 135/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2472 - accuracy: 0.8941\n",
      "Epoch 136/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2422 - accuracy: 0.8951\n",
      "Epoch 137/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2483 - accuracy: 0.8925\n",
      "Epoch 138/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2411 - accuracy: 0.8936\n",
      "Epoch 139/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2404 - accuracy: 0.8945\n",
      "Epoch 140/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2407 - accuracy: 0.8949\n",
      "Epoch 141/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2407 - accuracy: 0.8942\n",
      "Epoch 142/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2470 - accuracy: 0.8946\n",
      "Epoch 143/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2467 - accuracy: 0.8927\n",
      "Epoch 144/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2485 - accuracy: 0.8931\n",
      "Epoch 145/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2465 - accuracy: 0.8918\n",
      "Epoch 146/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2425 - accuracy: 0.8946\n",
      "Epoch 147/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2434 - accuracy: 0.8946\n",
      "Epoch 148/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2403 - accuracy: 0.8962\n",
      "Epoch 149/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2435 - accuracy: 0.8946\n",
      "Epoch 150/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2443 - accuracy: 0.8934\n",
      "Epoch 151/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2455 - accuracy: 0.8930\n",
      "Epoch 152/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2467 - accuracy: 0.8937\n",
      "Epoch 153/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2433 - accuracy: 0.8940\n",
      "Epoch 154/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2411 - accuracy: 0.8952\n",
      "Epoch 155/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2443 - accuracy: 0.8962\n",
      "Epoch 156/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2444 - accuracy: 0.8931\n",
      "Epoch 157/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2420 - accuracy: 0.8941\n",
      "Epoch 158/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2412 - accuracy: 0.8957\n",
      "Epoch 159/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2391 - accuracy: 0.8956\n",
      "Epoch 160/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2397 - accuracy: 0.8945\n",
      "Epoch 161/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2394 - accuracy: 0.8942\n",
      "Epoch 162/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2402 - accuracy: 0.8932\n",
      "Epoch 163/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2432 - accuracy: 0.8950\n",
      "Epoch 164/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2413 - accuracy: 0.8958\n",
      "Epoch 165/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2405 - accuracy: 0.8973\n",
      "Epoch 166/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2404 - accuracy: 0.8956\n",
      "Epoch 167/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2437 - accuracy: 0.8942\n",
      "Epoch 168/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2403 - accuracy: 0.8959\n",
      "Epoch 169/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2449 - accuracy: 0.8932\n",
      "Epoch 170/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2396 - accuracy: 0.8951\n",
      "Epoch 171/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2394 - accuracy: 0.8965\n",
      "Epoch 172/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2453 - accuracy: 0.8935\n",
      "Epoch 173/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2421 - accuracy: 0.8956\n",
      "Epoch 174/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2418 - accuracy: 0.8964\n",
      "Epoch 175/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2366 - accuracy: 0.8951\n",
      "Epoch 176/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2414 - accuracy: 0.8951\n",
      "Epoch 177/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2484 - accuracy: 0.8886\n",
      "Epoch 178/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2390 - accuracy: 0.8969\n",
      "Epoch 179/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2366 - accuracy: 0.8972\n",
      "Epoch 180/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2390 - accuracy: 0.8976\n",
      "Epoch 181/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2401 - accuracy: 0.8957\n",
      "Epoch 182/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2393 - accuracy: 0.8966\n",
      "Epoch 183/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2384 - accuracy: 0.8976\n",
      "Epoch 184/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2400 - accuracy: 0.8961\n",
      "Epoch 185/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2443 - accuracy: 0.8952\n",
      "Epoch 186/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2419 - accuracy: 0.8941\n",
      "Epoch 187/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2377 - accuracy: 0.8987\n",
      "Epoch 188/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2393 - accuracy: 0.8957\n",
      "Epoch 189/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2397 - accuracy: 0.8977\n",
      "Epoch 190/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2402 - accuracy: 0.8961\n",
      "Epoch 191/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2355 - accuracy: 0.8982\n",
      "Epoch 192/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2376 - accuracy: 0.8974\n",
      "Epoch 193/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2397 - accuracy: 0.8951\n",
      "Epoch 194/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2389 - accuracy: 0.8963\n",
      "Epoch 195/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2365 - accuracy: 0.8971\n",
      "Epoch 196/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2377 - accuracy: 0.8966\n",
      "Epoch 197/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2389 - accuracy: 0.8970\n",
      "Epoch 198/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2371 - accuracy: 0.8958\n",
      "Epoch 199/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2381 - accuracy: 0.8971\n",
      "Epoch 200/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2379 - accuracy: 0.8971\n",
      "Epoch 201/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2400 - accuracy: 0.8970\n",
      "Epoch 202/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2398 - accuracy: 0.8937\n",
      "Epoch 203/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2422 - accuracy: 0.8941\n",
      "Epoch 204/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2372 - accuracy: 0.8982\n",
      "Epoch 205/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2373 - accuracy: 0.8977\n",
      "Epoch 206/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2384 - accuracy: 0.8972\n",
      "Epoch 207/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2412 - accuracy: 0.8951\n",
      "Epoch 208/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2385 - accuracy: 0.8958\n",
      "Epoch 209/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2398 - accuracy: 0.8951\n",
      "Epoch 210/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2344 - accuracy: 0.8987\n",
      "Epoch 211/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2370 - accuracy: 0.8972\n",
      "Epoch 212/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2355 - accuracy: 0.8968\n",
      "Epoch 213/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2330 - accuracy: 0.8989\n",
      "Epoch 214/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2362 - accuracy: 0.8965\n",
      "Epoch 215/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2360 - accuracy: 0.8971\n",
      "Epoch 216/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2329 - accuracy: 0.8976\n",
      "Epoch 217/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2437 - accuracy: 0.8933\n",
      "Epoch 218/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2406 - accuracy: 0.8960\n",
      "Epoch 219/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2413 - accuracy: 0.8939\n",
      "Epoch 220/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2393 - accuracy: 0.8971\n",
      "Epoch 221/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2355 - accuracy: 0.8986\n",
      "Epoch 222/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2472 - accuracy: 0.8932\n",
      "Epoch 223/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2363 - accuracy: 0.8990\n",
      "Epoch 224/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2376 - accuracy: 0.8979\n",
      "Epoch 225/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2372 - accuracy: 0.8964\n",
      "Epoch 226/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2366 - accuracy: 0.8973\n",
      "Epoch 227/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2406 - accuracy: 0.8953\n",
      "Epoch 228/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2401 - accuracy: 0.8941\n",
      "Epoch 229/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2369 - accuracy: 0.8986\n",
      "Epoch 230/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2391 - accuracy: 0.8960\n",
      "Epoch 231/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2396 - accuracy: 0.8987\n",
      "Epoch 232/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2399 - accuracy: 0.8956\n",
      "Epoch 233/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2366 - accuracy: 0.8977\n",
      "Epoch 234/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2330 - accuracy: 0.8988\n",
      "Epoch 235/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2351 - accuracy: 0.8959\n",
      "Epoch 236/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2383 - accuracy: 0.8955\n",
      "Epoch 237/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2385 - accuracy: 0.8971\n",
      "Epoch 238/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2340 - accuracy: 0.8966\n",
      "Epoch 239/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2323 - accuracy: 0.9000\n",
      "Epoch 240/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2346 - accuracy: 0.8963\n",
      "Epoch 241/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2380 - accuracy: 0.8975\n",
      "Epoch 242/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2354 - accuracy: 0.8991\n",
      "Epoch 243/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2328 - accuracy: 0.8993\n",
      "Epoch 244/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2362 - accuracy: 0.8985\n",
      "Epoch 245/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2368 - accuracy: 0.8967\n",
      "Epoch 246/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2349 - accuracy: 0.8968\n",
      "Epoch 247/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2355 - accuracy: 0.8982\n",
      "Epoch 248/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2362 - accuracy: 0.8953\n",
      "Epoch 249/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2377 - accuracy: 0.8976\n",
      "Epoch 250/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2331 - accuracy: 0.8993\n",
      "Epoch 251/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2344 - accuracy: 0.8973\n",
      "Epoch 252/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2322 - accuracy: 0.8998\n",
      "Epoch 253/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2374 - accuracy: 0.8975\n",
      "Epoch 254/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2385 - accuracy: 0.8961\n",
      "Epoch 255/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2338 - accuracy: 0.8991\n",
      "Epoch 256/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2301 - accuracy: 0.9012\n",
      "Epoch 257/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2283 - accuracy: 0.9009\n",
      "Epoch 258/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2350 - accuracy: 0.8980\n",
      "Epoch 259/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2363 - accuracy: 0.8975\n",
      "Epoch 260/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2315 - accuracy: 0.8987\n",
      "Epoch 261/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2371 - accuracy: 0.8966\n",
      "Epoch 262/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2291 - accuracy: 0.9003\n",
      "Epoch 263/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2318 - accuracy: 0.8999\n",
      "Epoch 264/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2350 - accuracy: 0.8990\n",
      "Epoch 265/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2365 - accuracy: 0.8986\n",
      "Epoch 266/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2334 - accuracy: 0.8983\n",
      "Epoch 267/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2353 - accuracy: 0.8966\n",
      "Epoch 268/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2342 - accuracy: 0.8996\n",
      "Epoch 269/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2376 - accuracy: 0.8965\n",
      "Epoch 270/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2312 - accuracy: 0.9005\n",
      "Epoch 271/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2349 - accuracy: 0.8963\n",
      "Epoch 272/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2360 - accuracy: 0.8951\n",
      "Epoch 273/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2317 - accuracy: 0.8997\n",
      "Epoch 274/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2323 - accuracy: 0.8991\n",
      "Epoch 275/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2337 - accuracy: 0.8989\n",
      "Epoch 276/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2273 - accuracy: 0.9012\n",
      "Epoch 277/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2326 - accuracy: 0.8983\n",
      "Epoch 278/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2295 - accuracy: 0.9007\n",
      "Epoch 279/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2296 - accuracy: 0.8990\n",
      "Epoch 280/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2353 - accuracy: 0.8991\n",
      "Epoch 281/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2297 - accuracy: 0.8996\n",
      "Epoch 282/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2362 - accuracy: 0.8969\n",
      "Epoch 283/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2299 - accuracy: 0.9014\n",
      "Epoch 284/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2343 - accuracy: 0.8981\n",
      "Epoch 285/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2298 - accuracy: 0.9014\n",
      "Epoch 286/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2299 - accuracy: 0.9011\n",
      "Epoch 287/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2343 - accuracy: 0.8947\n",
      "Epoch 288/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2309 - accuracy: 0.9017\n",
      "Epoch 289/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2306 - accuracy: 0.9011\n",
      "Epoch 290/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2366 - accuracy: 0.8979\n",
      "Epoch 291/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2286 - accuracy: 0.9002\n",
      "Epoch 292/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2292 - accuracy: 0.9021\n",
      "Epoch 293/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2317 - accuracy: 0.9007\n",
      "Epoch 294/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2280 - accuracy: 0.9021\n",
      "Epoch 295/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2268 - accuracy: 0.9027\n",
      "Epoch 296/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2258 - accuracy: 0.9028\n",
      "Epoch 297/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2299 - accuracy: 0.9028\n",
      "Epoch 298/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2334 - accuracy: 0.8974\n",
      "Epoch 299/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2319 - accuracy: 0.8992\n",
      "Epoch 300/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2325 - accuracy: 0.8986\n",
      "Epoch 301/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2303 - accuracy: 0.8996\n",
      "Epoch 302/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2281 - accuracy: 0.9006\n",
      "Epoch 303/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2293 - accuracy: 0.9022\n",
      "Epoch 304/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2304 - accuracy: 0.9011\n",
      "Epoch 305/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2289 - accuracy: 0.9003\n",
      "Epoch 306/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2276 - accuracy: 0.8998\n",
      "Epoch 307/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2310 - accuracy: 0.8998\n",
      "Epoch 308/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2272 - accuracy: 0.9027\n",
      "Epoch 309/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2298 - accuracy: 0.9000\n",
      "Epoch 310/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2255 - accuracy: 0.9025\n",
      "Epoch 311/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2272 - accuracy: 0.9014\n",
      "Epoch 312/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2264 - accuracy: 0.9011\n",
      "Epoch 313/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2257 - accuracy: 0.9027\n",
      "Epoch 314/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2291 - accuracy: 0.9010\n",
      "Epoch 315/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2286 - accuracy: 0.9023\n",
      "Epoch 316/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2261 - accuracy: 0.9004: 0s - loss: 0.2219 - accuracy: \n",
      "Epoch 317/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2270 - accuracy: 0.9031\n",
      "Epoch 318/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2271 - accuracy: 0.9005\n",
      "Epoch 319/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2267 - accuracy: 0.9022\n",
      "Epoch 320/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2323 - accuracy: 0.9005\n",
      "Epoch 321/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2211 - accuracy: 0.9040\n",
      "Epoch 322/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2267 - accuracy: 0.8991\n",
      "Epoch 323/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2225 - accuracy: 0.9046\n",
      "Epoch 324/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2279 - accuracy: 0.9024\n",
      "Epoch 325/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2265 - accuracy: 0.9035\n",
      "Epoch 326/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2257 - accuracy: 0.9021\n",
      "Epoch 327/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2270 - accuracy: 0.9031\n",
      "Epoch 328/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2265 - accuracy: 0.9042\n",
      "Epoch 329/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2288 - accuracy: 0.9019\n",
      "Epoch 330/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2234 - accuracy: 0.9045\n",
      "Epoch 331/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2216 - accuracy: 0.9065\n",
      "Epoch 332/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2275 - accuracy: 0.9010\n",
      "Epoch 333/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2263 - accuracy: 0.9025\n",
      "Epoch 334/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2271 - accuracy: 0.9022\n",
      "Epoch 335/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2211 - accuracy: 0.9039\n",
      "Epoch 336/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2243 - accuracy: 0.9037\n",
      "Epoch 337/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2236 - accuracy: 0.9040\n",
      "Epoch 338/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2245 - accuracy: 0.9032\n",
      "Epoch 339/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2263 - accuracy: 0.9019\n",
      "Epoch 340/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2315 - accuracy: 0.9010\n",
      "Epoch 341/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2268 - accuracy: 0.9028\n",
      "Epoch 342/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2218 - accuracy: 0.9042\n",
      "Epoch 343/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2286 - accuracy: 0.9032\n",
      "Epoch 344/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2258 - accuracy: 0.9042\n",
      "Epoch 345/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2199 - accuracy: 0.9032\n",
      "Epoch 346/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2254 - accuracy: 0.9035\n",
      "Epoch 347/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2299 - accuracy: 0.9005\n",
      "Epoch 348/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2226 - accuracy: 0.9027\n",
      "Epoch 349/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2193 - accuracy: 0.9056\n",
      "Epoch 350/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2192 - accuracy: 0.9042\n",
      "Epoch 351/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2248 - accuracy: 0.9045\n",
      "Epoch 352/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2203 - accuracy: 0.9057\n",
      "Epoch 353/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2199 - accuracy: 0.9090\n",
      "Epoch 354/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2233 - accuracy: 0.9037\n",
      "Epoch 355/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2195 - accuracy: 0.9052\n",
      "Epoch 356/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2199 - accuracy: 0.9045\n",
      "Epoch 357/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2204 - accuracy: 0.9046\n",
      "Epoch 358/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2220 - accuracy: 0.9047\n",
      "Epoch 359/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2279 - accuracy: 0.9008\n",
      "Epoch 360/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2175 - accuracy: 0.9083\n",
      "Epoch 361/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2149 - accuracy: 0.9077\n",
      "Epoch 362/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2209 - accuracy: 0.9059\n",
      "Epoch 363/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2245 - accuracy: 0.9035\n",
      "Epoch 364/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2193 - accuracy: 0.9055\n",
      "Epoch 365/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2198 - accuracy: 0.9047\n",
      "Epoch 366/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2175 - accuracy: 0.9086\n",
      "Epoch 367/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2163 - accuracy: 0.9082\n",
      "Epoch 368/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2233 - accuracy: 0.9029\n",
      "Epoch 369/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2226 - accuracy: 0.9055\n",
      "Epoch 370/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2151 - accuracy: 0.9085\n",
      "Epoch 371/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2182 - accuracy: 0.9062\n",
      "Epoch 372/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2206 - accuracy: 0.9061\n",
      "Epoch 373/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2194 - accuracy: 0.9066\n",
      "Epoch 374/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2184 - accuracy: 0.9072\n",
      "Epoch 375/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2176 - accuracy: 0.9064\n",
      "Epoch 376/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2115 - accuracy: 0.9100\n",
      "Epoch 377/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2202 - accuracy: 0.9053\n",
      "Epoch 378/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2144 - accuracy: 0.9076\n",
      "Epoch 379/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2155 - accuracy: 0.9077\n",
      "Epoch 380/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2166 - accuracy: 0.9064\n",
      "Epoch 381/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2165 - accuracy: 0.9062\n",
      "Epoch 382/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2233 - accuracy: 0.9027\n",
      "Epoch 383/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2146 - accuracy: 0.9072\n",
      "Epoch 384/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2153 - accuracy: 0.9083\n",
      "Epoch 385/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2172 - accuracy: 0.9047\n",
      "Epoch 386/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2225 - accuracy: 0.9031\n",
      "Epoch 387/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2152 - accuracy: 0.9067\n",
      "Epoch 388/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2175 - accuracy: 0.9060\n",
      "Epoch 389/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2117 - accuracy: 0.9074\n",
      "Epoch 390/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2192 - accuracy: 0.9048\n",
      "Epoch 391/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2169 - accuracy: 0.9065\n",
      "Epoch 392/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2189 - accuracy: 0.9074\n",
      "Epoch 393/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2225 - accuracy: 0.9041\n",
      "Epoch 394/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2139 - accuracy: 0.9082\n",
      "Epoch 395/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2106 - accuracy: 0.9112\n",
      "Epoch 396/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2081 - accuracy: 0.9098\n",
      "Epoch 397/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2064 - accuracy: 0.9117\n",
      "Epoch 398/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2125 - accuracy: 0.9089\n",
      "Epoch 399/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2146 - accuracy: 0.9080\n",
      "Epoch 400/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2067 - accuracy: 0.9119\n",
      "Epoch 401/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2120 - accuracy: 0.9092\n",
      "Epoch 402/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2071 - accuracy: 0.9084\n",
      "Epoch 403/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2126 - accuracy: 0.9078\n",
      "Epoch 404/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2275 - accuracy: 0.9035\n",
      "Epoch 405/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2166 - accuracy: 0.9082\n",
      "Epoch 406/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2152 - accuracy: 0.9071\n",
      "Epoch 407/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2105 - accuracy: 0.9109\n",
      "Epoch 408/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2118 - accuracy: 0.9086\n",
      "Epoch 409/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2076 - accuracy: 0.9097\n",
      "Epoch 410/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2087 - accuracy: 0.9097\n",
      "Epoch 411/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2088 - accuracy: 0.9092\n",
      "Epoch 412/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2062 - accuracy: 0.9117\n",
      "Epoch 413/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2144 - accuracy: 0.9082\n",
      "Epoch 414/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2066 - accuracy: 0.9115\n",
      "Epoch 415/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2181 - accuracy: 0.9072\n",
      "Epoch 416/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2126 - accuracy: 0.9099\n",
      "Epoch 417/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2145 - accuracy: 0.9081\n",
      "Epoch 418/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2122 - accuracy: 0.9090\n",
      "Epoch 419/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2149 - accuracy: 0.9056\n",
      "Epoch 420/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2103 - accuracy: 0.9082\n",
      "Epoch 421/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2065 - accuracy: 0.9087\n",
      "Epoch 422/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2049 - accuracy: 0.9113\n",
      "Epoch 423/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2052 - accuracy: 0.9116\n",
      "Epoch 424/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2155 - accuracy: 0.9069\n",
      "Epoch 425/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2060 - accuracy: 0.9117\n",
      "Epoch 426/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2097 - accuracy: 0.9084\n",
      "Epoch 427/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2160 - accuracy: 0.9066\n",
      "Epoch 428/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2102 - accuracy: 0.9107\n",
      "Epoch 429/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2080 - accuracy: 0.9102\n",
      "Epoch 430/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2047 - accuracy: 0.9112\n",
      "Epoch 431/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2043 - accuracy: 0.9114\n",
      "Epoch 432/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2071 - accuracy: 0.9122\n",
      "Epoch 433/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2012 - accuracy: 0.9129\n",
      "Epoch 434/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2068 - accuracy: 0.9120\n",
      "Epoch 435/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2198 - accuracy: 0.9046\n",
      "Epoch 436/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2069 - accuracy: 0.9132\n",
      "Epoch 437/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2051 - accuracy: 0.9126\n",
      "Epoch 438/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2023 - accuracy: 0.9122\n",
      "Epoch 439/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2072 - accuracy: 0.9106\n",
      "Epoch 440/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2033 - accuracy: 0.9125\n",
      "Epoch 441/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.1996 - accuracy: 0.9152\n",
      "Epoch 442/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2030 - accuracy: 0.9123\n",
      "Epoch 443/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2101 - accuracy: 0.9118\n",
      "Epoch 444/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2017 - accuracy: 0.9130\n",
      "Epoch 445/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2017 - accuracy: 0.9145\n",
      "Epoch 446/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2103 - accuracy: 0.9076\n",
      "Epoch 447/500\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 0.2086 - accuracy: 0.9096\n",
      "Epoch 448/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2063 - accuracy: 0.9112\n",
      "Epoch 449/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2043 - accuracy: 0.9115\n",
      "Epoch 450/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2076 - accuracy: 0.9093\n",
      "Epoch 451/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.1997 - accuracy: 0.9153\n",
      "Epoch 452/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2030 - accuracy: 0.9123\n",
      "Epoch 453/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2092 - accuracy: 0.9097\n",
      "Epoch 454/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2127 - accuracy: 0.9071\n",
      "Epoch 455/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2260 - accuracy: 0.9025\n",
      "Epoch 456/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2079 - accuracy: 0.9111\n",
      "Epoch 457/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2041 - accuracy: 0.9106\n",
      "Epoch 458/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2042 - accuracy: 0.9135\n",
      "Epoch 459/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2225 - accuracy: 0.9047\n",
      "Epoch 460/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2064 - accuracy: 0.9099\n",
      "Epoch 461/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.1971 - accuracy: 0.9164\n",
      "Epoch 462/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.1981 - accuracy: 0.9146\n",
      "Epoch 463/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2061 - accuracy: 0.9115\n",
      "Epoch 464/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.1935 - accuracy: 0.9160\n",
      "Epoch 465/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2037 - accuracy: 0.9124\n",
      "Epoch 466/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2036 - accuracy: 0.9144\n",
      "Epoch 467/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.1979 - accuracy: 0.9141\n",
      "Epoch 468/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.1999 - accuracy: 0.9130\n",
      "Epoch 469/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.1972 - accuracy: 0.9169\n",
      "Epoch 470/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.1937 - accuracy: 0.9170\n",
      "Epoch 471/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.1970 - accuracy: 0.9151\n",
      "Epoch 472/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2121 - accuracy: 0.9084\n",
      "Epoch 473/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.1954 - accuracy: 0.9159\n",
      "Epoch 474/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2009 - accuracy: 0.9135\n",
      "Epoch 475/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.1922 - accuracy: 0.9168\n",
      "Epoch 476/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 2ms/step - loss: 0.1965 - accuracy: 0.9146\n",
      "Epoch 477/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.1949 - accuracy: 0.9153\n",
      "Epoch 478/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2035 - accuracy: 0.9117\n",
      "Epoch 479/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.1916 - accuracy: 0.9175\n",
      "Epoch 480/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2126 - accuracy: 0.9095\n",
      "Epoch 481/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.1982 - accuracy: 0.9147\n",
      "Epoch 482/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.1982 - accuracy: 0.9143\n",
      "Epoch 483/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.1926 - accuracy: 0.9158\n",
      "Epoch 484/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.1904 - accuracy: 0.9194\n",
      "Epoch 485/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.1918 - accuracy: 0.9175\n",
      "Epoch 486/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.1907 - accuracy: 0.9171\n",
      "Epoch 487/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.1911 - accuracy: 0.9196\n",
      "Epoch 488/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2161 - accuracy: 0.9068\n",
      "Epoch 489/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.1908 - accuracy: 0.9184\n",
      "Epoch 490/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2012 - accuracy: 0.9135\n",
      "Epoch 491/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.1889 - accuracy: 0.9181\n",
      "Epoch 492/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.1875 - accuracy: 0.9197\n",
      "Epoch 493/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.1927 - accuracy: 0.9189\n",
      "Epoch 494/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.1922 - accuracy: 0.9191\n",
      "Epoch 495/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.1902 - accuracy: 0.9194\n",
      "Epoch 496/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.1887 - accuracy: 0.9193\n",
      "Epoch 497/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.1897 - accuracy: 0.9192\n",
      "Epoch 498/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.1865 - accuracy: 0.9203\n",
      "Epoch 499/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.1915 - accuracy: 0.9168\n",
      "Epoch 500/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.1907 - accuracy: 0.9172\n",
      "94/94 [==============================] - 0s 667us/step - loss: 0.3635 - accuracy: 0.8823\n"
     ]
    }
   ],
   "source": [
    "#x_train_dae, x_test_dae, y_train_dae, y_test_dae\n",
    "x_train_dae, x_test_dae, y_train_dae, y_test_dae = train_test_split(imputation_dae, imputation_y_bin, \n",
    "                                                                    test_size=0.2, random_state=1)\n",
    "\n",
    "history_dae = model_clf.fit(x_train_dae, y_train_dae, epochs=500, batch_size=100)\n",
    "y_pred = model_clf.evaluate(x_test_dae, y_test_dae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss: 0.3635 - accuracy: 0.8823"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Files backup on Github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! git remote add origin https://github.com/shelly3025/FinalProject_DAE.git\n",
    "! git branch -M main\n",
    "! git push -u origin main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warning: LF will be replaced by CRLF in Denoising AutoEncoder.ipynb.\n",
      "The file will have its original line endings in your working directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main 6353769] 7th commit-20220107\n",
      " 1 file changed, 3289 insertions(+), 1850 deletions(-)\n"
     ]
    }
   ],
   "source": [
    "! git add -A\n",
    "! git commit -m \"7th commit-20220107\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6353769 7th commit-20220107\n",
      "ec5c762 6th commit-20211228\n",
      "6542403 5th commit-20211227\n",
      "9ff2d89 4th commit-20211225\n",
      "df0cc61 3rd commit-20211224\n",
      "85a84f1 2nd commit-20211223\n",
      "aff242d first commit-20211222\n"
     ]
    }
   ],
   "source": [
    "! git log --oneline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! git push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-env",
   "language": "python",
   "name": "dl-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
